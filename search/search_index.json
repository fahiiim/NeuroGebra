{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Neurogebra","text":""},{"location":"#the-executable-mathematical-formula-companion-for-ai-and-data-science","title":"The Executable Mathematical Formula Companion for AI and Data Science","text":"<p>v2.5.3 | 285 Symbolic Expressions | 100+ Datasets | Training Observatory Pro</p> <p>Neurogebra is a unified Python library that bridges symbolic mathematics, numerical computation, and machine learning. It provides 285 pre-built, tested, and documented mathematical expressions, 100+ curated datasets, and a training system with full mathematical transparency -- designed for students, researchers, and engineers alike.</p> <p>Who is this for?</p> <ul> <li>Students learning ML/AI who want to see the math behind every operation</li> <li>Researchers who need reproducibility, rapid formula prototyping, and transparent diagnostics</li> <li>Engineers who want a verified formula library with production-ready logging</li> </ul>"},{"location":"#what-makes-neurogebra-different","title":"What Makes Neurogebra Different?","text":"Feature Traditional Frameworks (PyTorch, TF) Neurogebra Learning curve Steep -- many hidden abstractions Gentle -- every step is explained Math visibility Hidden inside C++ kernels Symbolic -- you SEE the formulas Expressions Tensors/Modules you don't understand Mathematical expressions you can read Explanations Read research papers Built-in <code>.explain()</code> on everything Training diagnostics Basic loss curves Full math transparency with Observatory Pro Reproducibility Manual tracking Automatic training fingerprinting Formula library Build your own 285 verified, searchable, composable formulas Target audience Production engineers Students, researchers, and engineers"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from neurogebra import MathForge\n\n# Create the main interface\nforge = MathForge()\n\n# Get the ReLU activation function\nrelu = forge.get(\"relu\")\n\n# See what it actually IS\nprint(relu.explain())\n# Output: \"ReLU (Rectified Linear Unit) outputs x if x &gt; 0, else 0\"\n\n# See the formula\nprint(relu.formula)\n# Output: Max(0, x)\n\n# Evaluate it\nprint(relu.eval(x=5))    # 5\nprint(relu.eval(x=-3))   # 0\n\n# Get its derivative (gradient)\nrelu_grad = relu.gradient(\"x\")\nprint(relu_grad)\n</code></pre> <p>See how readable that is?</p> <p>Every expression tells you what it is, how it works, and why it matters. No magic. No black boxes.</p>"},{"location":"#what-youll-learn-in-this-documentation","title":"What You'll Learn in This Documentation","text":"<p>This documentation is structured as a progressive learning path, starting from absolute basics:</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Install Neurogebra and write your first program in under 5 minutes.</p>"},{"location":"#python-for-ml-refresher","title":"Python for ML (Refresher)","text":"<p>A quick refresher on Python basics, NumPy, and data handling -- the prerequisites for ML.</p>"},{"location":"#ml-fundamentals","title":"ML Fundamentals","text":"<p>What Machine Learning actually is, the types of ML, the standard workflow, and the math behind it all.</p>"},{"location":"#neurogebra-tutorial","title":"Neurogebra Tutorial","text":"<p>Step-by-step lessons on every feature -- from expressions to training to model building.</p>"},{"location":"#training-observatory-and-observatory-pro","title":"Training Observatory and Observatory Pro","text":"<p>Real-time training diagnostics with adaptive logging, automated health warnings, epoch summaries, visual dashboards, and training fingerprinting.</p>"},{"location":"#advanced-topics","title":"Advanced Topics","text":"<p>Custom expressions, framework bridges (PyTorch / TF / JAX), visualization, and optimization.</p>"},{"location":"#full-projects-neurogebra-vs-pytorch","title":"Full Projects (Neurogebra vs PyTorch)","text":"<p>3 complete ML/Deep Learning projects with side-by-side code comparison between Neurogebra and PyTorch.</p>"},{"location":"#install-now","title":"Install Now","text":"<pre><code>pip install neurogebra\n</code></pre> <p>Ready? Start with Installation \u2192</p>"},{"location":"getting-started/","title":"Getting Started with Neurogebra","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code>pip install neurogebra\n</code></pre> <p>For optional features: <pre><code>pip install neurogebra[viz]        # Visualization tools\npip install neurogebra[fast]       # Performance optimizations\npip install neurogebra[frameworks] # PyTorch, TensorFlow support\npip install neurogebra[all]        # Everything\n</code></pre></p>"},{"location":"getting-started/#your-first-expression","title":"Your First Expression","text":"<pre><code>from neurogebra import MathForge\n\n# Create forge instance\nforge = MathForge()\n\n# Get an activation function\nrelu = forge.get(\"relu\")\n\n# Evaluate it\nresult = relu.eval(x=5)\nprint(result)  # 5\n\nresult = relu.eval(x=-3)\nprint(result)  # 0\n</code></pre>"},{"location":"getting-started/#understanding-expressions","title":"Understanding Expressions","text":"<pre><code># Get explanation\nprint(relu.explain())\n\n# See the formula (LaTeX)\nprint(relu.formula)\n\n# Get gradient\nrelu_grad = relu.gradient(\"x\")\nprint(relu_grad)\n</code></pre>"},{"location":"getting-started/#exploring-available-expressions","title":"Exploring Available Expressions","text":"<pre><code># List all expressions\nprint(forge.list_all())\n\n# List by category\nprint(forge.list_all(category=\"activation\"))\nprint(forge.list_all(category=\"loss\"))\n\n# Search\nresults = forge.search(\"classification\")\nprint(results)\n</code></pre>"},{"location":"getting-started/#composing-expressions","title":"Composing Expressions","text":"<pre><code># Get multiple expressions\nmse = forge.get(\"mse\")\nmae = forge.get(\"mae\")\n\n# Arithmetic composition\nhybrid_loss = 0.7 * mse + 0.3 * mae\n\n# String-based composition\ncustom_loss = forge.compose(\"mse + 0.1*mae\")\n</code></pre>"},{"location":"getting-started/#training-expressions","title":"Training Expressions","text":"<pre><code>import numpy as np\nfrom neurogebra import Expression\nfrom neurogebra.core.trainer import Trainer\n\n# Create trainable expression\nexpr = Expression(\n    \"my_line\",\n    \"m*x + b\",\n    params={\"m\": 0.0, \"b\": 0.0},\n    trainable_params=[\"m\", \"b\"]\n)\n\n# Generate synthetic data\nX = np.linspace(0, 5, 50)\ny = 2 * X + 1\n\n# Train\ntrainer = Trainer(expr, learning_rate=0.01)\nhistory = trainer.fit(X, y, epochs=100)\n\nprint(f\"Learned: m={expr.params['m']:.2f}, b={expr.params['b']:.2f}\")\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Beginner Tutorial - Learn the fundamentals</li> <li>Intermediate Tutorial - Advanced features</li> <li>Advanced Tutorial - Expert-level usage</li> </ul>"},{"location":"advanced/custom-expressions/","title":"Custom Expressions","text":"<p>Learn how to create your own mathematical expressions and register them for reuse.</p>"},{"location":"advanced/custom-expressions/#why-custom-expressions","title":"Why Custom Expressions?","text":"<p>While Neurogebra ships with 50+ built-in expressions, you'll often want:</p> <ul> <li>Custom activation functions for research</li> <li>Specialized loss functions for your task</li> <li>Domain-specific mathematical formulas</li> <li>Experimental combinations</li> </ul>"},{"location":"advanced/custom-expressions/#creating-a-custom-expression","title":"Creating a Custom Expression","text":""},{"location":"advanced/custom-expressions/#basic-custom-expression","title":"Basic Custom Expression","text":"<pre><code>from neurogebra import Expression\n\n# Simple: just name and formula\nmy_func = Expression(\"my_square\", \"x**2 + 1\")\n\nprint(my_func.eval(x=3))          # 10.0\nprint(my_func.gradient(\"x\").eval(x=3))  # 6.0\n</code></pre>"},{"location":"advanced/custom-expressions/#with-parameters","title":"With Parameters","text":"<pre><code># Parametric expression\ngaussian = Expression(\n    \"gaussian\",\n    \"A * exp(-(x - mu)**2 / (2 * sigma**2))\",\n    params={\"A\": 1.0, \"mu\": 0.0, \"sigma\": 1.0}\n)\n\nprint(gaussian.eval(x=0))    # 1.0 (peak at mu=0)\nprint(gaussian.eval(x=1))    # \u2248 0.607\nprint(gaussian.eval(x=-1))   # \u2248 0.607\n</code></pre>"},{"location":"advanced/custom-expressions/#with-trainable-parameters","title":"With Trainable Parameters","text":"<pre><code># Parameters that can be learned from data\nlearnable = Expression(\n    \"adaptive_relu\",\n    \"Max(alpha * x, x)\",\n    params={\"alpha\": 0.01},\n    trainable_params=[\"alpha\"],  # This will be optimized\n    metadata={\n        \"category\": \"activation\",\n        \"description\": \"PReLU - Parametric ReLU with learnable slope\"\n    }\n)\n</code></pre>"},{"location":"advanced/custom-expressions/#with-full-metadata","title":"With Full Metadata","text":"<pre><code>my_activation = Expression(\n    \"mish\",\n    \"x * tanh(log(1 + exp(x)))\",\n    metadata={\n        \"category\": \"activation\",\n        \"description\": \"Mish activation function\",\n        \"usage\": \"Modern deep networks, alternative to Swish\",\n        \"pros\": [\"Self-regularizing\", \"Smooth\", \"Non-monotonic\"],\n        \"cons\": [\"Computationally expensive\"],\n        \"paper\": \"Mish: A Self Regularized Non-Monotonic Activation Function\"\n    }\n)\n\nprint(my_activation.metadata[\"description\"])\nprint(my_activation.metadata[\"pros\"])\n</code></pre>"},{"location":"advanced/custom-expressions/#registering-custom-expressions","title":"Registering Custom Expressions","text":"<p>Once created, register with MathForge for easy access:</p> <pre><code>from neurogebra import MathForge, Expression\n\nforge = MathForge()\n\n# Create\nmish = Expression(\n    \"mish\",\n    \"x * tanh(log(1 + exp(x)))\",\n    metadata={\"category\": \"activation\"}\n)\n\n# Register\nforge.register(\"mish\", mish)\n\n# Now use it like any built-in\nretrieved = forge.get(\"mish\")\nprint(retrieved.eval(x=1.0))\n\n# It appears in searches too\nresults = forge.search(\"mish\")\nprint(results)  # ['mish']\n</code></pre>"},{"location":"advanced/custom-expressions/#custom-activation-functions","title":"Custom Activation Functions","text":""},{"location":"advanced/custom-expressions/#elu-exponential-linear-unit","title":"ELU (Exponential Linear Unit)","text":"<pre><code>elu = Expression(\n    \"elu\",\n    \"Piecewise((x, x &gt; 0), (alpha*(exp(x) - 1), True))\",\n    params={\"alpha\": 1.0},\n    metadata={\n        \"category\": \"activation\",\n        \"description\": \"ELU - smooth alternative to ReLU for negative values\"\n    }\n)\n</code></pre>"},{"location":"advanced/custom-expressions/#selu-scaled-elu","title":"SELU (Scaled ELU)","text":"<pre><code>selu = Expression(\n    \"selu\",\n    \"scale * Piecewise((x, x &gt; 0), (alpha*(exp(x) - 1), True))\",\n    params={\"scale\": 1.0507, \"alpha\": 1.6733},\n    metadata={\n        \"category\": \"activation\",\n        \"description\": \"Self-normalizing activation\"\n    }\n)\n</code></pre>"},{"location":"advanced/custom-expressions/#hard-sigmoid","title":"Hard Sigmoid","text":"<pre><code>hard_sigmoid = Expression(\n    \"hard_sigmoid\",\n    \"Max(0, Min(1, 0.2*x + 0.5))\",\n    metadata={\n        \"category\": \"activation\",\n        \"description\": \"Computationally cheap approximation of sigmoid\"\n    }\n)\n</code></pre>"},{"location":"advanced/custom-expressions/#custom-loss-functions","title":"Custom Loss Functions","text":""},{"location":"advanced/custom-expressions/#smooth-l1-loss","title":"Smooth L1 Loss","text":"<pre><code>smooth_l1 = Expression(\n    \"smooth_l1\",\n    \"Piecewise((0.5 * (y_pred - y_true)**2, Abs(y_pred - y_true) &lt; 1), (Abs(y_pred - y_true) - 0.5, True))\",\n    metadata={\n        \"category\": \"loss\",\n        \"description\": \"Smooth L1 loss - used in object detection\"\n    }\n)\n</code></pre>"},{"location":"advanced/custom-expressions/#weighted-mse","title":"Weighted MSE","text":"<pre><code>weighted_mse = Expression(\n    \"weighted_mse\",\n    \"weight * (y_pred - y_true)**2\",\n    params={\"weight\": 1.0},\n    metadata={\n        \"category\": \"loss\",\n        \"description\": \"MSE with sample weight\"\n    }\n)\n</code></pre>"},{"location":"advanced/custom-expressions/#testing-custom-expressions","title":"Testing Custom Expressions","text":"<p>Always test your expressions thoroughly:</p> <pre><code>import numpy as np\nfrom neurogebra import Expression\n\n# Create expression\nmy_func = Expression(\"test\", \"x**3 - 3*x + 2\")\n\n# Test basic evaluation\nassert my_func.eval(x=0) == 2.0\nassert my_func.eval(x=1) == 0.0\n\n# Test gradient\ngrad = my_func.gradient(\"x\")\nassert grad.eval(x=0) == -3.0    # f'(0) = -3\nassert grad.eval(x=1) == 0.0     # f'(1) = 0\n\n# Test array evaluation\nx = np.array([-2, -1, 0, 1, 2])\nresult = my_func.eval(x=x)\nexpected = x**3 - 3*x + 2\nnp.testing.assert_array_almost_equal(result, expected)\n\nprint(\"All tests passed! \u2713\")\n</code></pre> <p>Next: Framework Bridges \u2192</p>"},{"location":"advanced/framework-bridges/","title":"Framework Bridges","text":"<p>Neurogebra can convert expressions to PyTorch, TensorFlow, and JAX \u2014 letting you use Neurogebra's readable syntax and then deploy with production frameworks.</p>"},{"location":"advanced/framework-bridges/#why-bridges","title":"Why Bridges?","text":"<pre><code>Design in Neurogebra (readable, educational)\n    \u2193\nConvert to PyTorch/TF/JAX (fast, production-ready)\n    \u2193\nDeploy in production\n</code></pre> <p>You get the best of both worlds: Neurogebra for understanding, production frameworks for speed.</p>"},{"location":"advanced/framework-bridges/#pytorch-bridge","title":"PyTorch Bridge","text":""},{"location":"advanced/framework-bridges/#setup","title":"Setup","text":"<pre><code>pip install neurogebra[frameworks]\n# or\npip install torch\n</code></pre>"},{"location":"advanced/framework-bridges/#converting-expressions","title":"Converting Expressions","text":"<pre><code>from neurogebra import MathForge\nfrom neurogebra.bridges.pytorch_bridge import to_pytorch\nimport torch\n\nforge = MathForge()\nsigmoid = forge.get(\"sigmoid\")\n\n# Convert to PyTorch module\ntorch_sigmoid = to_pytorch(sigmoid)\n\n# Use with PyTorch tensors\nx = torch.randn(10)\noutput = torch_sigmoid(x)\nprint(output)\n</code></pre>"},{"location":"advanced/framework-bridges/#custom-activation-as-pytorch-module","title":"Custom Activation as PyTorch Module","text":"<pre><code>from neurogebra import Expression\nfrom neurogebra.bridges.pytorch_bridge import to_pytorch\n\n# Design in Neurogebra\nmish = Expression(\"mish\", \"x * tanh(log(1 + exp(x)))\")\n\n# Convert to PyTorch\nmish_module = to_pytorch(mish)\n\n# Use in a PyTorch model\nimport torch.nn as nn\n\nmodel = nn.Sequential(\n    nn.Linear(784, 128),\n    mish_module,               # Your Neurogebra expression!\n    nn.Linear(128, 10),\n)\n</code></pre>"},{"location":"advanced/framework-bridges/#tensorflow-bridge","title":"TensorFlow Bridge","text":""},{"location":"advanced/framework-bridges/#setup_1","title":"Setup","text":"<pre><code>pip install tensorflow\n</code></pre>"},{"location":"advanced/framework-bridges/#converting-expressions_1","title":"Converting Expressions","text":"<pre><code>from neurogebra import MathForge\nfrom neurogebra.bridges.tensorflow_bridge import to_tensorflow\n\nforge = MathForge()\nswish = forge.get(\"swish\")\n\n# Convert to TensorFlow function\ntf_swish = to_tensorflow(swish)\n</code></pre>"},{"location":"advanced/framework-bridges/#jax-bridge","title":"JAX Bridge","text":""},{"location":"advanced/framework-bridges/#setup_2","title":"Setup","text":"<pre><code>pip install jax jaxlib\n</code></pre>"},{"location":"advanced/framework-bridges/#converting-expressions_2","title":"Converting Expressions","text":"<pre><code>from neurogebra import MathForge\nfrom neurogebra.bridges.jax_bridge import to_jax\n\nforge = MathForge()\ngelu = forge.get(\"gelu\")\n\n# Convert to JAX function\njax_gelu = to_jax(gelu)\n</code></pre>"},{"location":"advanced/framework-bridges/#workflow-design-convert-deploy","title":"Workflow: Design \u2192 Convert \u2192 Deploy","text":"<pre><code>from neurogebra import MathForge, Expression\n\nforge = MathForge()\n\n# 1. DESIGN: Explore and understand\nrelu = forge.get(\"relu\")\nprint(relu.explain())\nprint(relu.symbolic_expr)  # Max(0, x)\n\n# 2. EXPERIMENT: Try different options\nfor act_name in [\"relu\", \"swish\", \"gelu\"]:\n    act = forge.get(act_name)\n    print(f\"{act_name}(1.0) = {act.eval(x=1.0):.4f}\")\n\n# 3. CHOOSE: Pick the best one\nchosen = forge.get(\"swish\")\n\n# 4. CONVERT: Move to production framework\nfrom neurogebra.bridges.pytorch_bridge import to_pytorch\ntorch_activation = to_pytorch(chosen)\n\n# 5. DEPLOY: Use in production model\n# model = nn.Sequential(nn.Linear(...), torch_activation, ...)\n</code></pre>"},{"location":"advanced/framework-bridges/#bridge-comparison","title":"Bridge Comparison","text":"Feature PyTorch TensorFlow JAX Support level Full Full Full Returns <code>nn.Module</code> TF function JAX function GPU support \u2705 \u2705 \u2705 Autograd compatible \u2705 \u2705 \u2705 <p>Next: Visualization \u2192</p>"},{"location":"advanced/observatory-deep-dive/","title":"Observatory Deep Dive","text":"<p>Advanced usage patterns and internals of the Training Observatory system.</p>"},{"location":"advanced/observatory-deep-dive/#architecture-overview","title":"Architecture Overview","text":"<p>The Observatory follows an event-driven observer pattern:</p> <pre><code>Model.fit()\n  \u251c\u2500 TrainingLogger           (central dispatcher)\n  \u2502    \u251c\u2500 TerminalDisplay     (colourful terminal output)\n  \u2502    \u251c\u2500 JSONExporter        (structured log file)\n  \u2502    \u251c\u2500 CSVExporter         (metrics table)\n  \u2502    \u251c\u2500 HTMLExporter        (interactive report)\n  \u2502    \u2514\u2500 MarkdownExporter    (human-readable report)\n  \u251c\u2500 GradientMonitor          (gradient health tracking)\n  \u251c\u2500 WeightMonitor            (weight distribution analysis)\n  \u251c\u2500 ActivationMonitor        (dead neuron / saturation detection)\n  \u251c\u2500 PerformanceMonitor       (timing &amp; bottleneck detection)\n  \u2514\u2500 SmartHealthChecker       (aggregate diagnostics)\n</code></pre> <p>Events flow from the model through the logger, which filters by level and dispatches to all registered backends.</p>"},{"location":"advanced/observatory-deep-dive/#event-types","title":"Event Types","text":"Event Level When It Fires <code>train_start</code> BASIC Beginning of <code>model.fit()</code> <code>train_end</code> BASIC End of training <code>epoch_start</code> BASIC Start of each epoch <code>epoch_end</code> BASIC End of each epoch (with metrics) <code>batch_start</code> DETAILED Start of each mini-batch <code>batch_end</code> DETAILED End of each mini-batch (with batch loss) <code>layer_forward</code> EXPERT After each layer's forward pass <code>layer_backward</code> EXPERT After each layer's backward pass <code>gradient_computed</code> EXPERT When a gradient is computed <code>weight_updated</code> EXPERT When weights are updated <code>health_check</code> BASIC When a health alert is generated"},{"location":"advanced/observatory-deep-dive/#custom-backends","title":"Custom Backends","text":"<p>Create your own backend by implementing <code>handle_{event_type}</code> methods:</p> <pre><code>from neurogebra.logging.logger import TrainingLogger, LogLevel, LogEvent\n\nclass MyBackend:\n    def handle_epoch_end(self, event: LogEvent):\n        metrics = event.data.get(\"metrics\", {})\n        print(f\"My custom log: epoch {event.epoch}, loss={metrics.get('loss'):.4f}\")\n\n    def handle_health_check(self, event: LogEvent):\n        if event.severity in (\"danger\", \"critical\"):\n            send_slack_alert(event.message)  # your custom action\n\nlogger = TrainingLogger(level=LogLevel.BASIC)\nlogger.add_backend(MyBackend())\n</code></pre> <p>You can also register callbacks for specific event types:</p> <pre><code>logger.register_callback(\"epoch_end\", lambda e: log_to_database(e))\n</code></pre>"},{"location":"advanced/observatory-deep-dive/#custom-health-checks","title":"Custom Health Checks","text":"<p>The <code>SmartHealthChecker</code> is configurable:</p> <pre><code>from neurogebra.logging.health_checks import SmartHealthChecker\n\nchecker = SmartHealthChecker(\n    patience=10,                  # epochs before stagnation alert\n    overfit_ratio=1.3,            # val_loss/train_loss threshold\n    stagnation_eps=1e-5,          # minimum improvement threshold\n    gradient_vanish_thresh=1e-8,  # custom vanishing threshold\n    gradient_explode_thresh=50,   # custom exploding threshold\n    dead_neuron_pct=30.0,         # alert at 30% dead neurons\n)\n</code></pre>"},{"location":"advanced/observatory-deep-dive/#computation-graph","title":"Computation Graph","text":"<p>Track every operation as a DAG:</p> <pre><code>from neurogebra.logging.computation_graph import GraphTracker\n\ntracker = GraphTracker(store_values=True)\n\nnid = tracker.record_operation(\n    operation=\"matmul\",\n    layer_name=\"dense_0\",\n    inputs=[X, W],\n    output=Z,\n    formula=\"Z = X\u00b7W\",\n)\n\n# Query\nnodes = tracker.get_layer_subgraph(\"dense_0\")\npath = tracker.get_forward_path()\n\n# Export\ndata = tracker.export_graph()     # JSON-serialisable dict\ntracker.print_graph()             # Rich tree in terminal\n</code></pre>"},{"location":"advanced/observatory-deep-dive/#formula-renderer","title":"Formula Renderer","text":"<p>Render mathematical formulas in Unicode or LaTeX:</p> <pre><code>from neurogebra.logging.formula_renderer import FormulaRenderer\n\nrenderer = FormulaRenderer()\n\n# Full model equation\neq = renderer.full_model_equation([\n    {\"type\": \"dense\", \"activation\": \"relu\"},\n    {\"type\": \"dense\", \"activation\": \"sigmoid\"},\n])\nprint(eq)  # \u0177 = \u03c3(W\u2082 \u00b7 relu(W\u2081\u00b7x + b\u2081) + b\u2082)\n\n# Loss formula\nrenderer.render_loss(\"cross_entropy\")\n</code></pre>"},{"location":"advanced/observatory-deep-dive/#image-logger","title":"Image Logger","text":"<p>Render images as ASCII art in your terminal:</p> <pre><code>from neurogebra.logging.image_logger import ImageLogger\nimport numpy as np\n\nimg_logger = ImageLogger()\n\n# Render a grayscale image (28\u00d728 MNIST-style)\nimage = np.random.rand(28, 28)\nimg_logger.render_image(image, title=\"Sample Digit\")\n\n# Check if data looks like images\nif img_logger.is_image_data(X_train):\n    img_logger.render_image(X_train[0])\n</code></pre>"},{"location":"advanced/observatory-deep-dive/#environment-variables","title":"Environment Variables","text":"<p>You can set the default log level via environment variable:</p> <pre><code>export NEUROGEBRA_LOG_LEVEL=EXPERT\n</code></pre> <pre><code>config = LogConfig.from_env()  # reads NEUROGEBRA_LOG_LEVEL\n</code></pre>"},{"location":"advanced/observatory-deep-dive/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>BASIC/DETAILED levels add negligible overhead (&lt; 1% training time)</li> <li>EXPERT level adds ~5-10% overhead due to per-layer statistics computation</li> <li>DEBUG level may add 15-20% overhead \u2014 use for debugging only</li> <li>Export backends write asynchronously at the end of training (no mid-training I/O)</li> <li>Use <code>LogConfig.production()</code> for minimal overhead in deployment</li> </ul>"},{"location":"advanced/observatory-deep-dive/#integration-with-external-tools","title":"Integration with External Tools","text":""},{"location":"advanced/observatory-deep-dive/#tensorboard-optional","title":"TensorBoard (optional)","text":"<pre><code>pip install neurogebra[logging]\n</code></pre> <pre><code># TensorBoard backend coming in v1.3.0\n</code></pre>"},{"location":"advanced/observatory-deep-dive/#weights-biases-optional","title":"Weights &amp; Biases (optional)","text":"<pre><code>pip install neurogebra[logging]\n</code></pre> <pre><code># W&amp;B backend coming in v1.3.0\n</code></pre>"},{"location":"advanced/observatory-pro/","title":"Observatory Pro -- v2.5.3","text":"<p>Available since v1.3.0, current in v2.5.3 -- Six major upgrades that turn the Training Observatory from a passive log dump into an active diagnostic engine.</p>"},{"location":"advanced/observatory-pro/#whats-new","title":"What's New","text":"Feature Problem Solved Impact Adaptive Logging EXPERT logs everything \u2192 77k entries 80-90% log reduction Health Warnings \"All clear\" despite 58% dead neurons Catches problems automatically Epoch Summaries No statistical view per epoch Mean/std/min/max per metric Tiered Storage One flat JSON file 3 focused files: basic/health/debug Visual Dashboard Raw JSON only Interactive HTML charts Training Fingerprint Can't reproduce runs Full environment capture"},{"location":"advanced/observatory-pro/#1-smart-adaptive-logging","title":"1. Smart / Adaptive Logging","text":"<p>The <code>AdaptiveLogger</code> wraps a standard <code>TrainingLogger</code> and only escalates to EXPERT detail when something looks suspicious. In normal operation it stays at BASIC level, reducing log size by 80-90%.</p>"},{"location":"advanced/observatory-pro/#anomaly-triggers","title":"Anomaly Triggers","text":"Trigger Default Threshold What Happens Dead neurons (zeros %) 50% Escalate + emit warning Gradient spike 5\u00d7 rolling average Escalate + emit warning Vanishing gradient L2 &lt; 1e-7 Escalate + emit danger Exploding gradient L2 &gt; 100 Escalate + emit danger Loss spike +50% between batches Escalate + emit warning NaN / Inf anywhere Any Escalate + emit critical Weight stagnation \u0394 &lt; 1e-6 for 5 batches Escalate + emit warning Activation saturation &gt; 40% Escalate + emit warning"},{"location":"advanced/observatory-pro/#usage","title":"Usage","text":"<pre><code>from neurogebra.logging.adaptive import AdaptiveLogger, AnomalyConfig\nfrom neurogebra.logging.logger import TrainingLogger, LogLevel\n\n# Create a base logger at EXPERT level\nbase_logger = TrainingLogger(level=LogLevel.EXPERT)\n\n# Wrap it in the adaptive logger\nadaptive = AdaptiveLogger(base_logger, config=AnomalyConfig(\n    zeros_pct_threshold=50.0,      # trigger on &gt;50% dead neurons\n    gradient_spike_factor=5.0,     # trigger on 5\u00d7 gradient spike\n    escalation_cooldown=10,        # stay escalated for 10 events\n))\n\n# Use adaptive as a drop-in replacement\nadaptive.on_train_start(total_epochs=20)\nadaptive.on_epoch_start(0)\n\n# This won't produce EXPERT events (normal data):\nadaptive.on_layer_forward(0, \"dense_0\", output_data=normal_activations)\n\n# This WILL produce EXPERT events (all zeros \u2192 dead neurons):\nadaptive.on_layer_forward(0, \"dense_0\", output_data=dead_activations)\n\n# Check what anomalies were detected\nprint(adaptive.get_anomaly_summary())\n</code></pre>"},{"location":"advanced/observatory-pro/#customising-thresholds","title":"Customising Thresholds","text":"<pre><code>config = AnomalyConfig(\n    zeros_pct_threshold=30.0,          # more sensitive dead neuron detection\n    gradient_spike_factor=3.0,         # more sensitive spike detection\n    loss_spike_pct=30.0,               # trigger on 30% loss increase\n    weight_stagnation_window=10,       # look at 10 consecutive updates\n    escalation_cooldown=20,            # stay in detail mode longer\n)\nadaptive = AdaptiveLogger(base_logger, config=config)\n</code></pre>"},{"location":"advanced/observatory-pro/#2-automated-health-warnings","title":"2. Automated Health Warnings","text":"<p>The <code>AutoHealthWarnings</code> engine runs threshold-based rules on every batch and epoch, emitting structured <code>HealthWarning</code> objects with human-readable diagnoses and actionable advice.</p>"},{"location":"advanced/observatory-pro/#rules","title":"Rules","text":"Rule Condition Severity Message <code>dead_relu</code> zeros_pct &gt; 50% warning \"Possible dying ReLU in dense_0\" <code>gradient_spike</code> norm &gt; 5\u00d7 rolling avg warning \"Possible exploding gradient\" <code>vanishing_gradient</code> norm &lt; 1e-7 danger \"Vanishing gradient in dense_0\" <code>exploding_gradient</code> norm &gt; 100 danger \"Exploding gradient in dense_0\" <code>overfitting</code> val_loss / train_loss &gt; 1.3 warning \"Possible overfitting\" <code>loss_stagnation</code> \u0394loss &lt; 1e-4 for N epochs warning \"Loss stagnant\" <code>weight_stagnation</code> \u0394weight &lt; 1e-6 for N batches warning \"Optimizer may have stagnated\" <code>nan_inf_loss</code> NaN or Inf in loss critical \"NaN/Inf detected in loss!\" <code>loss_divergence</code> loss \u00d73 over N batches danger \"Loss diverging\" <code>activation_saturation</code> saturation &gt; 40% warning \"Activations saturated\""},{"location":"advanced/observatory-pro/#usage_1","title":"Usage","text":"<pre><code>from neurogebra.logging.health_warnings import AutoHealthWarnings, WarningConfig\n\nwarnings_engine = AutoHealthWarnings(config=WarningConfig(\n    dead_relu_zeros_pct=50.0,\n    overfit_patience=3,\n    overfit_ratio=1.3,\n))\n\n# Call during training\nfor epoch in range(epochs):\n    for batch_idx, (X_batch, y_batch) in enumerate(batches):\n        # ... forward/backward ...\n\n        # Check batch-level health\n        batch_alerts = warnings_engine.check_batch(\n            epoch=epoch,\n            batch=batch_idx,\n            loss=current_loss,\n            gradient_norms={\"dense_0\": 0.05, \"dense_1\": 0.03},\n            activation_stats={\"dense_0\": {\"zeros_pct\": 62.0, \"activation_type\": \"relu\"}},\n        )\n        for alert in batch_alerts:\n            print(f\"  \u26a0\ufe0f [{alert.severity}] {alert.message}\")\n\n    # Check epoch-level health\n    epoch_alerts = warnings_engine.check_epoch(\n        epoch=epoch,\n        train_loss=train_loss,\n        val_loss=val_loss,\n    )\n\n# Get summary\nprint(warnings_engine.get_summary())\n</code></pre> <p>Each <code>HealthWarning</code> contains:</p> <pre><code>HealthWarning(\n    rule_name=\"dead_relu\",\n    severity=\"warning\",\n    message=\"Possible dying ReLU in 'dense_0' (62.0% zeros)\",\n    diagnosis=\"Neurons producing zero outputs will receive zero gradients and never recover.\",\n    recommendations=[\n        \"Use LeakyReLU(negative_slope=0.01) instead of ReLU\",\n        \"Lower the learning rate\",\n        \"Use He initialisation\",\n    ],\n    layer_name=\"dense_0\",\n    epoch=5, batch=10,\n)\n</code></pre>"},{"location":"advanced/observatory-pro/#3-log-summarization-per-epoch","title":"3. Log Summarization Per Epoch","text":"<p>The <code>EpochSummarizer</code> aggregates batch-level statistics and produces mean, std, min, max across all batches in each epoch.</p>"},{"location":"advanced/observatory-pro/#usage_2","title":"Usage","text":"<pre><code>from neurogebra.logging.epoch_summary import EpochSummarizer\n\nsummarizer = EpochSummarizer()\n\nfor epoch in range(epochs):\n    for batch_idx in range(num_batches):\n        summarizer.record_batch(\n            epoch=epoch,\n            metrics={\"loss\": batch_loss, \"accuracy\": batch_acc},\n            gradient_norms={\"dense_0\": grad_norm_0, \"dense_1\": grad_norm_1},\n        )\n\n    summary = summarizer.finalize_epoch(epoch)\n    print(summary.format_text())\n</code></pre>"},{"location":"advanced/observatory-pro/#output","title":"Output","text":"<pre><code>\u2550\u2550 Epoch 5 Summary (32 batches) \u2550\u2550\n  Metrics:\n    loss                  mean=0.342100  std=0.015200  min=0.310000  max=0.380000\n    accuracy              mean=0.891200  std=0.008500  min=0.870000  max=0.910000\n  Gradient Norms:\n    dense_0               mean=5.23e-02  std=1.12e-02  min=3.10e-02  max=8.40e-02\n    dense_1               mean=2.10e-02  std=5.30e-03  min=1.20e-02  max=3.50e-02\n</code></pre>"},{"location":"advanced/observatory-pro/#programmatic-access","title":"Programmatic Access","text":"<pre><code># Get structured data\nd = summary.to_dict()\nprint(d[\"metrics\"][\"loss\"][\"mean\"])   # 0.3421\nprint(d[\"metrics\"][\"loss\"][\"std\"])    # 0.0152\n\n# Get all epoch summaries\nall_summaries = summarizer.get_all_summaries()\n</code></pre>"},{"location":"advanced/observatory-pro/#4-tiered-storage-streaming","title":"4. Tiered Storage / Streaming","text":"<p>Instead of one massive JSON file, <code>TieredStorage</code> writes three separate NDJSON (newline-delimited JSON) files:</p> File Contains When Written <code>basic.log</code> Epoch metrics, train start/end Every epoch <code>health.log</code> Warnings, anomalies, health checks On each alert (immediate) <code>debug.log</code> Full EXPERT-level detail Only when needed"},{"location":"advanced/observatory-pro/#usage_3","title":"Usage","text":"<pre><code>from neurogebra.logging.tiered_storage import TieredStorage\nfrom neurogebra.logging.logger import TrainingLogger, LogLevel\n\nstorage = TieredStorage(\n    base_dir=\"./training_logs\",\n    write_debug=True,       # set False in production to save I/O\n    buffer_size=50,          # flush every 50 events\n)\n\nlogger = TrainingLogger(level=LogLevel.EXPERT)\nlogger.add_backend(storage)\n\n# ... train as normal ...\n\nstorage.flush()    # final flush\nstorage.close()    # cleanup\n\n# Check what was written\nprint(storage.summary())\n# {'basic': {'events': 42, 'size_bytes': 8192},\n#  'health': {'events': 3, 'size_bytes': 1024},\n#  'debug': {'events': 12500, 'size_bytes': 2097152},\n#  'total_events': 12545}\n</code></pre>"},{"location":"advanced/observatory-pro/#reading-logs","title":"Reading Logs","text":"<pre><code># Easy to grep through specific tiers\nbasic_events = storage.read_basic()\nhealth_events = storage.read_health()\n\n# Or from command line:\n# grep \"overfitting\" training_logs/health.log\n# grep \"dense_0\" training_logs/debug.log\n</code></pre>"},{"location":"advanced/observatory-pro/#ndjson-format","title":"NDJSON Format","text":"<p>Each line is a self-contained JSON object \u2014 easy to stream, grep, and parse:</p> <pre><code>{\"event_type\":\"epoch_end\",\"level\":\"BASIC\",\"timestamp\":1740000000.0,\"epoch\":0,\"severity\":\"info\",\"message\":\"Epoch 1 done\",\"data\":{\"metrics\":{\"loss\":0.85,\"accuracy\":0.72}}}\n{\"event_type\":\"epoch_end\",\"level\":\"BASIC\",\"timestamp\":1740000001.5,\"epoch\":1,\"severity\":\"info\",\"message\":\"Epoch 2 done\",\"data\":{\"metrics\":{\"loss\":0.63,\"accuracy\":0.81}}}\n</code></pre>"},{"location":"advanced/observatory-pro/#5-visual-dashboard","title":"5. Visual Dashboard","text":"<p>The <code>DashboardExporter</code> generates a self-contained interactive HTML dashboard with Chart.js charts.</p>"},{"location":"advanced/observatory-pro/#charts-included","title":"Charts Included","text":"<ul> <li>\ud83d\udcc9 Loss curves (train + validation)</li> <li>\ud83d\udcc8 Accuracy curves (train + validation)</li> <li>\u23f1\ufe0f Epoch timing bar chart</li> <li>\ud83d\udcca Raw batch-level loss curve</li> <li>\ud83e\ude7a Health diagnostics timeline</li> </ul>"},{"location":"advanced/observatory-pro/#usage_4","title":"Usage","text":"<pre><code>from neurogebra.logging.dashboard import DashboardExporter\nfrom neurogebra.logging.logger import TrainingLogger, LogLevel\n\ndashboard = DashboardExporter(path=\"training_logs/dashboard.html\")\nlogger = TrainingLogger(level=LogLevel.EXPERT)\nlogger.add_backend(dashboard)\n\n# ... train as normal ...\n\ndashboard.save()  # generates the interactive HTML file\n# Open training_logs/dashboard.html in any browser\n</code></pre>"},{"location":"advanced/observatory-pro/#tensorboard-integration","title":"TensorBoard Integration","text":"<pre><code>from neurogebra.logging.dashboard import TensorBoardBridge\n\ntb = TensorBoardBridge(log_dir=\"./tb_logs\")\nif tb.available:\n    logger.add_backend(tb)\n    # ... after training ...\n    tb.close()\n    # Then: tensorboard --logdir=./tb_logs\n</code></pre>"},{"location":"advanced/observatory-pro/#weights-biases-integration","title":"Weights &amp; Biases Integration","text":"<pre><code>from neurogebra.logging.dashboard import WandBBridge\n\nwandb_bridge = WandBBridge(\n    project=\"my_experiment\",\n    run_name=\"experiment_001\",\n    config={\"lr\": 0.01, \"epochs\": 50},\n)\nif wandb_bridge.available:\n    logger.add_backend(wandb_bridge)\n    # ... after training ...\n    wandb_bridge.close()\n</code></pre>"},{"location":"advanced/observatory-pro/#6-training-fingerprint-reproducibility-block","title":"6. Training Fingerprint / Reproducibility Block","text":"<p>The <code>TrainingFingerprint</code> captures everything needed to reproduce a training run:</p>"},{"location":"advanced/observatory-pro/#what-it-captures","title":"What It Captures","text":"Category Fields Seeds random_seed, numpy_seed Dataset SHA-256 hash, shape, dtype, sample count Versions Neurogebra, Python, NumPy, SciPy, SymPy, Rich Hardware CPU model, core count, RAM, GPU (if available) OS System, release, machine architecture Model Architecture hash, full model info dict Hyperparameters All training hyperparameters Git Commit hash, branch name, dirty status"},{"location":"advanced/observatory-pro/#usage_5","title":"Usage","text":"<pre><code>from neurogebra.logging.fingerprint import TrainingFingerprint\nimport numpy as np\n\nfingerprint = TrainingFingerprint.capture(\n    model_info={\"name\": \"my_model\", \"layers\": [...]},\n    hyperparameters={\"lr\": 0.01, \"batch_size\": 32, \"epochs\": 50},\n    dataset=X_train,        # auto-hashed\n    random_seed=42,\n)\n\n# Pretty-print\nprint(fingerprint.format_text())\n</code></pre>"},{"location":"advanced/observatory-pro/#output_1","title":"Output","text":"<pre><code>\u2554\u2550\u2550 Training Fingerprint \u2550\u2550\u2557\n  Run ID:       a1b2c3d4e5f6\n  Timestamp:    2026-02-27 14:30:00\n  Seed:         42\n  Dataset Hash: 8f14e45fceea167a\n  Dataset:      (10000, 784) (float64)\n  Neurogebra:   1.3.0\n  Python:       3.11.5\n  NumPy:        1.26.0\n  CPU:          AMD64 Family (8 cores)\n  RAM:          16.0 GB\n  GPU:          NVIDIA GeForce RTX 3060\n  OS:           Windows 10\n  Git:          main@a1b2c3d4 (dirty)\n  Model Hash:   f47ac10b58cc\n  Hyperparams:  {'lr': 0.01, 'batch_size': 32, 'epochs': 50}\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre>"},{"location":"advanced/observatory-pro/#serialisation","title":"Serialisation","text":"<pre><code># Save to JSON\nimport json\nwith open(\"fingerprint.json\", \"w\") as f:\n    json.dump(fingerprint.to_dict(), f, indent=2)\n\n# Load back\nwith open(\"fingerprint.json\") as f:\n    fp2 = TrainingFingerprint.from_dict(json.load(f))\n</code></pre>"},{"location":"advanced/observatory-pro/#full-integration-example","title":"Full Integration Example","text":"<p>Using all v1.3.0 features together:</p> <pre><code>from neurogebra.builders.model_builder import ModelBuilder\nfrom neurogebra.logging.adaptive import AdaptiveLogger, AnomalyConfig\nfrom neurogebra.logging.health_warnings import AutoHealthWarnings\nfrom neurogebra.logging.epoch_summary import EpochSummarizer\nfrom neurogebra.logging.tiered_storage import TieredStorage\nfrom neurogebra.logging.dashboard import DashboardExporter\nfrom neurogebra.logging.fingerprint import TrainingFingerprint\nfrom neurogebra.logging.logger import TrainingLogger, LogLevel\nimport numpy as np\n\n# 1. Build model\nbuilder = ModelBuilder()\nmodel = builder.Sequential([\n    builder.Dense(64, activation=\"relu\"),\n    builder.Dense(32, activation=\"tanh\"),\n    builder.Dense(1, activation=\"sigmoid\"),\n], name=\"my_model\")\n\n# 2. Create logging pipeline\nbase_logger = TrainingLogger(level=LogLevel.EXPERT)\nadaptive = AdaptiveLogger(base_logger)              # Smart filtering\nstorage = TieredStorage(base_dir=\"./logs\")           # Tiered files\ndashboard = DashboardExporter(path=\"./logs/dash.html\")  # Visual dashboard\nbase_logger.add_backend(storage)\nbase_logger.add_backend(dashboard)\n\nwarnings = AutoHealthWarnings()                      # Auto health rules\nsummarizer = EpochSummarizer()                       # Epoch aggregation\n\n# 3. Capture fingerprint\nfp = TrainingFingerprint.capture(\n    model_info={\"name\": \"my_model\", \"layers\": 3},\n    hyperparameters={\"lr\": 0.01, \"batch_size\": 32, \"epochs\": 20},\n    dataset=X_train,\n    random_seed=42,\n)\nprint(fp.format_text())\n\n# 4. Train with full diagnostics\nadaptive.on_train_start(total_epochs=20, model_info=fp.model_info)\nfor epoch in range(20):\n    adaptive.on_epoch_start(epoch)\n    for batch in range(num_batches):\n        # ... training step ...\n        summarizer.record_batch(epoch=epoch, metrics={\"loss\": loss})\n        warnings.check_batch(loss=loss, epoch=epoch, batch=batch)\n\n    summary = summarizer.finalize_epoch(epoch)\n    print(summary.format_text())\n    warnings.check_epoch(epoch=epoch, train_loss=train_loss, val_loss=val_loss)\n    adaptive.on_epoch_end(epoch, metrics={\"loss\": train_loss})\n\nadaptive.on_train_end()\n\n# 5. Save everything\nstorage.close()\ndashboard.save()\nprint(f\"Anomalies detected: {adaptive.get_anomaly_summary()['total_anomalies']}\")\nprint(f\"Health warnings: {warnings.get_summary()['total_warnings']}\")\n</code></pre>"},{"location":"advanced/observatory-pro/#api-reference","title":"API Reference","text":""},{"location":"advanced/observatory-pro/#adaptivelogger","title":"<code>AdaptiveLogger</code>","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.adaptive.AdaptiveLogger","title":"<code>neurogebra.logging.adaptive.AdaptiveLogger</code>","text":"<p>Wraps a :class:<code>TrainingLogger</code> and filters events adaptively.</p> <p>In normal mode only BASIC-level events are emitted. When an anomaly is detected the logger temporarily escalates to EXPERT for <code>escalation_cooldown</code> events, so the user gets the full picture around the anomaly without drowning in noise the rest of the time.</p> <p>The underlying <code>TrainingLogger</code> must be created with <code>level=LogLevel.EXPERT</code> (or higher) so it can emit the detailed events when the adaptive logger un-mutes them.</p> Source code in <code>src/neurogebra/logging/adaptive.py</code> <pre><code>class AdaptiveLogger:\n    \"\"\"\n    Wraps a :class:`TrainingLogger` and filters events adaptively.\n\n    In **normal** mode only BASIC-level events are emitted.\n    When an anomaly is detected the logger temporarily escalates to EXPERT\n    for ``escalation_cooldown`` events, so the user gets the full picture\n    around the anomaly without drowning in noise the rest of the time.\n\n    The underlying ``TrainingLogger`` must be created with\n    ``level=LogLevel.EXPERT`` (or higher) so it *can* emit the detailed\n    events when the adaptive logger un-mutes them.\n    \"\"\"\n\n    def __init__(\n        self,\n        base_logger: TrainingLogger,\n        config: Optional[AnomalyConfig] = None,\n    ):\n        self._base = base_logger\n        self.config = config or AnomalyConfig()\n\n        # Ensure the base logger will accept EXPERT events\n        if self._base.level &lt; LogLevel.EXPERT:\n            self._base.level = LogLevel.EXPERT\n\n        # Rolling state\n        self._gradient_norms: Dict[str, Deque[float]] = {}\n        self._last_batch_loss: Optional[float] = None\n        self._weight_deltas: Dict[str, Deque[float]] = {}\n        self._anomalies: List[AnomalyRecord] = []\n\n        # Escalation bookkeeping\n        self._escalated = False\n        self._escalation_counter = 0\n\n        # Shadow level: the level we *pretend* the logger is at\n        self._effective_level = LogLevel.BASIC\n\n    # ------------------------------------------------------------------\n    # Public API \u2014 mirrors TrainingLogger\n    # ------------------------------------------------------------------\n\n    @property\n    def anomalies(self) -&gt; List[AnomalyRecord]:\n        \"\"\"Return all detected anomalies so far.\"\"\"\n        return list(self._anomalies)\n\n    @property\n    def is_escalated(self) -&gt; bool:\n        return self._escalated\n\n    # Delegate attribute access to the base logger for anything not overridden\n    def __getattr__(self, name: str):\n        return getattr(self._base, name)\n\n    # -- train lifecycle --------------------------------------------------\n\n    def on_train_start(self, **kwargs) -&gt; None:\n        self._base.on_train_start(**kwargs)\n\n    def on_train_end(self, **kwargs) -&gt; None:\n        self._base.on_train_end(**kwargs)\n\n    def on_epoch_start(self, epoch: int, **kwargs) -&gt; None:\n        self._base.on_epoch_start(epoch, **kwargs)\n\n    def on_epoch_end(self, epoch: int, **kwargs) -&gt; None:\n        self._base.on_epoch_end(epoch, **kwargs)\n\n    def on_batch_start(self, batch: int, **kwargs) -&gt; None:\n        self._base.on_batch_start(batch, **kwargs)\n\n    def on_batch_end(self, batch: int, **kwargs) -&gt; None:\n        loss = kwargs.get(\"loss\")\n        if loss is not None:\n            self._check_loss_spike(loss, kwargs.get(\"epoch\"), batch)\n            self._last_batch_loss = loss\n        self._base.on_batch_end(batch, **kwargs)\n\n    # -- layer-level (gated) -----------------------------------------------\n\n    def on_layer_forward(self, layer_index: int, layer_name: str, **kwargs) -&gt; None:\n        \"\"\"Only emit EXPERT-level layer_forward when escalated or anomalous.\"\"\"\n        anomaly = self._check_forward_anomaly(layer_name, kwargs)\n        if anomaly or self._escalated:\n            self._base.on_layer_forward(layer_index, layer_name, **kwargs)\n        # else: silently skip\n\n    def on_layer_backward(self, layer_index: int, layer_name: str, **kwargs) -&gt; None:\n        anomaly = self._check_backward_anomaly(layer_name, kwargs)\n        if anomaly or self._escalated:\n            self._base.on_layer_backward(layer_index, layer_name, **kwargs)\n\n    def on_gradient_computed(self, param_name: str, gradient: float, **kwargs) -&gt; None:\n        anomaly = self._check_gradient_anomaly(param_name, gradient)\n        if anomaly or self._escalated:\n            self._base.on_gradient_computed(param_name, gradient, **kwargs)\n\n    def on_weight_updated(self, param_name: str, old_value: float,\n                          new_value: float, **kwargs) -&gt; None:\n        anomaly = self._check_weight_stagnation(param_name, old_value, new_value)\n        if anomaly or self._escalated:\n            self._base.on_weight_updated(param_name, old_value, new_value, **kwargs)\n\n    def on_health_check(self, *args, **kwargs) -&gt; None:\n        self._base.on_health_check(*args, **kwargs)\n\n    # ------------------------------------------------------------------\n    # Anomaly detection helpers\n    # ------------------------------------------------------------------\n\n    def _flag_anomaly(self, record: AnomalyRecord) -&gt; None:\n        \"\"\"Register an anomaly and enter escalated mode.\"\"\"\n        self._anomalies.append(record)\n        self._escalated = True\n        self._escalation_counter = self.config.escalation_cooldown\n\n        # Also emit a health-check event\n        self._base.on_health_check(\n            check_name=f\"adaptive_{record.anomaly_type}\",\n            severity=record.severity,\n            message=record.message,\n            recommendations=[],\n            anomaly_data=record.data,\n        )\n\n    def _tick_escalation(self) -&gt; None:\n        \"\"\"Count down the escalation cooldown after each gated event.\"\"\"\n        if self._escalated:\n            self._escalation_counter -= 1\n            if self._escalation_counter &lt;= 0:\n                self._escalated = False\n\n    # -- forward checks ---------------------------------------------------\n\n    def _check_forward_anomaly(self, layer_name: str, kwargs: Dict) -&gt; bool:\n        self._tick_escalation()\n\n        output_data = kwargs.get(\"output_data\")\n        if output_data is not None:\n            arr = np.asarray(output_data, dtype=np.float64)\n\n            # NaN / Inf\n            if np.any(np.isnan(arr)) or np.any(np.isinf(arr)):\n                self._flag_anomaly(AnomalyRecord(\n                    anomaly_type=\"nan_inf_activation\",\n                    layer_name=layer_name,\n                    severity=\"critical\",\n                    message=f\"NaN/Inf detected in activations of '{layer_name}'\",\n                    data={\"nan_count\": int(np.sum(np.isnan(arr))),\n                          \"inf_count\": int(np.sum(np.isinf(arr)))},\n                ))\n                return True\n\n            # Dead neurons\n            flat = arr.ravel()\n            zeros_pct = float(np.sum(flat == 0) / max(flat.size, 1) * 100)\n            if zeros_pct &gt; self.config.zeros_pct_threshold:\n                self._flag_anomaly(AnomalyRecord(\n                    anomaly_type=\"dead_neurons\",\n                    layer_name=layer_name,\n                    severity=\"warning\",\n                    message=(f\"{zeros_pct:.1f}% zeros in '{layer_name}' \"\n                             f\"\u2014 possible dying ReLU\"),\n                    data={\"zeros_pct\": zeros_pct},\n                ))\n                return True\n\n            # Saturation (sigmoid/tanh activations mostly \u2208 (0,1) or (-1,1))\n            if flat.size &gt; 0:\n                sat_low = float(np.sum(np.abs(flat) &lt; 0.01) / flat.size * 100)\n                sat_high = float(np.sum(np.abs(flat) &gt; 0.99) / flat.size * 100)\n                sat_total = sat_low + sat_high\n                if sat_total &gt; self.config.saturation_threshold:\n                    self._flag_anomaly(AnomalyRecord(\n                        anomaly_type=\"activation_saturation\",\n                        layer_name=layer_name,\n                        severity=\"warning\",\n                        message=(f\"{sat_total:.1f}% activations saturated \"\n                                 f\"in '{layer_name}'\"),\n                        data={\"saturation_pct\": sat_total},\n                    ))\n                    return True\n\n        return False\n\n    # -- backward checks --------------------------------------------------\n\n    def _check_backward_anomaly(self, layer_name: str, kwargs: Dict) -&gt; bool:\n        self._tick_escalation()\n        grad_output = kwargs.get(\"grad_output\")\n        if grad_output is not None:\n            arr = np.asarray(grad_output, dtype=np.float64)\n            if np.any(np.isnan(arr)) or np.any(np.isinf(arr)):\n                self._flag_anomaly(AnomalyRecord(\n                    anomaly_type=\"nan_inf_gradient\",\n                    layer_name=layer_name,\n                    severity=\"critical\",\n                    message=f\"NaN/Inf in gradient output of '{layer_name}'\",\n                ))\n                return True\n        return False\n\n    # -- gradient norm checks ---------------------------------------------\n\n    def _check_gradient_anomaly(self, param_name: str, gradient: float) -&gt; bool:\n        self._tick_escalation()\n        g = abs(gradient)\n\n        # Absolute thresholds\n        if g &lt; self.config.gradient_vanish_threshold:\n            self._flag_anomaly(AnomalyRecord(\n                anomaly_type=\"vanishing_gradient\",\n                layer_name=param_name,\n                severity=\"danger\",\n                message=f\"Vanishing gradient for '{param_name}' (|g|={g:.2e})\",\n                data={\"gradient\": gradient},\n            ))\n            return True\n\n        if g &gt; self.config.gradient_explode_threshold:\n            self._flag_anomaly(AnomalyRecord(\n                anomaly_type=\"exploding_gradient\",\n                layer_name=param_name,\n                severity=\"danger\",\n                message=f\"Exploding gradient for '{param_name}' (|g|={g:.2e})\",\n                data={\"gradient\": gradient},\n            ))\n            return True\n\n        # Spike detection\n        buf = self._gradient_norms.setdefault(\n            param_name, deque(maxlen=self.config.gradient_rolling_window)\n        )\n        if len(buf) &gt;= 3:\n            rolling_mean = float(np.mean(buf))\n            if rolling_mean &gt; 0 and g &gt; rolling_mean * self.config.gradient_spike_factor:\n                self._flag_anomaly(AnomalyRecord(\n                    anomaly_type=\"gradient_spike\",\n                    layer_name=param_name,\n                    severity=\"warning\",\n                    message=(f\"Gradient spike in '{param_name}': \"\n                             f\"{g:.2e} vs rolling avg {rolling_mean:.2e}\"),\n                    data={\"gradient\": gradient, \"rolling_mean\": rolling_mean},\n                ))\n                buf.append(g)\n                return True\n        buf.append(g)\n        return False\n\n    # -- loss spike -------------------------------------------------------\n\n    def _check_loss_spike(self, loss: float, epoch: Optional[int],\n                          batch: int) -&gt; bool:\n        if self._last_batch_loss is not None and self._last_batch_loss &gt; 0:\n            pct_increase = (loss - self._last_batch_loss) / self._last_batch_loss * 100\n            if pct_increase &gt; self.config.loss_spike_pct:\n                self._flag_anomaly(AnomalyRecord(\n                    anomaly_type=\"loss_spike\",\n                    epoch=epoch,\n                    batch=batch,\n                    severity=\"warning\",\n                    message=(f\"Loss spiked by {pct_increase:.1f}% \"\n                             f\"({self._last_batch_loss:.4f} \u2192 {loss:.4f})\"),\n                    data={\"prev_loss\": self._last_batch_loss, \"new_loss\": loss,\n                          \"pct_increase\": pct_increase},\n                ))\n                return True\n        return False\n\n    # -- weight stagnation ------------------------------------------------\n\n    def _check_weight_stagnation(self, param_name: str, old: float,\n                                 new: float) -&gt; bool:\n        self._tick_escalation()\n        delta = abs(new - old)\n        buf = self._weight_deltas.setdefault(\n            param_name, deque(maxlen=self.config.weight_stagnation_window)\n        )\n        buf.append(delta)\n        if len(buf) &gt;= self.config.weight_stagnation_window:\n            if all(d &lt; self.config.weight_stagnation_threshold for d in buf):\n                self._flag_anomaly(AnomalyRecord(\n                    anomaly_type=\"weight_stagnation\",\n                    layer_name=param_name,\n                    severity=\"warning\",\n                    message=(f\"Weight '{param_name}' stagnant for \"\n                             f\"{self.config.weight_stagnation_window} updates \"\n                             f\"(max \u0394={max(buf):.2e})\"),\n                    data={\"max_delta\": float(max(buf))},\n                ))\n                return True\n        return False\n\n    # ------------------------------------------------------------------\n    # Utilities\n    # ------------------------------------------------------------------\n\n    def get_anomaly_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Return a structured summary of all detected anomalies.\"\"\"\n        by_type: Dict[str, int] = {}\n        by_severity: Dict[str, int] = {}\n        for a in self._anomalies:\n            by_type[a.anomaly_type] = by_type.get(a.anomaly_type, 0) + 1\n            by_severity[a.severity] = by_severity.get(a.severity, 0) + 1\n        return {\n            \"total_anomalies\": len(self._anomalies),\n            \"by_type\": by_type,\n            \"by_severity\": by_severity,\n            \"anomalies\": [\n                {\n                    \"type\": a.anomaly_type,\n                    \"severity\": a.severity,\n                    \"message\": a.message,\n                    \"layer\": a.layer_name,\n                    \"epoch\": a.epoch,\n                    \"batch\": a.batch,\n                    \"timestamp\": a.timestamp,\n                }\n                for a in self._anomalies\n            ],\n        }\n\n    def reset(self) -&gt; None:\n        \"\"\"Clear all anomaly state and go back to BASIC mode.\"\"\"\n        self._anomalies.clear()\n        self._gradient_norms.clear()\n        self._weight_deltas.clear()\n        self._last_batch_loss = None\n        self._escalated = False\n        self._escalation_counter = 0\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.adaptive.AdaptiveLogger-attributes","title":"Attributes","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.adaptive.AdaptiveLogger.anomalies","title":"<code>anomalies</code>  <code>property</code>","text":"<p>Return all detected anomalies so far.</p>"},{"location":"advanced/observatory-pro/#neurogebra.logging.adaptive.AdaptiveLogger-functions","title":"Functions","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.adaptive.AdaptiveLogger.on_layer_forward","title":"<code>on_layer_forward(layer_index, layer_name, **kwargs)</code>","text":"<p>Only emit EXPERT-level layer_forward when escalated or anomalous.</p> Source code in <code>src/neurogebra/logging/adaptive.py</code> <pre><code>def on_layer_forward(self, layer_index: int, layer_name: str, **kwargs) -&gt; None:\n    \"\"\"Only emit EXPERT-level layer_forward when escalated or anomalous.\"\"\"\n    anomaly = self._check_forward_anomaly(layer_name, kwargs)\n    if anomaly or self._escalated:\n        self._base.on_layer_forward(layer_index, layer_name, **kwargs)\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.adaptive.AdaptiveLogger.get_anomaly_summary","title":"<code>get_anomaly_summary()</code>","text":"<p>Return a structured summary of all detected anomalies.</p> Source code in <code>src/neurogebra/logging/adaptive.py</code> <pre><code>def get_anomaly_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"Return a structured summary of all detected anomalies.\"\"\"\n    by_type: Dict[str, int] = {}\n    by_severity: Dict[str, int] = {}\n    for a in self._anomalies:\n        by_type[a.anomaly_type] = by_type.get(a.anomaly_type, 0) + 1\n        by_severity[a.severity] = by_severity.get(a.severity, 0) + 1\n    return {\n        \"total_anomalies\": len(self._anomalies),\n        \"by_type\": by_type,\n        \"by_severity\": by_severity,\n        \"anomalies\": [\n            {\n                \"type\": a.anomaly_type,\n                \"severity\": a.severity,\n                \"message\": a.message,\n                \"layer\": a.layer_name,\n                \"epoch\": a.epoch,\n                \"batch\": a.batch,\n                \"timestamp\": a.timestamp,\n            }\n            for a in self._anomalies\n        ],\n    }\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.adaptive.AdaptiveLogger.reset","title":"<code>reset()</code>","text":"<p>Clear all anomaly state and go back to BASIC mode.</p> Source code in <code>src/neurogebra/logging/adaptive.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Clear all anomaly state and go back to BASIC mode.\"\"\"\n    self._anomalies.clear()\n    self._gradient_norms.clear()\n    self._weight_deltas.clear()\n    self._last_batch_loss = None\n    self._escalated = False\n    self._escalation_counter = 0\n</code></pre>"},{"location":"advanced/observatory-pro/#anomalyconfig","title":"<code>AnomalyConfig</code>","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.adaptive.AnomalyConfig","title":"<code>neurogebra.logging.adaptive.AnomalyConfig</code>  <code>dataclass</code>","text":"<p>Thresholds that trigger escalation from BASIC \u2192 EXPERT logging.</p> Source code in <code>src/neurogebra/logging/adaptive.py</code> <pre><code>@dataclass\nclass AnomalyConfig:\n    \"\"\"Thresholds that trigger escalation from BASIC \u2192 EXPERT logging.\"\"\"\n\n    # Dead neuron / zero activation threshold (percent)\n    zeros_pct_threshold: float = 50.0\n\n    # Gradient spike: current norm &gt; rolling_mean \u00d7 spike_factor\n    gradient_spike_factor: float = 5.0\n    gradient_rolling_window: int = 20\n\n    # Gradient absolute thresholds\n    gradient_vanish_threshold: float = 1e-7\n    gradient_explode_threshold: float = 100.0\n\n    # Loss spike between consecutive batches (percent increase)\n    loss_spike_pct: float = 50.0\n\n    # Activation saturation threshold (percent)\n    saturation_threshold: float = 40.0\n\n    # Weight delta near-zero (consecutive batches)\n    weight_stagnation_threshold: float = 1e-6\n    weight_stagnation_window: int = 5\n\n    # How many events to keep in \"escalated\" mode after an anomaly\n    escalation_cooldown: int = 10\n</code></pre>"},{"location":"advanced/observatory-pro/#autohealthwarnings","title":"<code>AutoHealthWarnings</code>","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.health_warnings.AutoHealthWarnings","title":"<code>neurogebra.logging.health_warnings.AutoHealthWarnings</code>","text":"<p>Stateful warning engine that tracks training metrics over time and fires threshold-based rules automatically.</p> <p>Attach to a training loop and call :meth:<code>check_batch</code> / :meth:<code>check_epoch</code> each iteration.  Accumulated warnings are accessible via :attr:<code>warnings</code>.</p> Source code in <code>src/neurogebra/logging/health_warnings.py</code> <pre><code>class AutoHealthWarnings:\n    \"\"\"\n    Stateful warning engine that tracks training metrics over time\n    and fires threshold-based rules automatically.\n\n    Attach to a training loop and call :meth:`check_batch` /\n    :meth:`check_epoch` each iteration.  Accumulated warnings are\n    accessible via :attr:`warnings`.\n    \"\"\"\n\n    def __init__(self, config: Optional[WarningConfig] = None):\n        self.config = config or WarningConfig()\n\n        # Rolling state\n        self._gradient_norms: Dict[str, Deque[float]] = {}\n        self._weight_deltas: Dict[str, Deque[float]] = {}\n        self._train_losses: List[float] = []\n        self._val_losses: List[float] = []\n        self._batch_losses: Deque[float] = deque(maxlen=100)\n\n        # Collected warnings\n        self._warnings: List[HealthWarning] = []\n\n        # Dedup: avoid spamming the same warning every batch\n        self._fired_rules: Dict[str, float] = {}  # rule_key \u2192 last-fired timestamp\n        self._dedup_interval = 30.0  # seconds\n\n    @property\n    def warnings(self) -&gt; List[HealthWarning]:\n        return list(self._warnings)\n\n    # ------------------------------------------------------------------\n    # Per-batch check\n    # ------------------------------------------------------------------\n\n    def check_batch(\n        self,\n        *,\n        epoch: Optional[int] = None,\n        batch: Optional[int] = None,\n        loss: Optional[float] = None,\n        gradient_norms: Optional[Dict[str, float]] = None,\n        weight_stats: Optional[Dict[str, Dict[str, Any]]] = None,\n        activation_stats: Optional[Dict[str, Dict[str, Any]]] = None,\n        weight_deltas: Optional[Dict[str, float]] = None,\n    ) -&gt; List[HealthWarning]:\n        \"\"\"Run all batch-level rules and return new warnings.\"\"\"\n        new: List[HealthWarning] = []\n\n        if loss is not None:\n            self._batch_losses.append(loss)\n            new.extend(self._check_nan_inf_loss(loss, epoch, batch))\n            new.extend(self._check_loss_divergence(epoch, batch))\n\n        if gradient_norms:\n            new.extend(self._check_gradients(gradient_norms, epoch, batch))\n\n        if activation_stats:\n            new.extend(self._check_activations(activation_stats, epoch, batch))\n\n        if weight_stats:\n            new.extend(self._check_dead_weights(weight_stats, epoch, batch))\n\n        if weight_deltas:\n            new.extend(self._check_weight_stagnation(weight_deltas, epoch, batch))\n\n        self._warnings.extend(new)\n        return new\n\n    # ------------------------------------------------------------------\n    # Per-epoch check\n    # ------------------------------------------------------------------\n\n    def check_epoch(\n        self,\n        *,\n        epoch: int,\n        train_loss: Optional[float] = None,\n        val_loss: Optional[float] = None,\n        train_acc: Optional[float] = None,\n        val_acc: Optional[float] = None,\n        gradient_norms: Optional[Dict[str, float]] = None,\n        weight_stats: Optional[Dict[str, Dict[str, Any]]] = None,\n        activation_stats: Optional[Dict[str, Dict[str, Any]]] = None,\n    ) -&gt; List[HealthWarning]:\n        \"\"\"Run all epoch-level rules and return new warnings.\"\"\"\n        new: List[HealthWarning] = []\n\n        if train_loss is not None:\n            self._train_losses.append(train_loss)\n        if val_loss is not None:\n            self._val_losses.append(val_loss)\n\n        # Overfitting check\n        new.extend(self._check_overfitting(epoch))\n\n        # Loss stagnation\n        new.extend(self._check_loss_stagnation(epoch))\n\n        # Gradient checks (epoch-level too)\n        if gradient_norms:\n            new.extend(self._check_gradients(gradient_norms, epoch, None))\n\n        # Activation / weight checks\n        if activation_stats:\n            new.extend(self._check_activations(activation_stats, epoch, None))\n        if weight_stats:\n            new.extend(self._check_dead_weights(weight_stats, epoch, None))\n\n        self._warnings.extend(new)\n        return new\n\n    # ------------------------------------------------------------------\n    # Rule implementations\n    # ------------------------------------------------------------------\n\n    def _should_fire(self, rule_key: str) -&gt; bool:\n        \"\"\"De-duplicate: don't fire the same rule twice within the interval.\"\"\"\n        now = time.time()\n        last = self._fired_rules.get(rule_key)\n        if last is not None and (now - last) &lt; self._dedup_interval:\n            return False\n        self._fired_rules[rule_key] = now\n        return True\n\n    # -- NaN / Inf --------------------------------------------------------\n\n    def _check_nan_inf_loss(self, loss: float, epoch, batch) -&gt; List[HealthWarning]:\n        if not (np.isnan(loss) or np.isinf(loss)):\n            return []\n        key = \"nan_inf_loss\"\n        if not self._should_fire(key):\n            return []\n        return [HealthWarning(\n            rule_name=\"nan_inf_loss\",\n            severity=\"critical\",\n            message=\"NaN/Inf detected in loss!\",\n            diagnosis=(\n                \"Numerical instability has corrupted the loss. \"\n                \"Training should be stopped immediately.\"\n            ),\n            recommendations=[\n                \"Lower the learning rate (current may be too high)\",\n                \"Add gradient clipping (max_norm=1.0)\",\n                \"Check input data for NaN/Inf values\",\n                \"Use a more numerically stable loss function\",\n            ],\n            epoch=epoch, batch=batch,\n            data={\"loss\": float(loss) if np.isfinite(loss) else str(loss)},\n        )]\n\n    # -- Loss divergence --------------------------------------------------\n\n    def _check_loss_divergence(self, epoch, batch) -&gt; List[HealthWarning]:\n        w = self.config.loss_divergence_window\n        if len(self._batch_losses) &lt; w:\n            return []\n        recent = list(self._batch_losses)[-w:]\n        if recent[-1] &gt; recent[0] * self.config.lr_too_high_loss_factor:\n            key = \"loss_divergence\"\n            if not self._should_fire(key):\n                return []\n            return [HealthWarning(\n                rule_name=\"loss_divergence\",\n                severity=\"danger\",\n                message=f\"Loss diverging over last {w} batches\",\n                diagnosis=(\n                    \"The loss is increasing rapidly, indicating training instability.\"\n                ),\n                recommendations=[\n                    \"Immediately lower the learning rate\",\n                    \"Add gradient clipping\",\n                    \"Check data preprocessing (normalise inputs)\",\n                ],\n                epoch=epoch, batch=batch,\n                data={\"recent_losses\": recent},\n            )]\n        return []\n\n    # -- Gradient checks --------------------------------------------------\n\n    def _check_gradients(self, gradient_norms: Dict[str, float],\n                         epoch, batch) -&gt; List[HealthWarning]:\n        alerts: List[HealthWarning] = []\n        cfg = self.config\n\n        for layer, norm in gradient_norms.items():\n            # NaN/Inf\n            if np.isnan(norm) or np.isinf(norm):\n                key = f\"gradient_nan_{layer}\"\n                if self._should_fire(key):\n                    alerts.append(HealthWarning(\n                        rule_name=\"gradient_nan_inf\",\n                        severity=\"critical\",\n                        message=f\"NaN/Inf gradient in '{layer}'\",\n                        diagnosis=\"Gradient corruption prevents learning.\",\n                        recommendations=[\n                            \"Lower the learning rate\",\n                            \"Add gradient clipping (max_norm=1.0)\",\n                            \"Use batch normalisation before this layer\",\n                        ],\n                        layer_name=layer, epoch=epoch, batch=batch,\n                    ))\n                continue\n\n            # Vanishing\n            if norm &lt; cfg.gradient_vanish_thresh:\n                key = f\"gradient_vanish_{layer}\"\n                if self._should_fire(key):\n                    alerts.append(HealthWarning(\n                        rule_name=\"vanishing_gradient\",\n                        severity=\"danger\",\n                        message=f\"Vanishing gradient in '{layer}' (norm={norm:.2e})\",\n                        diagnosis=\"Gradients too small \u2014 this layer is effectively frozen.\",\n                        recommendations=[\n                            \"Switch to ReLU or LeakyReLU activation\",\n                            \"Use batch normalisation\",\n                            \"Try skip connections (ResNet-style)\",\n                        ],\n                        layer_name=layer, epoch=epoch, batch=batch,\n                        data={\"norm\": norm},\n                    ))\n\n            # Exploding\n            if norm &gt; cfg.gradient_explode_thresh:\n                key = f\"gradient_explode_{layer}\"\n                if self._should_fire(key):\n                    alerts.append(HealthWarning(\n                        rule_name=\"exploding_gradient\",\n                        severity=\"danger\",\n                        message=f\"Exploding gradient in '{layer}' (norm={norm:.2e})\",\n                        diagnosis=\"Excessively large gradients cause unstable weight updates.\",\n                        recommendations=[\n                            \"Add gradient clipping (max_norm=1.0)\",\n                            \"Lower the learning rate\",\n                            \"Use batch normalisation\",\n                        ],\n                        layer_name=layer, epoch=epoch, batch=batch,\n                        data={\"norm\": norm},\n                    ))\n\n            # Spike\n            buf = self._gradient_norms.setdefault(\n                layer, deque(maxlen=cfg.gradient_rolling_window))\n            if len(buf) &gt;= 3:\n                rolling_mean = float(np.mean(buf))\n                if rolling_mean &gt; 0 and norm &gt; rolling_mean * cfg.gradient_spike_factor:\n                    key = f\"gradient_spike_{layer}\"\n                    if self._should_fire(key):\n                        alerts.append(HealthWarning(\n                            rule_name=\"gradient_spike\",\n                            severity=\"warning\",\n                            message=(f\"Possible exploding gradient in '{layer}': \"\n                                     f\"norm {norm:.2e} vs rolling avg {rolling_mean:.2e}\"),\n                            diagnosis=\"A sudden gradient spike may indicate instability.\",\n                            recommendations=[\n                                \"Add gradient clipping\",\n                                \"Reduce learning rate temporarily\",\n                                \"Check for outlier data in the current batch\",\n                            ],\n                            layer_name=layer, epoch=epoch, batch=batch,\n                            data={\"norm\": norm, \"rolling_mean\": rolling_mean},\n                        ))\n            buf.append(norm)\n\n        return alerts\n\n    # -- Activation checks ------------------------------------------------\n\n    def _check_activations(self, activation_stats: Dict[str, Dict],\n                           epoch, batch) -&gt; List[HealthWarning]:\n        alerts: List[HealthWarning] = []\n        for layer, stats in activation_stats.items():\n            zeros_pct = stats.get(\"zeros_pct\", 0)\n            act_type = stats.get(\"activation_type\", \"\")\n\n            # Dead ReLU\n            if act_type in (\"relu\", \"leaky_relu\") and zeros_pct &gt; self.config.dead_relu_zeros_pct:\n                key = f\"dead_relu_{layer}\"\n                if self._should_fire(key):\n                    alerts.append(HealthWarning(\n                        rule_name=\"dead_relu\",\n                        severity=\"warning\",\n                        message=f\"Possible dying ReLU in '{layer}' ({zeros_pct:.1f}% zeros)\",\n                        diagnosis=(\n                            \"Neurons producing zero outputs will receive zero gradients \"\n                            \"and never recover.\"\n                        ),\n                        recommendations=[\n                            \"Use LeakyReLU(negative_slope=0.01) instead of ReLU\",\n                            \"Lower the learning rate\",\n                            \"Use He initialisation\",\n                        ],\n                        layer_name=layer, epoch=epoch, batch=batch,\n                        data={\"zeros_pct\": zeros_pct},\n                    ))\n\n            # Saturation\n            sat_pct = stats.get(\"saturation_pct\", 0)\n            if sat_pct &gt; self.config.saturation_pct_thresh:\n                key = f\"saturation_{layer}\"\n                if self._should_fire(key):\n                    alerts.append(HealthWarning(\n                        rule_name=\"activation_saturation\",\n                        severity=\"warning\",\n                        message=f\"{sat_pct:.1f}% activations saturated in '{layer}'\",\n                        diagnosis=\"Saturated activations produce near-zero gradients.\",\n                        recommendations=[\n                            \"Switch to ReLU or GELU activation\",\n                            \"Normalise inputs to the layer\",\n                            \"Use batch normalisation\",\n                        ],\n                        layer_name=layer, epoch=epoch, batch=batch,\n                        data={\"saturation_pct\": sat_pct},\n                    ))\n        return alerts\n\n    # -- Weight checks ----------------------------------------------------\n\n    def _check_dead_weights(self, weight_stats: Dict[str, Dict],\n                            epoch, batch) -&gt; List[HealthWarning]:\n        alerts: List[HealthWarning] = []\n        for layer, stats in weight_stats.items():\n            zeros_pct = stats.get(\"zeros_pct\", 0)\n            if zeros_pct &gt; self.config.dead_relu_zeros_pct:\n                key = f\"dead_weights_{layer}\"\n                if self._should_fire(key):\n                    alerts.append(HealthWarning(\n                        rule_name=\"dead_weights\",\n                        severity=\"warning\",\n                        message=f\"{zeros_pct:.1f}% dead neurons in '{layer}'\",\n                        diagnosis=\"Most weights near zero \u2014 layer contributes nothing.\",\n                        recommendations=[\n                            \"Switch to LeakyReLU or ELU\",\n                            \"Use a different weight initialisation\",\n                            \"Lower the learning rate\",\n                        ],\n                        layer_name=layer, epoch=epoch, batch=batch,\n                        data={\"zeros_pct\": zeros_pct},\n                    ))\n        return alerts\n\n    # -- Weight stagnation ------------------------------------------------\n\n    def _check_weight_stagnation(self, weight_deltas: Dict[str, float],\n                                 epoch, batch) -&gt; List[HealthWarning]:\n        alerts: List[HealthWarning] = []\n        cfg = self.config\n        for param, delta in weight_deltas.items():\n            buf = self._weight_deltas.setdefault(\n                param, deque(maxlen=cfg.weight_stagnation_window))\n            buf.append(delta)\n            if len(buf) &gt;= cfg.weight_stagnation_window:\n                if all(d &lt; cfg.weight_stagnation_eps for d in buf):\n                    key = f\"weight_stagnation_{param}\"\n                    if self._should_fire(key):\n                        alerts.append(HealthWarning(\n                            rule_name=\"weight_stagnation\",\n                            severity=\"warning\",\n                            message=(f\"Optimizer may have stagnated for '{param}' \"\n                                     f\"({cfg.weight_stagnation_window} batches, \"\n                                     f\"max \u0394={max(buf):.2e})\"),\n                            diagnosis=(\n                                \"Weight updates are near-zero for several consecutive \"\n                                \"batches, suggesting the optimizer has plateaued.\"\n                            ),\n                            recommendations=[\n                                \"Reduce learning rate and use a scheduler\",\n                                \"Try a different optimizer (switch SGD\u2194Adam)\",\n                                \"Check that gradients are flowing to this parameter\",\n                            ],\n                            layer_name=param, epoch=epoch, batch=batch,\n                            data={\"max_delta\": float(max(buf))},\n                        ))\n        return alerts\n\n    # -- Overfitting ------------------------------------------------------\n\n    def _check_overfitting(self, epoch: int) -&gt; List[HealthWarning]:\n        p = self.config.overfit_patience\n        if len(self._train_losses) &lt; p or len(self._val_losses) &lt; p:\n            return []\n        recent_train = float(np.mean(self._train_losses[-p:]))\n        recent_val = float(np.mean(self._val_losses[-p:]))\n        if recent_train &lt; 1e-12:\n            return []\n        ratio = recent_val / max(recent_train, 1e-12)\n        if ratio &gt; self.config.overfit_ratio:\n            key = \"overfitting\"\n            if not self._should_fire(key):\n                return []\n            return [HealthWarning(\n                rule_name=\"overfitting\",\n                severity=\"warning\",\n                message=f\"Possible overfitting (val/train loss ratio = {ratio:.2f})\",\n                diagnosis=(\n                    \"Validation loss is diverging from training loss, \"\n                    \"indicating the model is memorising rather than learning.\"\n                ),\n                recommendations=[\n                    \"Add Dropout layers (rate=0.2-0.5)\",\n                    \"Use L2 regularization (weight_decay=1e-4)\",\n                    \"Get more training data or use data augmentation\",\n                    \"Reduce model complexity (fewer layers / neurons)\",\n                ],\n                epoch=epoch,\n                data={\"ratio\": ratio, \"train\": recent_train, \"val\": recent_val},\n            )]\n        return []\n\n    # -- Loss stagnation --------------------------------------------------\n\n    def _check_loss_stagnation(self, epoch: int) -&gt; List[HealthWarning]:\n        w = self.config.loss_stagnation_window\n        if len(self._train_losses) &lt; w:\n            return []\n        recent = self._train_losses[-w:]\n        delta = abs(recent[-1] - recent[0])\n        if delta &lt; self.config.loss_stagnation_eps:\n            key = \"loss_stagnation\"\n            if not self._should_fire(key):\n                return []\n            return [HealthWarning(\n                rule_name=\"loss_stagnation\",\n                severity=\"warning\",\n                message=f\"Loss stagnant for {w} epochs (\u0394={delta:.2e})\",\n                diagnosis=\"Training progress has plateaued.\",\n                recommendations=[\n                    \"Reduce learning rate (try lr \u00d7 0.1)\",\n                    \"Use learning rate scheduling (e.g., cosine annealing)\",\n                    \"Try a different optimizer (switch SGD\u2194Adam)\",\n                ],\n                epoch=epoch,\n                data={\"delta\": delta, \"window\": w},\n            )]\n        return []\n\n    # ------------------------------------------------------------------\n    # Utilities\n    # ------------------------------------------------------------------\n\n    def get_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Return a structured summary of all warnings fired.\"\"\"\n        by_rule: Dict[str, int] = {}\n        by_severity: Dict[str, int] = {}\n        for w in self._warnings:\n            by_rule[w.rule_name] = by_rule.get(w.rule_name, 0) + 1\n            by_severity[w.severity] = by_severity.get(w.severity, 0) + 1\n        return {\n            \"total_warnings\": len(self._warnings),\n            \"by_rule\": by_rule,\n            \"by_severity\": by_severity,\n            \"warnings\": [\n                {\n                    \"rule\": w.rule_name,\n                    \"severity\": w.severity,\n                    \"message\": w.message,\n                    \"layer\": w.layer_name,\n                    \"epoch\": w.epoch,\n                    \"batch\": w.batch,\n                }\n                for w in self._warnings\n            ],\n        }\n\n    def reset(self) -&gt; None:\n        \"\"\"Clear all state.\"\"\"\n        self._warnings.clear()\n        self._gradient_norms.clear()\n        self._weight_deltas.clear()\n        self._train_losses.clear()\n        self._val_losses.clear()\n        self._batch_losses.clear()\n        self._fired_rules.clear()\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.health_warnings.AutoHealthWarnings-functions","title":"Functions","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.health_warnings.AutoHealthWarnings.check_batch","title":"<code>check_batch(*, epoch=None, batch=None, loss=None, gradient_norms=None, weight_stats=None, activation_stats=None, weight_deltas=None)</code>","text":"<p>Run all batch-level rules and return new warnings.</p> Source code in <code>src/neurogebra/logging/health_warnings.py</code> <pre><code>def check_batch(\n    self,\n    *,\n    epoch: Optional[int] = None,\n    batch: Optional[int] = None,\n    loss: Optional[float] = None,\n    gradient_norms: Optional[Dict[str, float]] = None,\n    weight_stats: Optional[Dict[str, Dict[str, Any]]] = None,\n    activation_stats: Optional[Dict[str, Dict[str, Any]]] = None,\n    weight_deltas: Optional[Dict[str, float]] = None,\n) -&gt; List[HealthWarning]:\n    \"\"\"Run all batch-level rules and return new warnings.\"\"\"\n    new: List[HealthWarning] = []\n\n    if loss is not None:\n        self._batch_losses.append(loss)\n        new.extend(self._check_nan_inf_loss(loss, epoch, batch))\n        new.extend(self._check_loss_divergence(epoch, batch))\n\n    if gradient_norms:\n        new.extend(self._check_gradients(gradient_norms, epoch, batch))\n\n    if activation_stats:\n        new.extend(self._check_activations(activation_stats, epoch, batch))\n\n    if weight_stats:\n        new.extend(self._check_dead_weights(weight_stats, epoch, batch))\n\n    if weight_deltas:\n        new.extend(self._check_weight_stagnation(weight_deltas, epoch, batch))\n\n    self._warnings.extend(new)\n    return new\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.health_warnings.AutoHealthWarnings.check_epoch","title":"<code>check_epoch(*, epoch, train_loss=None, val_loss=None, train_acc=None, val_acc=None, gradient_norms=None, weight_stats=None, activation_stats=None)</code>","text":"<p>Run all epoch-level rules and return new warnings.</p> Source code in <code>src/neurogebra/logging/health_warnings.py</code> <pre><code>def check_epoch(\n    self,\n    *,\n    epoch: int,\n    train_loss: Optional[float] = None,\n    val_loss: Optional[float] = None,\n    train_acc: Optional[float] = None,\n    val_acc: Optional[float] = None,\n    gradient_norms: Optional[Dict[str, float]] = None,\n    weight_stats: Optional[Dict[str, Dict[str, Any]]] = None,\n    activation_stats: Optional[Dict[str, Dict[str, Any]]] = None,\n) -&gt; List[HealthWarning]:\n    \"\"\"Run all epoch-level rules and return new warnings.\"\"\"\n    new: List[HealthWarning] = []\n\n    if train_loss is not None:\n        self._train_losses.append(train_loss)\n    if val_loss is not None:\n        self._val_losses.append(val_loss)\n\n    # Overfitting check\n    new.extend(self._check_overfitting(epoch))\n\n    # Loss stagnation\n    new.extend(self._check_loss_stagnation(epoch))\n\n    # Gradient checks (epoch-level too)\n    if gradient_norms:\n        new.extend(self._check_gradients(gradient_norms, epoch, None))\n\n    # Activation / weight checks\n    if activation_stats:\n        new.extend(self._check_activations(activation_stats, epoch, None))\n    if weight_stats:\n        new.extend(self._check_dead_weights(weight_stats, epoch, None))\n\n    self._warnings.extend(new)\n    return new\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.health_warnings.AutoHealthWarnings.get_summary","title":"<code>get_summary()</code>","text":"<p>Return a structured summary of all warnings fired.</p> Source code in <code>src/neurogebra/logging/health_warnings.py</code> <pre><code>def get_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"Return a structured summary of all warnings fired.\"\"\"\n    by_rule: Dict[str, int] = {}\n    by_severity: Dict[str, int] = {}\n    for w in self._warnings:\n        by_rule[w.rule_name] = by_rule.get(w.rule_name, 0) + 1\n        by_severity[w.severity] = by_severity.get(w.severity, 0) + 1\n    return {\n        \"total_warnings\": len(self._warnings),\n        \"by_rule\": by_rule,\n        \"by_severity\": by_severity,\n        \"warnings\": [\n            {\n                \"rule\": w.rule_name,\n                \"severity\": w.severity,\n                \"message\": w.message,\n                \"layer\": w.layer_name,\n                \"epoch\": w.epoch,\n                \"batch\": w.batch,\n            }\n            for w in self._warnings\n        ],\n    }\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.health_warnings.AutoHealthWarnings.reset","title":"<code>reset()</code>","text":"<p>Clear all state.</p> Source code in <code>src/neurogebra/logging/health_warnings.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Clear all state.\"\"\"\n    self._warnings.clear()\n    self._gradient_norms.clear()\n    self._weight_deltas.clear()\n    self._train_losses.clear()\n    self._val_losses.clear()\n    self._batch_losses.clear()\n    self._fired_rules.clear()\n</code></pre>"},{"location":"advanced/observatory-pro/#warningconfig","title":"<code>WarningConfig</code>","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.health_warnings.WarningConfig","title":"<code>neurogebra.logging.health_warnings.WarningConfig</code>  <code>dataclass</code>","text":"<p>Configurable thresholds for the automated health warning system.</p> Source code in <code>src/neurogebra/logging/health_warnings.py</code> <pre><code>@dataclass\nclass WarningConfig:\n    \"\"\"Configurable thresholds for the automated health warning system.\"\"\"\n\n    # Dead ReLU / zero activation\n    dead_relu_zeros_pct: float = 50.0\n\n    # Gradient norms\n    gradient_vanish_thresh: float = 1e-7\n    gradient_explode_thresh: float = 100.0\n    gradient_spike_factor: float = 5.0\n    gradient_rolling_window: int = 20\n\n    # Overfitting\n    overfit_patience: int = 3\n    overfit_ratio: float = 1.3          # val_loss / train_loss\n\n    # Stagnation\n    weight_stagnation_eps: float = 1e-6\n    weight_stagnation_window: int = 5\n    loss_stagnation_eps: float = 1e-4\n    loss_stagnation_window: int = 5\n\n    # Divergence\n    loss_divergence_window: int = 3\n\n    # Activation saturation\n    saturation_pct_thresh: float = 40.0\n\n    # Learning rate heuristic\n    lr_too_high_loss_factor: float = 3.0\n</code></pre>"},{"location":"advanced/observatory-pro/#epochsummarizer","title":"<code>EpochSummarizer</code>","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.epoch_summary.EpochSummarizer","title":"<code>neurogebra.logging.epoch_summary.EpochSummarizer</code>","text":"<p>Accumulates batch-level data and produces per-epoch statistical summaries.</p> <p>Call :meth:<code>record_batch</code> for every batch, then :meth:<code>finalize_epoch</code> at the end of the epoch to get an :class:<code>EpochSummary</code>.</p> Source code in <code>src/neurogebra/logging/epoch_summary.py</code> <pre><code>class EpochSummarizer:\n    \"\"\"\n    Accumulates batch-level data and produces per-epoch statistical summaries.\n\n    Call :meth:`record_batch` for every batch, then :meth:`finalize_epoch`\n    at the end of the epoch to get an :class:`EpochSummary`.\n    \"\"\"\n\n    def __init__(self):\n        # {epoch: {metric_name: [values]}}\n        self._metric_buffers: Dict[int, Dict[str, List[float]]] = defaultdict(lambda: defaultdict(list))\n        # {epoch: {layer: [norm_values]}}\n        self._gradient_buffers: Dict[int, Dict[str, List[float]]] = defaultdict(lambda: defaultdict(list))\n        # {epoch: {layer: {stat_name: [values]}}}\n        self._weight_buffers: Dict[int, Dict[str, Dict[str, List[float]]]] = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(list))\n        )\n        self._activation_buffers: Dict[int, Dict[str, Dict[str, List[float]]]] = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(list))\n        )\n        self._batch_counts: Dict[int, int] = defaultdict(int)\n        self._summaries: List[EpochSummary] = []\n\n    @property\n    def summaries(self) -&gt; List[EpochSummary]:\n        return list(self._summaries)\n\n    def record_batch(\n        self,\n        epoch: int,\n        *,\n        metrics: Optional[Dict[str, float]] = None,\n        gradient_norms: Optional[Dict[str, float]] = None,\n        weight_stats: Optional[Dict[str, Dict[str, float]]] = None,\n        activation_stats: Optional[Dict[str, Dict[str, float]]] = None,\n    ) -&gt; None:\n        \"\"\"Buffer one batch of data for the given epoch.\"\"\"\n        self._batch_counts[epoch] += 1\n\n        if metrics:\n            buf = self._metric_buffers[epoch]\n            for key, val in metrics.items():\n                if isinstance(val, (int, float)) and np.isfinite(val):\n                    buf[key].append(float(val))\n\n        if gradient_norms:\n            buf = self._gradient_buffers[epoch]\n            for layer, norm in gradient_norms.items():\n                if np.isfinite(norm):\n                    buf[layer].append(float(norm))\n\n        if weight_stats:\n            buf = self._weight_buffers[epoch]\n            for layer, stats in weight_stats.items():\n                for key, val in stats.items():\n                    if isinstance(val, (int, float)) and np.isfinite(val):\n                        buf[layer][key].append(float(val))\n\n        if activation_stats:\n            buf = self._activation_buffers[epoch]\n            for layer, stats in activation_stats.items():\n                for key, val in stats.items():\n                    if isinstance(val, (int, float)) and np.isfinite(val):\n                        buf[layer][key].append(float(val))\n\n    def finalize_epoch(self, epoch: int) -&gt; EpochSummary:\n        \"\"\"\n        Compute and return the statistical summary for *epoch*.\n\n        Automatically clears batch buffers for that epoch.\n        \"\"\"\n        n_batches = self._batch_counts.get(epoch, 0)\n\n        # Metrics\n        metric_stats: Dict[str, EpochStats] = {}\n        for name, vals in self._metric_buffers.get(epoch, {}).items():\n            if vals:\n                metric_stats[name] = _compute_stats(name, vals)\n\n        # Gradient norms\n        grad_stats: Dict[str, EpochStats] = {}\n        for layer, vals in self._gradient_buffers.get(epoch, {}).items():\n            if vals:\n                grad_stats[layer] = _compute_stats(layer, vals)\n\n        # Weight summaries\n        weight_sums: Dict[str, Dict[str, EpochStats]] = {}\n        for layer, keys in self._weight_buffers.get(epoch, {}).items():\n            weight_sums[layer] = {}\n            for key, vals in keys.items():\n                if vals:\n                    weight_sums[layer][key] = _compute_stats(key, vals)\n\n        # Activation summaries\n        act_sums: Dict[str, Dict[str, EpochStats]] = {}\n        for layer, keys in self._activation_buffers.get(epoch, {}).items():\n            act_sums[layer] = {}\n            for key, vals in keys.items():\n                if vals:\n                    act_sums[layer][key] = _compute_stats(key, vals)\n\n        summary = EpochSummary(\n            epoch=epoch,\n            num_batches=n_batches,\n            metrics=metric_stats,\n            gradient_norms=grad_stats,\n            weight_summaries=weight_sums,\n            activation_summaries=act_sums,\n        )\n        self._summaries.append(summary)\n\n        # Cleanup\n        self._metric_buffers.pop(epoch, None)\n        self._gradient_buffers.pop(epoch, None)\n        self._weight_buffers.pop(epoch, None)\n        self._activation_buffers.pop(epoch, None)\n        self._batch_counts.pop(epoch, None)\n\n        return summary\n\n    def get_all_summaries(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Return all epoch summaries as dicts.\"\"\"\n        return [s.to_dict() for s in self._summaries]\n\n    def reset(self) -&gt; None:\n        \"\"\"Clear all state.\"\"\"\n        self._metric_buffers.clear()\n        self._gradient_buffers.clear()\n        self._weight_buffers.clear()\n        self._activation_buffers.clear()\n        self._batch_counts.clear()\n        self._summaries.clear()\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.epoch_summary.EpochSummarizer-functions","title":"Functions","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.epoch_summary.EpochSummarizer.record_batch","title":"<code>record_batch(epoch, *, metrics=None, gradient_norms=None, weight_stats=None, activation_stats=None)</code>","text":"<p>Buffer one batch of data for the given epoch.</p> Source code in <code>src/neurogebra/logging/epoch_summary.py</code> <pre><code>def record_batch(\n    self,\n    epoch: int,\n    *,\n    metrics: Optional[Dict[str, float]] = None,\n    gradient_norms: Optional[Dict[str, float]] = None,\n    weight_stats: Optional[Dict[str, Dict[str, float]]] = None,\n    activation_stats: Optional[Dict[str, Dict[str, float]]] = None,\n) -&gt; None:\n    \"\"\"Buffer one batch of data for the given epoch.\"\"\"\n    self._batch_counts[epoch] += 1\n\n    if metrics:\n        buf = self._metric_buffers[epoch]\n        for key, val in metrics.items():\n            if isinstance(val, (int, float)) and np.isfinite(val):\n                buf[key].append(float(val))\n\n    if gradient_norms:\n        buf = self._gradient_buffers[epoch]\n        for layer, norm in gradient_norms.items():\n            if np.isfinite(norm):\n                buf[layer].append(float(norm))\n\n    if weight_stats:\n        buf = self._weight_buffers[epoch]\n        for layer, stats in weight_stats.items():\n            for key, val in stats.items():\n                if isinstance(val, (int, float)) and np.isfinite(val):\n                    buf[layer][key].append(float(val))\n\n    if activation_stats:\n        buf = self._activation_buffers[epoch]\n        for layer, stats in activation_stats.items():\n            for key, val in stats.items():\n                if isinstance(val, (int, float)) and np.isfinite(val):\n                    buf[layer][key].append(float(val))\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.epoch_summary.EpochSummarizer.finalize_epoch","title":"<code>finalize_epoch(epoch)</code>","text":"<p>Compute and return the statistical summary for epoch.</p> <p>Automatically clears batch buffers for that epoch.</p> Source code in <code>src/neurogebra/logging/epoch_summary.py</code> <pre><code>def finalize_epoch(self, epoch: int) -&gt; EpochSummary:\n    \"\"\"\n    Compute and return the statistical summary for *epoch*.\n\n    Automatically clears batch buffers for that epoch.\n    \"\"\"\n    n_batches = self._batch_counts.get(epoch, 0)\n\n    # Metrics\n    metric_stats: Dict[str, EpochStats] = {}\n    for name, vals in self._metric_buffers.get(epoch, {}).items():\n        if vals:\n            metric_stats[name] = _compute_stats(name, vals)\n\n    # Gradient norms\n    grad_stats: Dict[str, EpochStats] = {}\n    for layer, vals in self._gradient_buffers.get(epoch, {}).items():\n        if vals:\n            grad_stats[layer] = _compute_stats(layer, vals)\n\n    # Weight summaries\n    weight_sums: Dict[str, Dict[str, EpochStats]] = {}\n    for layer, keys in self._weight_buffers.get(epoch, {}).items():\n        weight_sums[layer] = {}\n        for key, vals in keys.items():\n            if vals:\n                weight_sums[layer][key] = _compute_stats(key, vals)\n\n    # Activation summaries\n    act_sums: Dict[str, Dict[str, EpochStats]] = {}\n    for layer, keys in self._activation_buffers.get(epoch, {}).items():\n        act_sums[layer] = {}\n        for key, vals in keys.items():\n            if vals:\n                act_sums[layer][key] = _compute_stats(key, vals)\n\n    summary = EpochSummary(\n        epoch=epoch,\n        num_batches=n_batches,\n        metrics=metric_stats,\n        gradient_norms=grad_stats,\n        weight_summaries=weight_sums,\n        activation_summaries=act_sums,\n    )\n    self._summaries.append(summary)\n\n    # Cleanup\n    self._metric_buffers.pop(epoch, None)\n    self._gradient_buffers.pop(epoch, None)\n    self._weight_buffers.pop(epoch, None)\n    self._activation_buffers.pop(epoch, None)\n    self._batch_counts.pop(epoch, None)\n\n    return summary\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.epoch_summary.EpochSummarizer.get_all_summaries","title":"<code>get_all_summaries()</code>","text":"<p>Return all epoch summaries as dicts.</p> Source code in <code>src/neurogebra/logging/epoch_summary.py</code> <pre><code>def get_all_summaries(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Return all epoch summaries as dicts.\"\"\"\n    return [s.to_dict() for s in self._summaries]\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.epoch_summary.EpochSummarizer.reset","title":"<code>reset()</code>","text":"<p>Clear all state.</p> Source code in <code>src/neurogebra/logging/epoch_summary.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Clear all state.\"\"\"\n    self._metric_buffers.clear()\n    self._gradient_buffers.clear()\n    self._weight_buffers.clear()\n    self._activation_buffers.clear()\n    self._batch_counts.clear()\n    self._summaries.clear()\n</code></pre>"},{"location":"advanced/observatory-pro/#tieredstorage","title":"<code>TieredStorage</code>","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.tiered_storage.TieredStorage","title":"<code>neurogebra.logging.tiered_storage.TieredStorage</code>","text":"<p>Backend for :class:<code>TrainingLogger</code> that writes events into three separate NDJSON files based on their tier.</p> <p>Attributes:</p> Name Type Description <code>basic_path</code> <p>Path to <code>basic.log</code>.</p> <code>health_path</code> <p>Path to <code>health.log</code>.</p> <code>debug_path</code> <p>Path to <code>debug.log</code>.</p> Source code in <code>src/neurogebra/logging/tiered_storage.py</code> <pre><code>class TieredStorage:\n    \"\"\"\n    Backend for :class:`TrainingLogger` that writes events into three\n    separate NDJSON files based on their tier.\n\n    Attributes:\n        basic_path: Path to ``basic.log``.\n        health_path: Path to ``health.log``.\n        debug_path: Path to ``debug.log``.\n    \"\"\"\n\n    def __init__(\n        self,\n        base_dir: str = \"./training_logs\",\n        basic_filename: str = \"basic.log\",\n        health_filename: str = \"health.log\",\n        debug_filename: str = \"debug.log\",\n        write_debug: bool = True,\n        buffer_size: int = 50,\n    ):\n        \"\"\"\n        Args:\n            base_dir: Directory for log files.\n            basic_filename: Name of the epoch-metrics log file.\n            health_filename: Name of the health/warnings log file.\n            debug_filename: Name of the debug-level log file.\n            write_debug: Whether to write debug-tier events at all.\n                         Set to ``False`` in production to save I/O.\n            buffer_size: Number of events to buffer before flushing to disk.\n        \"\"\"\n        self.base_dir = base_dir\n        self.basic_path = os.path.join(base_dir, basic_filename)\n        self.health_path = os.path.join(base_dir, health_filename)\n        self.debug_path = os.path.join(base_dir, debug_filename)\n        self.write_debug = write_debug\n\n        self._buffer_size = buffer_size\n        self._basic_buffer: List[str] = []\n        self._health_buffer: List[str] = []\n        self._debug_buffer: List[str] = []\n\n        self._opened = False\n        self._basic_fh = None\n        self._health_fh = None\n        self._debug_fh = None\n\n        # Stats\n        self.basic_count = 0\n        self.health_count = 0\n        self.debug_count = 0\n\n    # ------------------------------------------------------------------\n    # Backend interface (called by TrainingLogger._emit)\n    # ------------------------------------------------------------------\n\n    def handle_event(self, event: LogEvent) -&gt; None:\n        \"\"\"Classify and route the event to the appropriate tier.\"\"\"\n        record = self._serialise(event)\n        line = json.dumps(record, default=str)\n\n        tier = self._classify(event)\n\n        if tier == \"basic\":\n            self._basic_buffer.append(line)\n            self.basic_count += 1\n            if len(self._basic_buffer) &gt;= self._buffer_size:\n                self._flush_buffer(\"basic\")\n\n        elif tier == \"health\":\n            self._health_buffer.append(line)\n            self.health_count += 1\n            if len(self._health_buffer) &gt;= self._buffer_size:\n                self._flush_buffer(\"health\")\n\n        else:  # debug\n            if self.write_debug:\n                self._debug_buffer.append(line)\n                self.debug_count += 1\n                if len(self._debug_buffer) &gt;= self._buffer_size:\n                    self._flush_buffer(\"debug\")\n\n    # ------------------------------------------------------------------\n    # Specific event handlers for named dispatch\n    # ------------------------------------------------------------------\n\n    def handle_train_start(self, event: LogEvent) -&gt; None:\n        self.handle_event(event)\n\n    def handle_train_end(self, event: LogEvent) -&gt; None:\n        self.handle_event(event)\n        self.flush()\n\n    def handle_epoch_start(self, event: LogEvent) -&gt; None:\n        self.handle_event(event)\n\n    def handle_epoch_end(self, event: LogEvent) -&gt; None:\n        self.handle_event(event)\n        # Flush basic tier at end of each epoch\n        self._flush_buffer(\"basic\")\n\n    def handle_health_check(self, event: LogEvent) -&gt; None:\n        self.handle_event(event)\n        # Health events are flushed immediately (important)\n        self._flush_buffer(\"health\")\n\n    # ------------------------------------------------------------------\n    # Classification\n    # ------------------------------------------------------------------\n\n    @staticmethod\n    def _classify(event: LogEvent) -&gt; str:\n        \"\"\"Return 'basic', 'health', or 'debug'.\"\"\"\n        if event.event_type in _HEALTH_EVENTS:\n            return \"health\"\n        if event.severity in _HEALTH_SEVERITIES:\n            return \"health\"\n        if event.event_type in _BASIC_EVENTS:\n            return \"basic\"\n        return \"debug\"\n\n    # ------------------------------------------------------------------\n    # Serialisation\n    # ------------------------------------------------------------------\n\n    @staticmethod\n    def _serialise(event: LogEvent) -&gt; Dict[str, Any]:\n        return {\n            \"event_type\": event.event_type,\n            \"level\": event.level.name,\n            \"timestamp\": event.timestamp,\n            \"epoch\": event.epoch,\n            \"batch\": event.batch,\n            \"layer_name\": event.layer_name,\n            \"layer_index\": event.layer_index,\n            \"severity\": event.severity,\n            \"message\": event.message,\n            \"data\": _safe(event.data),\n        }\n\n    # ------------------------------------------------------------------\n    # File I/O\n    # ------------------------------------------------------------------\n\n    def _ensure_dir(self) -&gt; None:\n        if not self._opened:\n            os.makedirs(self.base_dir, exist_ok=True)\n            self._opened = True\n\n    def _flush_buffer(self, tier: str) -&gt; None:\n        buf_attr = f\"_{tier}_buffer\"\n        path_attr = f\"{tier}_path\"\n        buf: List[str] = getattr(self, buf_attr)\n        if not buf:\n            return\n        self._ensure_dir()\n        path = getattr(self, path_attr)\n        with open(path, \"a\", encoding=\"utf-8\") as f:\n            for line in buf:\n                f.write(line + \"\\n\")\n        buf.clear()\n\n    def flush(self) -&gt; None:\n        \"\"\"Flush all buffered events to disk.\"\"\"\n        self._flush_buffer(\"basic\")\n        self._flush_buffer(\"health\")\n        if self.write_debug:\n            self._flush_buffer(\"debug\")\n\n    def close(self) -&gt; None:\n        \"\"\"Flush and release resources.\"\"\"\n        self.flush()\n\n    # ------------------------------------------------------------------\n    # Reading helpers\n    # ------------------------------------------------------------------\n\n    def read_basic(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Read all basic-tier events from disk.\"\"\"\n        return self._read_ndjson(self.basic_path)\n\n    def read_health(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Read all health-tier events from disk.\"\"\"\n        return self._read_ndjson(self.health_path)\n\n    def read_debug(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Read all debug-tier events from disk.\"\"\"\n        return self._read_ndjson(self.debug_path)\n\n    @staticmethod\n    def _read_ndjson(path: str) -&gt; List[Dict[str, Any]]:\n        if not os.path.exists(path):\n            return []\n        events = []\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    try:\n                        events.append(json.loads(line))\n                    except json.JSONDecodeError:\n                        continue\n        return events\n\n    # ------------------------------------------------------------------\n    # Utilities\n    # ------------------------------------------------------------------\n\n    def summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Return file-size and event-count statistics.\"\"\"\n        def _size(path):\n            try:\n                return os.path.getsize(path)\n            except OSError:\n                return 0\n\n        return {\n            \"basic\": {\"events\": self.basic_count, \"size_bytes\": _size(self.basic_path)},\n            \"health\": {\"events\": self.health_count, \"size_bytes\": _size(self.health_path)},\n            \"debug\": {\"events\": self.debug_count, \"size_bytes\": _size(self.debug_path)},\n            \"total_events\": self.basic_count + self.health_count + self.debug_count,\n        }\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.tiered_storage.TieredStorage-functions","title":"Functions","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.tiered_storage.TieredStorage.__init__","title":"<code>__init__(base_dir='./training_logs', basic_filename='basic.log', health_filename='health.log', debug_filename='debug.log', write_debug=True, buffer_size=50)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>str</code> <p>Directory for log files.</p> <code>'./training_logs'</code> <code>basic_filename</code> <code>str</code> <p>Name of the epoch-metrics log file.</p> <code>'basic.log'</code> <code>health_filename</code> <code>str</code> <p>Name of the health/warnings log file.</p> <code>'health.log'</code> <code>debug_filename</code> <code>str</code> <p>Name of the debug-level log file.</p> <code>'debug.log'</code> <code>write_debug</code> <code>bool</code> <p>Whether to write debug-tier events at all.          Set to <code>False</code> in production to save I/O.</p> <code>True</code> <code>buffer_size</code> <code>int</code> <p>Number of events to buffer before flushing to disk.</p> <code>50</code> Source code in <code>src/neurogebra/logging/tiered_storage.py</code> <pre><code>def __init__(\n    self,\n    base_dir: str = \"./training_logs\",\n    basic_filename: str = \"basic.log\",\n    health_filename: str = \"health.log\",\n    debug_filename: str = \"debug.log\",\n    write_debug: bool = True,\n    buffer_size: int = 50,\n):\n    \"\"\"\n    Args:\n        base_dir: Directory for log files.\n        basic_filename: Name of the epoch-metrics log file.\n        health_filename: Name of the health/warnings log file.\n        debug_filename: Name of the debug-level log file.\n        write_debug: Whether to write debug-tier events at all.\n                     Set to ``False`` in production to save I/O.\n        buffer_size: Number of events to buffer before flushing to disk.\n    \"\"\"\n    self.base_dir = base_dir\n    self.basic_path = os.path.join(base_dir, basic_filename)\n    self.health_path = os.path.join(base_dir, health_filename)\n    self.debug_path = os.path.join(base_dir, debug_filename)\n    self.write_debug = write_debug\n\n    self._buffer_size = buffer_size\n    self._basic_buffer: List[str] = []\n    self._health_buffer: List[str] = []\n    self._debug_buffer: List[str] = []\n\n    self._opened = False\n    self._basic_fh = None\n    self._health_fh = None\n    self._debug_fh = None\n\n    # Stats\n    self.basic_count = 0\n    self.health_count = 0\n    self.debug_count = 0\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.tiered_storage.TieredStorage.handle_event","title":"<code>handle_event(event)</code>","text":"<p>Classify and route the event to the appropriate tier.</p> Source code in <code>src/neurogebra/logging/tiered_storage.py</code> <pre><code>def handle_event(self, event: LogEvent) -&gt; None:\n    \"\"\"Classify and route the event to the appropriate tier.\"\"\"\n    record = self._serialise(event)\n    line = json.dumps(record, default=str)\n\n    tier = self._classify(event)\n\n    if tier == \"basic\":\n        self._basic_buffer.append(line)\n        self.basic_count += 1\n        if len(self._basic_buffer) &gt;= self._buffer_size:\n            self._flush_buffer(\"basic\")\n\n    elif tier == \"health\":\n        self._health_buffer.append(line)\n        self.health_count += 1\n        if len(self._health_buffer) &gt;= self._buffer_size:\n            self._flush_buffer(\"health\")\n\n    else:  # debug\n        if self.write_debug:\n            self._debug_buffer.append(line)\n            self.debug_count += 1\n            if len(self._debug_buffer) &gt;= self._buffer_size:\n                self._flush_buffer(\"debug\")\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.tiered_storage.TieredStorage.flush","title":"<code>flush()</code>","text":"<p>Flush all buffered events to disk.</p> Source code in <code>src/neurogebra/logging/tiered_storage.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flush all buffered events to disk.\"\"\"\n    self._flush_buffer(\"basic\")\n    self._flush_buffer(\"health\")\n    if self.write_debug:\n        self._flush_buffer(\"debug\")\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.tiered_storage.TieredStorage.close","title":"<code>close()</code>","text":"<p>Flush and release resources.</p> Source code in <code>src/neurogebra/logging/tiered_storage.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Flush and release resources.\"\"\"\n    self.flush()\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.tiered_storage.TieredStorage.read_basic","title":"<code>read_basic()</code>","text":"<p>Read all basic-tier events from disk.</p> Source code in <code>src/neurogebra/logging/tiered_storage.py</code> <pre><code>def read_basic(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Read all basic-tier events from disk.\"\"\"\n    return self._read_ndjson(self.basic_path)\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.tiered_storage.TieredStorage.read_health","title":"<code>read_health()</code>","text":"<p>Read all health-tier events from disk.</p> Source code in <code>src/neurogebra/logging/tiered_storage.py</code> <pre><code>def read_health(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Read all health-tier events from disk.\"\"\"\n    return self._read_ndjson(self.health_path)\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.tiered_storage.TieredStorage.read_debug","title":"<code>read_debug()</code>","text":"<p>Read all debug-tier events from disk.</p> Source code in <code>src/neurogebra/logging/tiered_storage.py</code> <pre><code>def read_debug(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Read all debug-tier events from disk.\"\"\"\n    return self._read_ndjson(self.debug_path)\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.tiered_storage.TieredStorage.summary","title":"<code>summary()</code>","text":"<p>Return file-size and event-count statistics.</p> Source code in <code>src/neurogebra/logging/tiered_storage.py</code> <pre><code>def summary(self) -&gt; Dict[str, Any]:\n    \"\"\"Return file-size and event-count statistics.\"\"\"\n    def _size(path):\n        try:\n            return os.path.getsize(path)\n        except OSError:\n            return 0\n\n    return {\n        \"basic\": {\"events\": self.basic_count, \"size_bytes\": _size(self.basic_path)},\n        \"health\": {\"events\": self.health_count, \"size_bytes\": _size(self.health_path)},\n        \"debug\": {\"events\": self.debug_count, \"size_bytes\": _size(self.debug_path)},\n        \"total_events\": self.basic_count + self.health_count + self.debug_count,\n    }\n</code></pre>"},{"location":"advanced/observatory-pro/#dashboardexporter","title":"<code>DashboardExporter</code>","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.dashboard.DashboardExporter","title":"<code>neurogebra.logging.dashboard.DashboardExporter</code>","text":"<p>Advanced HTML dashboard backend.</p> <p>Collects metrics during training and generates a self-contained interactive HTML file with Chart.js visualisations.</p> Source code in <code>src/neurogebra/logging/dashboard.py</code> <pre><code>class DashboardExporter:\n    \"\"\"\n    Advanced HTML dashboard backend.\n\n    Collects metrics during training and generates a self-contained\n    interactive HTML file with Chart.js visualisations.\n    \"\"\"\n\n    def __init__(self, path: str = \"training_logs/dashboard.html\"):\n        self.path = path\n        self._events: List[LogEvent] = []\n        self._epoch_metrics: List[Dict] = []\n        self._gradient_data: Dict[str, List[float]] = {}  # layer \u2192 [norms per epoch]\n        self._weight_data: Dict[str, List[Dict]] = {}     # layer \u2192 [stats per epoch]\n        self._health_events: List[Dict] = []\n        self._train_info: Dict[str, Any] = {}\n        self._batch_losses: List[float] = []\n\n    # ------------------------------------------------------------------\n    # Backend interface\n    # ------------------------------------------------------------------\n\n    def handle_event(self, event: LogEvent) -&gt; None:\n        self._events.append(event)\n\n    def handle_train_start(self, event: LogEvent) -&gt; None:\n        self._train_info = event.data\n        self._events.append(event)\n\n    def handle_train_end(self, event: LogEvent) -&gt; None:\n        self._events.append(event)\n\n    def handle_epoch_end(self, event: LogEvent) -&gt; None:\n        self._events.append(event)\n        metrics = dict(event.data.get(\"metrics\", {}))\n        metrics[\"epoch\"] = event.epoch\n        metrics[\"epoch_time\"] = event.data.get(\"epoch_time\", 0)\n        self._epoch_metrics.append(metrics)\n\n    def handle_batch_end(self, event: LogEvent) -&gt; None:\n        self._events.append(event)\n        loss = event.data.get(\"loss\")\n        if loss is not None:\n            self._batch_losses.append(float(loss))\n\n    def handle_health_check(self, event: LogEvent) -&gt; None:\n        self._events.append(event)\n        self._health_events.append({\n            \"epoch\": event.epoch,\n            \"severity\": event.severity,\n            \"message\": event.message,\n            \"check\": event.data.get(\"check\", \"\"),\n            \"recommendations\": event.data.get(\"recommendations\", []),\n            \"timestamp\": event.timestamp,\n        })\n\n    def handle_layer_forward(self, event: LogEvent) -&gt; None:\n        self._events.append(event)\n\n    def handle_layer_backward(self, event: LogEvent) -&gt; None:\n        self._events.append(event)\n        grad_stats = event.data.get(\"grad_weights_stats\")\n        if grad_stats and event.layer_name:\n            norms = self._gradient_data.setdefault(event.layer_name, [])\n            norms.append(grad_stats.get(\"norm_l2\", 0))\n\n    def handle_weight_updated(self, event: LogEvent) -&gt; None:\n        self._events.append(event)\n\n    # ------------------------------------------------------------------\n    # Save\n    # ------------------------------------------------------------------\n\n    def save(self) -&gt; str:\n        \"\"\"Generate and save the HTML dashboard. Returns the file path.\"\"\"\n        os.makedirs(os.path.dirname(self.path) or \".\", exist_ok=True)\n\n        epochs_list = list(range(1, len(self._epoch_metrics) + 1))\n        losses = [m.get(\"loss\", 0) for m in self._epoch_metrics]\n        val_losses = [m.get(\"val_loss\", 0) for m in self._epoch_metrics]\n        accs = [m.get(\"accuracy\", 0) for m in self._epoch_metrics]\n        val_accs = [m.get(\"val_accuracy\", 0) for m in self._epoch_metrics]\n        epoch_times = [m.get(\"epoch_time\", 0) for m in self._epoch_metrics]\n\n        # Gradient data for heatmap\n        grad_layers = list(self._gradient_data.keys())\n        grad_matrix = [self._gradient_data.get(l, []) for l in grad_layers]\n\n        # Health timeline\n        health_rows = \"\"\n        for h in self._health_events:\n            sev = h[\"severity\"]\n            colour = {\n                \"danger\": \"#e74c3c\", \"warning\": \"#f39c12\",\n                \"critical\": \"#c0392b\", \"success\": \"#2ecc71\",\n                \"info\": \"#3498db\",\n            }.get(sev, \"#95a5a6\")\n            recs = h.get(\"recommendations\", [])\n            rec_html = \"&lt;ul&gt;\" + \"\".join(f\"&lt;li&gt;{r}&lt;/li&gt;\" for r in recs) + \"&lt;/ul&gt;\" if recs else \"\"\n            health_rows += (\n                f'&lt;tr style=\"border-left:4px solid {colour}\"&gt;'\n                f'&lt;td&gt;E{h.get(\"epoch\", \"?\")}&lt;/td&gt;'\n                f\"&lt;td&gt;&lt;span class='badge' style='background:{colour}'&gt;{sev.upper()}&lt;/span&gt;&lt;/td&gt;\"\n                f\"&lt;td&gt;{h['message']}&lt;/td&gt;\"\n                f\"&lt;td&gt;{rec_html}&lt;/td&gt;&lt;/tr&gt;\\n\"\n            )\n\n        # Model info\n        model_info = self._train_info.get(\"model_info\", {})\n        n_epochs = self._train_info.get(\"total_epochs\", len(epochs_list))\n        batch_size = self._train_info.get(\"batch_size\", \"?\")\n\n        html = _DASHBOARD_TEMPLATE.format(\n            timestamp=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            n_epochs=n_epochs,\n            batch_size=batch_size,\n            final_loss=f\"{losses[-1]:.6f}\" if losses else \"\u2014\",\n            final_acc=f\"{accs[-1]:.4f}\" if accs else \"\u2014\",\n            final_val_loss=f\"{val_losses[-1]:.6f}\" if val_losses else \"\u2014\",\n            final_val_acc=f\"{val_accs[-1]:.4f}\" if val_accs else \"\u2014\",\n            total_events=len(self._events),\n            n_warnings=sum(1 for h in self._health_events if h[\"severity\"] in (\"warning\", \"danger\", \"critical\")),\n            epochs=json.dumps(epochs_list),\n            losses=json.dumps(losses),\n            val_losses=json.dumps(val_losses),\n            accs=json.dumps(accs),\n            val_accs=json.dumps(val_accs),\n            epoch_times=json.dumps(epoch_times),\n            batch_losses=json.dumps(self._batch_losses[:2000]),  # cap for perf\n            grad_layers=json.dumps(grad_layers),\n            grad_matrix=json.dumps(grad_matrix),\n            health_rows=health_rows,\n            model_info=json.dumps(model_info, indent=2, default=str),\n        )\n\n        with open(self.path, \"w\", encoding=\"utf-8\") as f:\n            f.write(html)\n        return self.path\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.dashboard.DashboardExporter-functions","title":"Functions","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.dashboard.DashboardExporter.save","title":"<code>save()</code>","text":"<p>Generate and save the HTML dashboard. Returns the file path.</p> Source code in <code>src/neurogebra/logging/dashboard.py</code> <pre><code>def save(self) -&gt; str:\n    \"\"\"Generate and save the HTML dashboard. Returns the file path.\"\"\"\n    os.makedirs(os.path.dirname(self.path) or \".\", exist_ok=True)\n\n    epochs_list = list(range(1, len(self._epoch_metrics) + 1))\n    losses = [m.get(\"loss\", 0) for m in self._epoch_metrics]\n    val_losses = [m.get(\"val_loss\", 0) for m in self._epoch_metrics]\n    accs = [m.get(\"accuracy\", 0) for m in self._epoch_metrics]\n    val_accs = [m.get(\"val_accuracy\", 0) for m in self._epoch_metrics]\n    epoch_times = [m.get(\"epoch_time\", 0) for m in self._epoch_metrics]\n\n    # Gradient data for heatmap\n    grad_layers = list(self._gradient_data.keys())\n    grad_matrix = [self._gradient_data.get(l, []) for l in grad_layers]\n\n    # Health timeline\n    health_rows = \"\"\n    for h in self._health_events:\n        sev = h[\"severity\"]\n        colour = {\n            \"danger\": \"#e74c3c\", \"warning\": \"#f39c12\",\n            \"critical\": \"#c0392b\", \"success\": \"#2ecc71\",\n            \"info\": \"#3498db\",\n        }.get(sev, \"#95a5a6\")\n        recs = h.get(\"recommendations\", [])\n        rec_html = \"&lt;ul&gt;\" + \"\".join(f\"&lt;li&gt;{r}&lt;/li&gt;\" for r in recs) + \"&lt;/ul&gt;\" if recs else \"\"\n        health_rows += (\n            f'&lt;tr style=\"border-left:4px solid {colour}\"&gt;'\n            f'&lt;td&gt;E{h.get(\"epoch\", \"?\")}&lt;/td&gt;'\n            f\"&lt;td&gt;&lt;span class='badge' style='background:{colour}'&gt;{sev.upper()}&lt;/span&gt;&lt;/td&gt;\"\n            f\"&lt;td&gt;{h['message']}&lt;/td&gt;\"\n            f\"&lt;td&gt;{rec_html}&lt;/td&gt;&lt;/tr&gt;\\n\"\n        )\n\n    # Model info\n    model_info = self._train_info.get(\"model_info\", {})\n    n_epochs = self._train_info.get(\"total_epochs\", len(epochs_list))\n    batch_size = self._train_info.get(\"batch_size\", \"?\")\n\n    html = _DASHBOARD_TEMPLATE.format(\n        timestamp=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        n_epochs=n_epochs,\n        batch_size=batch_size,\n        final_loss=f\"{losses[-1]:.6f}\" if losses else \"\u2014\",\n        final_acc=f\"{accs[-1]:.4f}\" if accs else \"\u2014\",\n        final_val_loss=f\"{val_losses[-1]:.6f}\" if val_losses else \"\u2014\",\n        final_val_acc=f\"{val_accs[-1]:.4f}\" if val_accs else \"\u2014\",\n        total_events=len(self._events),\n        n_warnings=sum(1 for h in self._health_events if h[\"severity\"] in (\"warning\", \"danger\", \"critical\")),\n        epochs=json.dumps(epochs_list),\n        losses=json.dumps(losses),\n        val_losses=json.dumps(val_losses),\n        accs=json.dumps(accs),\n        val_accs=json.dumps(val_accs),\n        epoch_times=json.dumps(epoch_times),\n        batch_losses=json.dumps(self._batch_losses[:2000]),  # cap for perf\n        grad_layers=json.dumps(grad_layers),\n        grad_matrix=json.dumps(grad_matrix),\n        health_rows=health_rows,\n        model_info=json.dumps(model_info, indent=2, default=str),\n    )\n\n    with open(self.path, \"w\", encoding=\"utf-8\") as f:\n        f.write(html)\n    return self.path\n</code></pre>"},{"location":"advanced/observatory-pro/#tensorboardbridge","title":"<code>TensorBoardBridge</code>","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.dashboard.TensorBoardBridge","title":"<code>neurogebra.logging.dashboard.TensorBoardBridge</code>","text":"<p>Write Training Observatory events to TensorBoard.</p> <p>Requires <code>tensorboard</code> to be installed (<code>pip install neurogebra[logging]</code>).</p> Source code in <code>src/neurogebra/logging/dashboard.py</code> <pre><code>class TensorBoardBridge:\n    \"\"\"\n    Write Training Observatory events to TensorBoard.\n\n    Requires ``tensorboard`` to be installed\n    (``pip install neurogebra[logging]``).\n    \"\"\"\n\n    def __init__(self, log_dir: str = \"./tb_logs\"):\n        self.log_dir = log_dir\n        self._writer = None\n        self._step = 0\n        try:\n            from torch.utils.tensorboard import SummaryWriter\n            self._writer = SummaryWriter(log_dir=log_dir)\n        except ImportError:\n            pass  # TensorBoard not available\n\n    @property\n    def available(self) -&gt; bool:\n        return self._writer is not None\n\n    def handle_event(self, event: LogEvent) -&gt; None:\n        if not self._writer:\n            return\n        if event.event_type == \"epoch_end\":\n            metrics = event.data.get(\"metrics\", {})\n            epoch = event.epoch or self._step\n            for key, val in metrics.items():\n                if isinstance(val, (int, float)):\n                    self._writer.add_scalar(f\"metrics/{key}\", val, epoch)\n            self._step += 1\n\n    def handle_epoch_end(self, event: LogEvent) -&gt; None:\n        self.handle_event(event)\n\n    def handle_health_check(self, event: LogEvent) -&gt; None:\n        if not self._writer:\n            return\n        self._writer.add_text(\n            \"health_checks\",\n            f\"**[{event.severity.upper()}]** {event.message}\",\n            self._step,\n        )\n\n    def close(self) -&gt; None:\n        if self._writer:\n            self._writer.close()\n</code></pre>"},{"location":"advanced/observatory-pro/#wandbbridge","title":"<code>WandBBridge</code>","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.dashboard.WandBBridge","title":"<code>neurogebra.logging.dashboard.WandBBridge</code>","text":"<p>Log Training Observatory events to Weights &amp; Biases.</p> <p>Requires <code>wandb</code> to be installed (<code>pip install neurogebra[logging]</code>).</p> Source code in <code>src/neurogebra/logging/dashboard.py</code> <pre><code>class WandBBridge:\n    \"\"\"\n    Log Training Observatory events to Weights &amp; Biases.\n\n    Requires ``wandb`` to be installed (``pip install neurogebra[logging]``).\n    \"\"\"\n\n    def __init__(self, project: str = \"neurogebra\", run_name: Optional[str] = None,\n                 config: Optional[Dict] = None):\n        self._run = None\n        try:\n            import wandb\n            self._run = wandb.init(\n                project=project,\n                name=run_name,\n                config=config or {},\n                reinit=True,\n            )\n        except ImportError:\n            pass\n\n    @property\n    def available(self) -&gt; bool:\n        return self._run is not None\n\n    def handle_event(self, event: LogEvent) -&gt; None:\n        if not self._run:\n            return\n        import wandb\n        if event.event_type == \"epoch_end\":\n            metrics = event.data.get(\"metrics\", {})\n            wandb.log({k: v for k, v in metrics.items() if isinstance(v, (int, float))},\n                      step=event.epoch)\n\n    def handle_epoch_end(self, event: LogEvent) -&gt; None:\n        self.handle_event(event)\n\n    def handle_health_check(self, event: LogEvent) -&gt; None:\n        if not self._run:\n            return\n        import wandb\n        wandb.alert(\n            title=f\"Health: {event.severity.upper()}\",\n            text=event.message,\n            level=wandb.AlertLevel.WARN if event.severity == \"warning\" else wandb.AlertLevel.ERROR,\n        )\n\n    def close(self) -&gt; None:\n        if self._run:\n            import wandb\n            wandb.finish()\n</code></pre>"},{"location":"advanced/observatory-pro/#trainingfingerprint","title":"<code>TrainingFingerprint</code>","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.fingerprint.TrainingFingerprint","title":"<code>neurogebra.logging.fingerprint.TrainingFingerprint</code>  <code>dataclass</code>","text":"<p>Immutable reproducibility block for a training run.</p> Source code in <code>src/neurogebra/logging/fingerprint.py</code> <pre><code>@dataclass\nclass TrainingFingerprint:\n    \"\"\"Immutable reproducibility block for a training run.\"\"\"\n\n    # Identifiers\n    run_id: str = \"\"\n    timestamp: str = \"\"\n    timestamp_unix: float = 0.0\n\n    # Seeds\n    random_seed: Optional[int] = None\n    numpy_seed: Optional[int] = None\n\n    # Dataset\n    dataset_hash: Optional[str] = None\n    dataset_shape: Optional[tuple] = None\n    dataset_dtype: Optional[str] = None\n    dataset_samples: Optional[int] = None\n\n    # Versions\n    neurogebra_version: str = \"\"\n    python_version: str = \"\"\n    numpy_version: str = \"\"\n    dependency_versions: Dict[str, str] = field(default_factory=dict)\n\n    # Hardware\n    cpu: str = \"\"\n    cpu_count: int = 0\n    ram_gb: float = 0.0\n    gpu: Optional[str] = None\n    os_info: str = \"\"\n    machine: str = \"\"\n\n    # Model\n    model_architecture_hash: Optional[str] = None\n    model_info: Dict[str, Any] = field(default_factory=dict)\n\n    # Hyperparameters\n    hyperparameters: Dict[str, Any] = field(default_factory=dict)\n\n    # Git\n    git_commit: Optional[str] = None\n    git_branch: Optional[str] = None\n    git_dirty: Optional[bool] = None\n\n    # ------------------------------------------------------------------\n    # Factory\n    # ------------------------------------------------------------------\n\n    @classmethod\n    def capture(\n        cls,\n        *,\n        model_info: Optional[Dict[str, Any]] = None,\n        hyperparameters: Optional[Dict[str, Any]] = None,\n        dataset: Optional[Union[np.ndarray, str]] = None,\n        random_seed: Optional[int] = None,\n    ) -&gt; \"TrainingFingerprint\":\n        \"\"\"\n        Capture the current environment and return a fingerprint.\n\n        Args:\n            model_info: Dict describing the model architecture.\n            hyperparameters: Training hyperparameters dict.\n            dataset: Either a numpy array (will be hashed) or a\n                     pre-computed hash string.\n            random_seed: The seed used for reproducibility.\n        \"\"\"\n        fp = cls()\n        fp.timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        fp.timestamp_unix = time.time()\n        fp.run_id = hashlib.md5(str(fp.timestamp_unix).encode()).hexdigest()[:12]\n\n        # Seeds\n        fp.random_seed = random_seed\n        fp.numpy_seed = random_seed  # typically same\n\n        # Dataset\n        if isinstance(dataset, np.ndarray):\n            fp.dataset_hash = hashlib.sha256(dataset.tobytes()).hexdigest()[:16]\n            fp.dataset_shape = dataset.shape\n            fp.dataset_dtype = str(dataset.dtype)\n            fp.dataset_samples = dataset.shape[0]\n        elif isinstance(dataset, str):\n            fp.dataset_hash = dataset\n\n        # Versions\n        fp.neurogebra_version = _get_neurogebra_version()\n        fp.python_version = platform.python_version()\n        fp.numpy_version = np.__version__\n        fp.dependency_versions = _get_dependency_versions()\n\n        # Hardware\n        fp.cpu = platform.processor() or platform.machine()\n        fp.cpu_count = os.cpu_count() or 0\n        fp.ram_gb = _get_ram_gb()\n        fp.gpu = _detect_gpu()\n        fp.os_info = f\"{platform.system()} {platform.release()}\"\n        fp.machine = platform.machine()\n\n        # Model\n        fp.model_info = model_info or {}\n        if model_info:\n            fp.model_architecture_hash = hashlib.md5(\n                str(sorted(model_info.items())).encode()\n            ).hexdigest()[:12]\n\n        # Hyperparameters\n        fp.hyperparameters = hyperparameters or {}\n\n        # Git\n        fp.git_commit, fp.git_branch, fp.git_dirty = _get_git_info()\n\n        return fp\n\n    # ------------------------------------------------------------------\n    # Serialisation\n    # ------------------------------------------------------------------\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Return a JSON-serialisable dict.\"\"\"\n        return {\n            \"run_id\": self.run_id,\n            \"timestamp\": self.timestamp,\n            \"timestamp_unix\": self.timestamp_unix,\n            \"seeds\": {\n                \"random_seed\": self.random_seed,\n                \"numpy_seed\": self.numpy_seed,\n            },\n            \"dataset\": {\n                \"hash\": self.dataset_hash,\n                \"shape\": list(self.dataset_shape) if self.dataset_shape else None,\n                \"dtype\": self.dataset_dtype,\n                \"samples\": self.dataset_samples,\n            },\n            \"versions\": {\n                \"neurogebra\": self.neurogebra_version,\n                \"python\": self.python_version,\n                \"numpy\": self.numpy_version,\n                **self.dependency_versions,\n            },\n            \"hardware\": {\n                \"cpu\": self.cpu,\n                \"cpu_count\": self.cpu_count,\n                \"ram_gb\": round(self.ram_gb, 2),\n                \"gpu\": self.gpu,\n                \"os\": self.os_info,\n                \"machine\": self.machine,\n            },\n            \"model\": {\n                \"architecture_hash\": self.model_architecture_hash,\n                \"info\": self.model_info,\n            },\n            \"hyperparameters\": self.hyperparameters,\n            \"git\": {\n                \"commit\": self.git_commit,\n                \"branch\": self.git_branch,\n                \"dirty\": self.git_dirty,\n            },\n        }\n\n    @classmethod\n    def from_dict(cls, d: Dict[str, Any]) -&gt; \"TrainingFingerprint\":\n        \"\"\"Reconstruct from a dict (e.g. loaded from JSON).\"\"\"\n        fp = cls()\n        fp.run_id = d.get(\"run_id\", \"\")\n        fp.timestamp = d.get(\"timestamp\", \"\")\n        fp.timestamp_unix = d.get(\"timestamp_unix\", 0.0)\n\n        seeds = d.get(\"seeds\", {})\n        fp.random_seed = seeds.get(\"random_seed\")\n        fp.numpy_seed = seeds.get(\"numpy_seed\")\n\n        ds = d.get(\"dataset\", {})\n        fp.dataset_hash = ds.get(\"hash\")\n        fp.dataset_shape = tuple(ds[\"shape\"]) if ds.get(\"shape\") else None\n        fp.dataset_dtype = ds.get(\"dtype\")\n        fp.dataset_samples = ds.get(\"samples\")\n\n        vers = d.get(\"versions\", {})\n        fp.neurogebra_version = vers.get(\"neurogebra\", \"\")\n        fp.python_version = vers.get(\"python\", \"\")\n        fp.numpy_version = vers.get(\"numpy\", \"\")\n        fp.dependency_versions = {\n            k: v for k, v in vers.items()\n            if k not in (\"neurogebra\", \"python\", \"numpy\")\n        }\n\n        hw = d.get(\"hardware\", {})\n        fp.cpu = hw.get(\"cpu\", \"\")\n        fp.cpu_count = hw.get(\"cpu_count\", 0)\n        fp.ram_gb = hw.get(\"ram_gb\", 0.0)\n        fp.gpu = hw.get(\"gpu\")\n        fp.os_info = hw.get(\"os\", \"\")\n        fp.machine = hw.get(\"machine\", \"\")\n\n        model = d.get(\"model\", {})\n        fp.model_architecture_hash = model.get(\"architecture_hash\")\n        fp.model_info = model.get(\"info\", {})\n\n        fp.hyperparameters = d.get(\"hyperparameters\", {})\n\n        git = d.get(\"git\", {})\n        fp.git_commit = git.get(\"commit\")\n        fp.git_branch = git.get(\"branch\")\n        fp.git_dirty = git.get(\"dirty\")\n\n        return fp\n\n    def format_text(self) -&gt; str:\n        \"\"\"Human-readable fingerprint summary.\"\"\"\n        lines = [\n            f\"\u2554\u2550\u2550 Training Fingerprint \u2550\u2550\u2557\",\n            f\"  Run ID:       {self.run_id}\",\n            f\"  Timestamp:    {self.timestamp}\",\n            f\"  Seed:         {self.random_seed}\",\n        ]\n        if self.dataset_hash:\n            lines.append(f\"  Dataset Hash: {self.dataset_hash}\")\n        if self.dataset_shape:\n            lines.append(f\"  Dataset:      {self.dataset_shape} ({self.dataset_dtype})\")\n        lines.extend([\n            f\"  Neurogebra:   {self.neurogebra_version}\",\n            f\"  Python:       {self.python_version}\",\n            f\"  NumPy:        {self.numpy_version}\",\n            f\"  CPU:          {self.cpu} ({self.cpu_count} cores)\",\n            f\"  RAM:          {self.ram_gb:.1f} GB\",\n            f\"  GPU:          {self.gpu or 'None'}\",\n            f\"  OS:           {self.os_info}\",\n        ])\n        if self.git_commit:\n            dirty = \" (dirty)\" if self.git_dirty else \"\"\n            lines.append(f\"  Git:          {self.git_branch}@{self.git_commit[:8]}{dirty}\")\n        if self.model_architecture_hash:\n            lines.append(f\"  Model Hash:   {self.model_architecture_hash}\")\n        if self.hyperparameters:\n            lines.append(f\"  Hyperparams:  {self.hyperparameters}\")\n        lines.append(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\")\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.fingerprint.TrainingFingerprint-functions","title":"Functions","text":""},{"location":"advanced/observatory-pro/#neurogebra.logging.fingerprint.TrainingFingerprint.capture","title":"<code>capture(*, model_info=None, hyperparameters=None, dataset=None, random_seed=None)</code>  <code>classmethod</code>","text":"<p>Capture the current environment and return a fingerprint.</p> <p>Parameters:</p> Name Type Description Default <code>model_info</code> <code>Optional[Dict[str, Any]]</code> <p>Dict describing the model architecture.</p> <code>None</code> <code>hyperparameters</code> <code>Optional[Dict[str, Any]]</code> <p>Training hyperparameters dict.</p> <code>None</code> <code>dataset</code> <code>Optional[Union[ndarray, str]]</code> <p>Either a numpy array (will be hashed) or a      pre-computed hash string.</p> <code>None</code> <code>random_seed</code> <code>Optional[int]</code> <p>The seed used for reproducibility.</p> <code>None</code> Source code in <code>src/neurogebra/logging/fingerprint.py</code> <pre><code>@classmethod\ndef capture(\n    cls,\n    *,\n    model_info: Optional[Dict[str, Any]] = None,\n    hyperparameters: Optional[Dict[str, Any]] = None,\n    dataset: Optional[Union[np.ndarray, str]] = None,\n    random_seed: Optional[int] = None,\n) -&gt; \"TrainingFingerprint\":\n    \"\"\"\n    Capture the current environment and return a fingerprint.\n\n    Args:\n        model_info: Dict describing the model architecture.\n        hyperparameters: Training hyperparameters dict.\n        dataset: Either a numpy array (will be hashed) or a\n                 pre-computed hash string.\n        random_seed: The seed used for reproducibility.\n    \"\"\"\n    fp = cls()\n    fp.timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    fp.timestamp_unix = time.time()\n    fp.run_id = hashlib.md5(str(fp.timestamp_unix).encode()).hexdigest()[:12]\n\n    # Seeds\n    fp.random_seed = random_seed\n    fp.numpy_seed = random_seed  # typically same\n\n    # Dataset\n    if isinstance(dataset, np.ndarray):\n        fp.dataset_hash = hashlib.sha256(dataset.tobytes()).hexdigest()[:16]\n        fp.dataset_shape = dataset.shape\n        fp.dataset_dtype = str(dataset.dtype)\n        fp.dataset_samples = dataset.shape[0]\n    elif isinstance(dataset, str):\n        fp.dataset_hash = dataset\n\n    # Versions\n    fp.neurogebra_version = _get_neurogebra_version()\n    fp.python_version = platform.python_version()\n    fp.numpy_version = np.__version__\n    fp.dependency_versions = _get_dependency_versions()\n\n    # Hardware\n    fp.cpu = platform.processor() or platform.machine()\n    fp.cpu_count = os.cpu_count() or 0\n    fp.ram_gb = _get_ram_gb()\n    fp.gpu = _detect_gpu()\n    fp.os_info = f\"{platform.system()} {platform.release()}\"\n    fp.machine = platform.machine()\n\n    # Model\n    fp.model_info = model_info or {}\n    if model_info:\n        fp.model_architecture_hash = hashlib.md5(\n            str(sorted(model_info.items())).encode()\n        ).hexdigest()[:12]\n\n    # Hyperparameters\n    fp.hyperparameters = hyperparameters or {}\n\n    # Git\n    fp.git_commit, fp.git_branch, fp.git_dirty = _get_git_info()\n\n    return fp\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.fingerprint.TrainingFingerprint.to_dict","title":"<code>to_dict()</code>","text":"<p>Return a JSON-serialisable dict.</p> Source code in <code>src/neurogebra/logging/fingerprint.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Return a JSON-serialisable dict.\"\"\"\n    return {\n        \"run_id\": self.run_id,\n        \"timestamp\": self.timestamp,\n        \"timestamp_unix\": self.timestamp_unix,\n        \"seeds\": {\n            \"random_seed\": self.random_seed,\n            \"numpy_seed\": self.numpy_seed,\n        },\n        \"dataset\": {\n            \"hash\": self.dataset_hash,\n            \"shape\": list(self.dataset_shape) if self.dataset_shape else None,\n            \"dtype\": self.dataset_dtype,\n            \"samples\": self.dataset_samples,\n        },\n        \"versions\": {\n            \"neurogebra\": self.neurogebra_version,\n            \"python\": self.python_version,\n            \"numpy\": self.numpy_version,\n            **self.dependency_versions,\n        },\n        \"hardware\": {\n            \"cpu\": self.cpu,\n            \"cpu_count\": self.cpu_count,\n            \"ram_gb\": round(self.ram_gb, 2),\n            \"gpu\": self.gpu,\n            \"os\": self.os_info,\n            \"machine\": self.machine,\n        },\n        \"model\": {\n            \"architecture_hash\": self.model_architecture_hash,\n            \"info\": self.model_info,\n        },\n        \"hyperparameters\": self.hyperparameters,\n        \"git\": {\n            \"commit\": self.git_commit,\n            \"branch\": self.git_branch,\n            \"dirty\": self.git_dirty,\n        },\n    }\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.fingerprint.TrainingFingerprint.from_dict","title":"<code>from_dict(d)</code>  <code>classmethod</code>","text":"<p>Reconstruct from a dict (e.g. loaded from JSON).</p> Source code in <code>src/neurogebra/logging/fingerprint.py</code> <pre><code>@classmethod\ndef from_dict(cls, d: Dict[str, Any]) -&gt; \"TrainingFingerprint\":\n    \"\"\"Reconstruct from a dict (e.g. loaded from JSON).\"\"\"\n    fp = cls()\n    fp.run_id = d.get(\"run_id\", \"\")\n    fp.timestamp = d.get(\"timestamp\", \"\")\n    fp.timestamp_unix = d.get(\"timestamp_unix\", 0.0)\n\n    seeds = d.get(\"seeds\", {})\n    fp.random_seed = seeds.get(\"random_seed\")\n    fp.numpy_seed = seeds.get(\"numpy_seed\")\n\n    ds = d.get(\"dataset\", {})\n    fp.dataset_hash = ds.get(\"hash\")\n    fp.dataset_shape = tuple(ds[\"shape\"]) if ds.get(\"shape\") else None\n    fp.dataset_dtype = ds.get(\"dtype\")\n    fp.dataset_samples = ds.get(\"samples\")\n\n    vers = d.get(\"versions\", {})\n    fp.neurogebra_version = vers.get(\"neurogebra\", \"\")\n    fp.python_version = vers.get(\"python\", \"\")\n    fp.numpy_version = vers.get(\"numpy\", \"\")\n    fp.dependency_versions = {\n        k: v for k, v in vers.items()\n        if k not in (\"neurogebra\", \"python\", \"numpy\")\n    }\n\n    hw = d.get(\"hardware\", {})\n    fp.cpu = hw.get(\"cpu\", \"\")\n    fp.cpu_count = hw.get(\"cpu_count\", 0)\n    fp.ram_gb = hw.get(\"ram_gb\", 0.0)\n    fp.gpu = hw.get(\"gpu\")\n    fp.os_info = hw.get(\"os\", \"\")\n    fp.machine = hw.get(\"machine\", \"\")\n\n    model = d.get(\"model\", {})\n    fp.model_architecture_hash = model.get(\"architecture_hash\")\n    fp.model_info = model.get(\"info\", {})\n\n    fp.hyperparameters = d.get(\"hyperparameters\", {})\n\n    git = d.get(\"git\", {})\n    fp.git_commit = git.get(\"commit\")\n    fp.git_branch = git.get(\"branch\")\n    fp.git_dirty = git.get(\"dirty\")\n\n    return fp\n</code></pre>"},{"location":"advanced/observatory-pro/#neurogebra.logging.fingerprint.TrainingFingerprint.format_text","title":"<code>format_text()</code>","text":"<p>Human-readable fingerprint summary.</p> Source code in <code>src/neurogebra/logging/fingerprint.py</code> <pre><code>def format_text(self) -&gt; str:\n    \"\"\"Human-readable fingerprint summary.\"\"\"\n    lines = [\n        f\"\u2554\u2550\u2550 Training Fingerprint \u2550\u2550\u2557\",\n        f\"  Run ID:       {self.run_id}\",\n        f\"  Timestamp:    {self.timestamp}\",\n        f\"  Seed:         {self.random_seed}\",\n    ]\n    if self.dataset_hash:\n        lines.append(f\"  Dataset Hash: {self.dataset_hash}\")\n    if self.dataset_shape:\n        lines.append(f\"  Dataset:      {self.dataset_shape} ({self.dataset_dtype})\")\n    lines.extend([\n        f\"  Neurogebra:   {self.neurogebra_version}\",\n        f\"  Python:       {self.python_version}\",\n        f\"  NumPy:        {self.numpy_version}\",\n        f\"  CPU:          {self.cpu} ({self.cpu_count} cores)\",\n        f\"  RAM:          {self.ram_gb:.1f} GB\",\n        f\"  GPU:          {self.gpu or 'None'}\",\n        f\"  OS:           {self.os_info}\",\n    ])\n    if self.git_commit:\n        dirty = \" (dirty)\" if self.git_dirty else \"\"\n        lines.append(f\"  Git:          {self.git_branch}@{self.git_commit[:8]}{dirty}\")\n    if self.model_architecture_hash:\n        lines.append(f\"  Model Hash:   {self.model_architecture_hash}\")\n    if self.hyperparameters:\n        lines.append(f\"  Hyperparams:  {self.hyperparameters}\")\n    lines.append(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\")\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"advanced/optimization/","title":"Optimization","text":"<p>Optimizers control how your model updates its weights during training. Choosing the right optimizer can be the difference between a model that converges in 10 epochs vs. one that never learns.</p>"},{"location":"advanced/optimization/#what-is-optimization","title":"What is Optimization?","text":"<p>Training = finding the weights that minimize the loss function.</p> <pre><code>Current weights \u2192  Compute loss  \u2192  Compute gradients  \u2192  Update weights\n      \u2191                                                          |\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nThe OPTIMIZER decides how to use the gradients to update weights.\n</code></pre>"},{"location":"advanced/optimization/#sgd-stochastic-gradient-descent","title":"SGD (Stochastic Gradient Descent)","text":"<p>The simplest optimizer: move in the direction opposite to the gradient.</p> \\[w_{new} = w_{old} - \\eta \\cdot \\nabla L\\] <p>Where \\(\\eta\\) is the learning rate.</p> <pre><code>from neurogebra import MathForge, Expression\nfrom neurogebra.core.trainer import Trainer\nimport numpy as np\n\nforge = MathForge()\n\nmodel = Expression(\"linear\", \"w*x + b\",\n    params={\"w\": 0.0, \"b\": 0.0}, trainable_params=[\"w\", \"b\"])\nloss = forge.get(\"mse\")\n\n# SGD optimizer\ntrainer = Trainer(model, loss, optimizer=\"sgd\", lr=0.01)\n\nX = np.linspace(0, 10, 50)\ny = 3 * X + 2 + np.random.normal(0, 0.5, 50)\nhistory = trainer.fit(X, y, epochs=100)\n</code></pre>"},{"location":"advanced/optimization/#pros-and-cons","title":"Pros and Cons","text":"\u2705 Pros \u274c Cons Simple to understand Can be slow to converge Low memory usage Sensitive to learning rate Good for convex problems Gets stuck in local minima"},{"location":"advanced/optimization/#adam-adaptive-moment-estimation","title":"Adam (Adaptive Moment Estimation)","text":"<p>The most popular optimizer. It adapts the learning rate for each parameter individually.</p> <p>Adam combines:</p> <ul> <li>Momentum: Remembers past gradients (like a ball rolling downhill)</li> <li>Adaptive learning rate: Different learning rates for different parameters</li> </ul> <pre><code># Adam optimizer (recommended for most cases)\ntrainer = Trainer(model, loss, optimizer=\"adam\", lr=0.001)\nhistory = trainer.fit(X, y, epochs=100)\n</code></pre>"},{"location":"advanced/optimization/#pros-and-cons_1","title":"Pros and Cons","text":"\u2705 Pros \u274c Cons Fast convergence Slightly more memory Works well out-of-the-box May not generalize as well as SGD Handles sparse gradients Can overshoot minimum Good default for most problems"},{"location":"advanced/optimization/#sgd-vs-adam-when-to-use-what","title":"SGD vs Adam: When to Use What","text":"Scenario Recommended Why First attempt Adam Works well with defaults Simple regression SGD Sufficient for convex problems Deep neural networks Adam Handles complex loss landscapes Need best generalization SGD (with tuning) Often finds flatter minima Quick prototyping Adam Less hyperparameter tuning"},{"location":"advanced/optimization/#learning-rate","title":"Learning Rate","text":"<p>The most important hyperparameter. Controls step size:</p> \\[w_{new} = w_{old} - \\underbrace{\\eta}_{\\text{learning rate}} \\cdot \\nabla L\\] <pre><code>import numpy as np\nfrom neurogebra import Expression\nfrom neurogebra.core.trainer import Trainer\n\n# Too small: slow convergence\ntrainer_slow = Trainer(model, loss, optimizer=\"adam\", lr=0.0001)\n\n# Just right: converges nicely\ntrainer_good = Trainer(model, loss, optimizer=\"adam\", lr=0.001)\n\n# Too large: overshoots, loss bounces\ntrainer_fast = Trainer(model, loss, optimizer=\"adam\", lr=0.1)\n</code></pre>"},{"location":"advanced/optimization/#learning-rate-effects","title":"Learning Rate Effects","text":"<pre><code>Learning Rate Too Small:\nLoss: \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591 (barely decreasing after 1000 epochs)\n\nLearning Rate Just Right:\nLoss: \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 (steadily decreasing, converges at ~200 epochs)\n\nLearning Rate Too Large:\nLoss: \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 (bouncing around, never converges)\n</code></pre>"},{"location":"advanced/optimization/#recommended-starting-points","title":"Recommended Starting Points","text":"Optimizer Starting LR Range to Try SGD 0.01 0.001 - 0.1 Adam 0.001 0.0001 - 0.01"},{"location":"advanced/optimization/#manual-gradient-descent","title":"Manual Gradient Descent","text":"<p>For understanding, you can implement optimization manually:</p> <pre><code>from neurogebra.core.autograd import Value\nimport numpy as np\n\n# Simple optimization: find x that minimizes x^2 - 4x + 4  (answer: x=2)\nx = Value(0.0)  # Start at x=0\nlr = 0.1\n\nfor epoch in range(50):\n    # Forward pass\n    loss = x**2 - 4*x + 4\n\n    # Backward pass\n    loss.backward()\n\n    # Update\n    x.data -= lr * x.grad\n\n    # Zero gradients\n    x.grad = 0.0\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}: x = {x.data:.4f}, loss = {loss.data:.4f}\")\n\nprint(f\"\\nOptimal x = {x.data:.4f}\")  # Should be \u2248 2.0\n</code></pre>"},{"location":"advanced/optimization/#batch-size-and-optimization","title":"Batch Size and Optimization","text":"<p>How much data you use per update also matters:</p> <pre><code># Full batch: use all data per update\ntrainer = Trainer(model, loss, optimizer=\"adam\", lr=0.001)\nhistory = trainer.fit(X, y, epochs=100, batch_size=len(X))\n\n# Mini-batch: use 32 samples per update (most common)\nhistory = trainer.fit(X, y, epochs=100, batch_size=32)\n\n# Stochastic: use 1 sample per update\nhistory = trainer.fit(X, y, epochs=100, batch_size=1)\n</code></pre> Batch Size Speed Stability Memory Full batch Slow per epoch Stable High Mini-batch (32) Good balance Moderate noise Moderate Single (1) Fast per epoch Very noisy Low <p>Recommendation: Start with batch_size=32.</p>"},{"location":"advanced/optimization/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Start with Adam, lr=0.001 \u2014 works for 90% of cases</li> <li>Watch the loss curve \u2014 should decrease smoothly</li> <li>If loss plateaus \u2014 try reducing learning rate by 10x</li> <li>If loss bounces \u2014 reduce learning rate</li> <li>If loss NaN \u2014 learning rate is way too high</li> <li>Train longer if needed \u2014 some problems need more epochs</li> </ol> <p>Next: Performance Tips \u2192</p>"},{"location":"advanced/performance/","title":"Performance Tips","text":"<p>Tips to make your Neurogebra code run faster and use less memory.</p>"},{"location":"advanced/performance/#1-use-numpy-arrays-not-python-lists","title":"1. Use NumPy Arrays, Not Python Lists","text":"<pre><code>import numpy as np\nfrom neurogebra import MathForge\n\nforge = MathForge()\nrelu = forge.get(\"relu\")\n\n# \u274c SLOW: Python list\nx_list = [1, 2, 3, 4, 5]\nresult = [relu.eval(x=val) for val in x_list]\n\n# \u2705 FAST: NumPy array (vectorized)\nx_array = np.array([1, 2, 3, 4, 5])\nresult = relu.eval(x=x_array)\n</code></pre> <p>NumPy arrays are 100-1000x faster for mathematical operations.</p>"},{"location":"advanced/performance/#2-batch-evaluation","title":"2. Batch Evaluation","text":"<p>Evaluate many points at once instead of one at a time:</p> <pre><code>from neurogebra import Expression\nimport numpy as np\n\nmodel = Expression(\"quadratic\", \"a*x**2 + b*x + c\",\n    params={\"a\": 1.0, \"b\": -2.0, \"c\": 1.0})\n\n# \u274c SLOW: One at a time\nresults = []\nfor x_val in range(1000):\n    results.append(model.eval(x=x_val))\n\n# \u2705 FAST: All at once\nx = np.arange(1000)\nresults = model.eval(x=x)\n</code></pre>"},{"location":"advanced/performance/#3-pre-compute-gradients","title":"3. Pre-compute Gradients","text":"<p>If you need the same gradient repeatedly, compute it once:</p> <pre><code>from neurogebra import MathForge\n\nforge = MathForge()\nsigmoid = forge.get(\"sigmoid\")\n\n# \u274c SLOW: Compute gradient each time\nfor x_val in range(100):\n    grad = sigmoid.gradient(\"x\")      # Recalculates symbolic gradient each time\n    result = grad.eval(x=float(x_val))\n\n# \u2705 FAST: Compute gradient once, evaluate many times\ngrad = sigmoid.gradient(\"x\")          # Compute once\nx = np.arange(100, dtype=float)\nresults = grad.eval(x=x)              # Evaluate all at once\n</code></pre>"},{"location":"advanced/performance/#4-use-appropriate-batch-sizes-for-training","title":"4. Use Appropriate Batch Sizes for Training","text":"<pre><code>from neurogebra.core.trainer import Trainer\n\n# For small datasets (&lt; 1000 samples)\n# Use full batch \u2014 no overhead from splitting\ntrainer.fit(X, y, epochs=100, batch_size=len(X))\n\n# For medium datasets (1000 - 100000 samples)\n# Use mini-batches of 32-128\ntrainer.fit(X, y, epochs=100, batch_size=32)\n\n# For large datasets (&gt; 100000 samples)\n# Use mini-batches of 64-256\ntrainer.fit(X, y, epochs=50, batch_size=128)\n</code></pre>"},{"location":"advanced/performance/#5-use-the-right-optimizer","title":"5. Use the Right Optimizer","text":"<p>Adam converges faster than SGD in most cases:</p> <pre><code># \u274c May need 1000 epochs to converge\ntrainer = Trainer(model, loss, optimizer=\"sgd\", lr=0.01)\n\n# \u2705 Often converges in 100-200 epochs\ntrainer = Trainer(model, loss, optimizer=\"adam\", lr=0.001)\n</code></pre>"},{"location":"advanced/performance/#6-avoid-unnecessary-expression-copies","title":"6. Avoid Unnecessary Expression Copies","text":"<pre><code>from neurogebra import MathForge\n\nforge = MathForge()\n\n# \u274c Creates new Expression object each time\nfor i in range(100):\n    relu = forge.get(\"relu\")        # Unnecessary repeated lookup\n    result = relu.eval(x=float(i))\n\n# \u2705 Get once, use many times\nrelu = forge.get(\"relu\")\nfor i in range(100):\n    result = relu.eval(x=float(i))\n\n# \u2705\u2705 Even better: vectorized\nrelu = forge.get(\"relu\")\nx = np.arange(100, dtype=float)\nresults = relu.eval(x=x)\n</code></pre>"},{"location":"advanced/performance/#7-memory-management-with-autograd","title":"7. Memory Management with Autograd","text":"<p>When training manually with <code>Value</code>/<code>Tensor</code>, always zero gradients:</p> <pre><code>from neurogebra.core.autograd import Value\n\nx = Value(2.0)\n\nfor epoch in range(1000):\n    y = x**2 + 3*x\n    y.backward()\n\n    x.data -= 0.01 * x.grad\n    x.grad = 0.0      # \u2190 CRITICAL: prevents gradient accumulation\n</code></pre>"},{"location":"advanced/performance/#8-profile-your-code","title":"8. Profile Your Code","text":"<p>Find bottlenecks with Python's built-in profiler:</p> <pre><code>import time\n\nstart = time.time()\n\n# Your Neurogebra code here\nforge = MathForge()\nsigmoid = forge.get(\"sigmoid\")\nx = np.linspace(-5, 5, 10000)\nresult = sigmoid.eval(x=x)\n\nend = time.time()\nprint(f\"Time: {end - start:.4f} seconds\")\n</code></pre>"},{"location":"advanced/performance/#performance-cheat-sheet","title":"Performance Cheat Sheet","text":"Tip Impact Effort Use NumPy arrays \ud83d\ude80\ud83d\ude80\ud83d\ude80 Low Batch evaluation \ud83d\ude80\ud83d\ude80\ud83d\ude80 Low Pre-compute gradients \ud83d\ude80\ud83d\ude80 Low Use Adam optimizer \ud83d\ude80\ud83d\ude80 Low Appropriate batch size \ud83d\ude80 Low Cache expression lookups \ud83d\ude80 Low Zero gradients \ud83d\ude80 (correctness) Low Profile bottlenecks \ud83d\ude80\ud83d\ude80 Medium <p>Next: Project 1: Linear Regression \u2192</p>"},{"location":"advanced/regularization/","title":"Regularization","text":"<p>Regularization prevents overfitting \u2014 when a model memorizes training data instead of learning general patterns.</p>"},{"location":"advanced/regularization/#what-is-overfitting","title":"What is Overfitting?","text":"<pre><code>Training accuracy: 99%    \u2190 model memorized training data\nTest accuracy:     60%    \u2190 fails on new data\n                          = OVERFITTING\n\nTraining accuracy: 85%\nTest accuracy:     83%    \u2190 similar performance\n                          = GOOD GENERALIZATION\n</code></pre> <p>Regularization adds a penalty to the loss function that discourages complex models.</p>"},{"location":"advanced/regularization/#types-of-regularization","title":"Types of Regularization","text":""},{"location":"advanced/regularization/#l1-regularization-lasso","title":"L1 Regularization (Lasso)","text":"<p>Pushes some weights to exactly zero \u2014 acts as feature selection.</p> \\[L_{total} = L_{data} + \\lambda \\sum |w_i|\\] <pre><code>from neurogebra import MathForge\n\nforge = MathForge()\nl1 = forge.get(\"l1_regularizer\")\n\nprint(l1.explain())\nprint(l1.eval(w=0.5, lambda_=0.01))\n</code></pre> <p>When to use: You suspect many features are irrelevant and want automatic feature selection.</p>"},{"location":"advanced/regularization/#l2-regularization-ridge","title":"L2 Regularization (Ridge)","text":"<p>Pushes all weights toward small values \u2014 prevents any one weight from dominating.</p> \\[L_{total} = L_{data} + \\lambda \\sum w_i^2\\] <pre><code>l2 = forge.get(\"l2_regularizer\")\n\nprint(l2.explain())\nprint(l2.eval(w=0.5, lambda_=0.01))\n</code></pre> <p>When to use: All features might be relevant but you want to prevent large weights.</p>"},{"location":"advanced/regularization/#elastic-net","title":"Elastic Net","text":"<p>Combines L1 and L2 \u2014 best of both worlds:</p> \\[L_{total} = L_{data} + \\lambda_1 \\sum |w_i| + \\lambda_2 \\sum w_i^2\\] <pre><code>elastic = forge.get(\"elastic_net\")\n\nprint(elastic.explain())\nprint(elastic.eval(w=0.5, lambda1=0.01, lambda2=0.01))\n</code></pre> <p>When to use: When you want both feature selection (L1) and small weights (L2).</p>"},{"location":"advanced/regularization/#comparison","title":"Comparison","text":"Type Effect on Weights Feature Selection Best For L1 (Lasso) Some become 0 \u2705 Yes Sparse models L2 (Ridge) All become small \u274c No Preventing large weights Elastic Net Mix of both \u2705 Partial General use"},{"location":"advanced/regularization/#adding-regularization-to-training","title":"Adding Regularization to Training","text":""},{"location":"advanced/regularization/#step-by-step","title":"Step-by-Step","text":"<pre><code>import numpy as np\nfrom neurogebra import MathForge, Expression\nfrom neurogebra.core.trainer import Trainer\n\nforge = MathForge()\n\n# 1. Create model\nmodel = Expression(\n    \"linear_model\",\n    \"w1*x1 + w2*x2 + w3*x3 + b\",\n    params={\"w1\": 0.5, \"w2\": 0.5, \"w3\": 0.5, \"b\": 0.0},\n    trainable_params=[\"w1\", \"w2\", \"w3\", \"b\"]\n)\n\n# 2. Get loss and regularizer\nmse = forge.get(\"mse\")\nl2 = forge.get(\"l2_regularizer\")\n\n# 3. Create regularized loss (manually)\n# total_loss = mse_loss + lambda * sum(w^2)\nlambda_reg = 0.01\n\n# 4. Train with the combined loss\ntrainer = Trainer(model, mse, optimizer=\"adam\", lr=0.01)\n</code></pre>"},{"location":"advanced/regularization/#regularization-strength-lambda","title":"Regularization Strength (\\(\\lambda\\))","text":"<p>The \\(\\lambda\\) parameter controls how much regularization to apply:</p> \\(\\lambda\\) Value Effect 0.0 No regularization (may overfit) 0.001 Light regularization (usually good start) 0.01 Moderate regularization 0.1 Strong regularization (may underfit) 1.0 Very strong (likely underfitting) <pre><code># Experiment with different lambda values\nfor lam in [0.0, 0.001, 0.01, 0.1]:\n    penalty = l2.eval(w=0.5, lambda_=lam)\n    print(f\"\u03bb={lam:.3f} \u2192 L2 penalty = {penalty:.6f}\")\n</code></pre> <p>Rule of thumb: Start with \\(\\lambda = 0.001\\) and adjust based on validation performance.</p>"},{"location":"advanced/regularization/#dropout-concept","title":"Dropout (Concept)","text":"<p>Another form of regularization \u2014 randomly \"turns off\" neurons during training:</p> <pre><code>from neurogebra.builders.model_builder import ModelBuilder\n\nbuilder = ModelBuilder()\nmodel = builder.sequential([\n    {\"type\": \"dense\", \"units\": 128, \"activation\": \"relu\"},\n    {\"type\": \"dropout\", \"rate\": 0.3},       # 30% of neurons turned off randomly\n    {\"type\": \"dense\", \"units\": 64, \"activation\": \"relu\"},\n    {\"type\": \"dropout\", \"rate\": 0.2},       # 20% of neurons turned off\n    {\"type\": \"dense\", \"units\": 10, \"activation\": \"softmax\"}\n])\n</code></pre>"},{"location":"advanced/regularization/#quick-decision-guide","title":"Quick Decision Guide","text":"<pre><code>Is your model overfitting?\n\u251c\u2500\u2500 YES \u2192 Add regularization\n\u2502   \u251c\u2500\u2500 Too many features? \u2192 Use L1 (Lasso)\n\u2502   \u251c\u2500\u2500 Weights too large? \u2192 Use L2 (Ridge)\n\u2502   \u251c\u2500\u2500 Not sure? \u2192 Use Elastic Net or L2\n\u2502   \u2514\u2500\u2500 Neural network? \u2192 Use Dropout + L2\n\u2514\u2500\u2500 NO \u2192 You might not need regularization\n</code></pre> <p>Next: Optimization \u2192</p>"},{"location":"advanced/visualization/","title":"Visualization","text":"<p>Neurogebra includes built-in tools to visualize expressions, gradients, and training progress.</p>"},{"location":"advanced/visualization/#setup","title":"Setup","text":"<pre><code>pip install neurogebra[viz]\n# or\npip install matplotlib\n</code></pre>"},{"location":"advanced/visualization/#plotting-expressions","title":"Plotting Expressions","text":""},{"location":"advanced/visualization/#single-expression","title":"Single Expression","text":"<pre><code>from neurogebra import MathForge\nfrom neurogebra.viz.plotting import plot_expression\n\nforge = MathForge()\nsigmoid = forge.get(\"sigmoid\")\n\n# Plot from x=-5 to x=5\nplot_expression(sigmoid, x_range=(-5, 5), title=\"Sigmoid Function\")\n</code></pre>"},{"location":"advanced/visualization/#multiple-expressions-side-by-side","title":"Multiple Expressions Side by Side","text":"<pre><code>from neurogebra.viz.plotting import plot_comparison\n\nforge = MathForge()\n\nactivations = {\n    \"ReLU\": forge.get(\"relu\"),\n    \"Sigmoid\": forge.get(\"sigmoid\"),\n    \"Tanh\": forge.get(\"tanh\"),\n    \"Swish\": forge.get(\"swish\")\n}\n\nplot_comparison(\n    activations,\n    x_range=(-5, 5),\n    title=\"Activation Function Comparison\"\n)\n</code></pre>"},{"location":"advanced/visualization/#plotting-gradients","title":"Plotting Gradients","text":"<p>Visualize how gradients behave \u2014 essential for understanding vanishing/exploding gradients:</p> <pre><code>from neurogebra.viz.plotting import plot_gradient\n\nforge = MathForge()\nsigmoid = forge.get(\"sigmoid\")\n\n# Plot function and its gradient together\nplot_gradient(\n    sigmoid,\n    x_range=(-5, 5),\n    title=\"Sigmoid and Its Gradient\"\n)\n</code></pre>"},{"location":"advanced/visualization/#training-history","title":"Training History","text":"<p>After training, visualize how the loss decreased:</p> <pre><code>from neurogebra import MathForge, Expression\nfrom neurogebra.core.trainer import Trainer\nfrom neurogebra.viz.plotting import plot_training_history\nimport numpy as np\n\n# Setup\nforge = MathForge()\nmodel = Expression(\"linear\", \"w * x + b\", params={\"w\": 0.0, \"b\": 0.0}, trainable_params=[\"w\", \"b\"])\n\n# Generate data\nX = np.linspace(0, 10, 50)\ny = 2.5 * X + 1.0 + np.random.normal(0, 0.5, 50)\n\n# Train\nloss_fn = forge.get(\"mse\")\ntrainer = Trainer(model, loss_fn, optimizer=\"adam\", lr=0.01)\nhistory = trainer.fit(X, y, epochs=100)\n\n# Plot the training history\nplot_training_history(history, title=\"Training Progress\")\n</code></pre>"},{"location":"advanced/visualization/#manual-visualization-with-matplotlib","title":"Manual Visualization with Matplotlib","text":"<p>You can also create custom plots directly:</p>"},{"location":"advanced/visualization/#expression-evaluation-plot","title":"Expression Evaluation Plot","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom neurogebra import MathForge\n\nforge = MathForge()\nrelu = forge.get(\"relu\")\nsigmoid = forge.get(\"sigmoid\")\n\nx = np.linspace(-5, 5, 200)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, relu.eval(x=x), label=\"ReLU\", linewidth=2)\nplt.plot(x, sigmoid.eval(x=x), label=\"Sigmoid\", linewidth=2)\nplt.axhline(y=0, color='k', linewidth=0.5)\nplt.axvline(x=0, color='k', linewidth=0.5)\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Activation Functions\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"advanced/visualization/#gradient-comparison-plot","title":"Gradient Comparison Plot","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom neurogebra import MathForge\n\nforge = MathForge()\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nx = np.linspace(-5, 5, 200)\n\nfor ax, name in zip(axes.flat, [\"relu\", \"sigmoid\", \"tanh\", \"swish\"]):\n    expr = forge.get(name)\n    grad = expr.gradient(\"x\")\n\n    ax.plot(x, expr.eval(x=x), label=f\"{name}(x)\", linewidth=2)\n    ax.plot(x, grad.eval(x=x), label=f\"{name}'(x)\", linewidth=2, linestyle=\"--\")\n    ax.set_title(name.upper())\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.axhline(y=0, color='k', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"advanced/visualization/#training-loss-curve","title":"Training Loss Curve","text":"<pre><code>import matplotlib.pyplot as plt\n\n# After training, plot manually:\n# history = trainer.fit(X, y, epochs=100)\n\nplt.figure(figsize=(8, 5))\nplt.plot(history[\"loss\"], linewidth=2)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Over Time\")\nplt.grid(True, alpha=0.3)\nplt.yscale(\"log\")  # Log scale to see details\nplt.show()\n</code></pre>"},{"location":"advanced/visualization/#interactive-visualization","title":"Interactive Visualization","text":"<p>For Jupyter notebooks:</p> <pre><code>from neurogebra.viz.interactive import interactive_plot\n\nforge = MathForge()\nsigmoid = forge.get(\"sigmoid\")\n\n# Interactive slider for parameters\ninteractive_plot(sigmoid, x_range=(-5, 5))\n</code></pre>"},{"location":"advanced/visualization/#visualization-best-practices","title":"Visualization Best Practices","text":"Tip Why Always label axes Readability Use <code>grid(True, alpha=0.3)</code> Easier to read values Compare related functions together Better understanding Plot gradients with functions See the relationship Use log scale for loss curves See small improvements Save with <code>plt.savefig(\"plot.png\", dpi=150)</code> High quality output <p>Next: Regularization \u2192</p>"},{"location":"api/reference/","title":"API Reference","text":"<p>Complete reference for every class and function in Neurogebra.</p> <p>New here?</p> <p>Start with the tutorials first, then come back here when you need details.</p>"},{"location":"api/reference/#quick-import-guide","title":"Quick Import Guide","text":"<pre><code># Core imports \u2014 you'll use these the most\nfrom neurogebra import MathForge, Expression\n\n# Autograd (manual neural networks)\nfrom neurogebra.core.autograd import Value, Tensor\n\n# Training\nfrom neurogebra.core.trainer import Trainer\n\n# Model building\nfrom neurogebra.builders.model_builder import ModelBuilder\n\n# Educational interface\nfrom neurogebra.core.neurocraft import NeuroCraft\n\n# Datasets\nfrom neurogebra.datasets.loaders import generate_linear, generate_spirals\n</code></pre>"},{"location":"api/reference/#core-classes","title":"Core Classes","text":""},{"location":"api/reference/#expression","title":"Expression","text":"<p>The fundamental building block \u2014 a mathematical expression with symbolic and numerical capabilities.</p>"},{"location":"api/reference/#neurogebra.core.expression.Expression","title":"<code>neurogebra.core.expression.Expression</code>","text":"<p>Unified mathematical expression supporting symbolic, numerical, and trainable operations.</p> <p>Attributes:</p> Name Type Description <code>name</code> <p>Human-readable name of the expression</p> <code>symbolic_expr</code> <p>SymPy symbolic representation</p> <code>params</code> <p>Dictionary of parameters</p> <code>trainable_params</code> <p>List of parameter names that can be trained</p> <code>metadata</code> <p>Additional information about the expression</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>class Expression:\n    \"\"\"\n    Unified mathematical expression supporting symbolic, numerical,\n    and trainable operations.\n\n    Attributes:\n        name: Human-readable name of the expression\n        symbolic_expr: SymPy symbolic representation\n        params: Dictionary of parameters\n        trainable_params: List of parameter names that can be trained\n        metadata: Additional information about the expression\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        symbolic_expr: Union[str, sp.Expr],\n        params: Optional[Dict[str, Any]] = None,\n        trainable_params: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"\n        Initialize an Expression.\n\n        Args:\n            name: Name identifier for the expression\n            symbolic_expr: Symbolic mathematical expression (string or SymPy)\n            params: Dictionary of parameter values\n            trainable_params: List of parameters that can be trained\n            metadata: Additional information (description, usage, etc.)\n        \"\"\"\n        self.name = name\n        self.params = params or {}\n        self.trainable_params = trainable_params or []\n        self.metadata = metadata or {}\n\n        # Convert string to SymPy expression\n        if isinstance(symbolic_expr, str):\n            self.symbolic_expr = sympify(symbolic_expr)\n        else:\n            self.symbolic_expr = symbolic_expr\n\n        # Extract variables from symbolic expression\n        self.variables = sorted(\n            list(self.symbolic_expr.free_symbols), key=lambda s: s.name\n        )\n\n        # Create numerical function\n        self._numerical_func = None\n        self._compile_numerical()\n\n    def _compile_numerical(self):\n        \"\"\"Compile symbolic expression to numerical function.\"\"\"\n        if self.variables:\n            self._numerical_func = lambdify(\n                self.variables, self.symbolic_expr, modules=[\"numpy\"]\n            )\n        else:\n            # Constant expression\n            self._numerical_func = lambda: float(self.symbolic_expr)\n\n    def eval(self, *args: Any, **kwargs: Any) -&gt; Union[float, np.ndarray]:\n        \"\"\"\n        Evaluate the expression numerically.\n\n        Args:\n            *args: Positional arguments for variables\n            **kwargs: Keyword arguments for variables\n\n        Returns:\n            Numerical result (float or numpy array)\n\n        Examples:\n            &gt;&gt;&gt; expr = Expression(\"quadratic\", \"a*x**2 + b*x + c\")\n            &gt;&gt;&gt; result = expr.eval(x=2, a=1, b=2, c=1)\n        \"\"\"\n        # Substitute parameters into expression\n        expr_with_params = self.symbolic_expr.subs(self.params)\n\n        # Determine remaining free symbols after parameter substitution\n        remaining_vars = sorted(\n            list(expr_with_params.free_symbols), key=lambda s: s.name\n        )\n\n        if not remaining_vars:\n            return float(expr_with_params)\n\n        # Create function with current parameters\n        func = lambdify(remaining_vars, expr_with_params, modules=[\"numpy\"])\n\n        # Handle both positional and keyword arguments\n        if args:\n            return func(*args)\n        elif kwargs:\n            ordered_args = [kwargs.get(str(var), 0) for var in remaining_vars]\n            return func(*ordered_args)\n        else:\n            return func()\n\n    def gradient(self, var: Union[str, Symbol]) -&gt; \"Expression\":\n        \"\"\"\n        Compute symbolic gradient with respect to a variable.\n\n        Args:\n            var: Variable to differentiate with respect to\n\n        Returns:\n            New Expression representing the gradient\n        \"\"\"\n        if isinstance(var, str):\n            var = Symbol(var)\n\n        grad_expr = sp.diff(self.symbolic_expr, var)\n\n        return Expression(\n            name=f\"d({self.name})/d({var})\",\n            symbolic_expr=grad_expr,\n            params=self.params.copy(),\n            metadata={\"parent\": self.name, \"gradient_var\": str(var)},\n        )\n\n    def compose(self, other: \"Expression\") -&gt; \"Expression\":\n        \"\"\"\n        Compose two expressions: self(other(x)).\n\n        Args:\n            other: Expression to compose with\n\n        Returns:\n            New composed Expression\n        \"\"\"\n        # Simple composition: self(other)\n        if not self.variables:\n            return Expression(\n                name=f\"{self.name}\u2218{other.name}\",\n                symbolic_expr=self.symbolic_expr,\n                params={**self.params, **other.params},\n                metadata={\"composition\": [self.name, other.name]},\n            )\n\n        composed_expr = self.symbolic_expr.subs(\n            {self.variables[0]: other.symbolic_expr}\n        )\n\n        return Expression(\n            name=f\"{self.name}\u2218{other.name}\",\n            symbolic_expr=composed_expr,\n            params={**self.params, **other.params},\n            metadata={\"composition\": [self.name, other.name]},\n        )\n\n    def clone(self) -&gt; \"Expression\":\n        \"\"\"\n        Create a deep copy of this expression.\n\n        Returns:\n            New Expression with copied attributes\n        \"\"\"\n        return Expression(\n            name=self.name,\n            symbolic_expr=self.symbolic_expr,\n            params=self.params.copy(),\n            trainable_params=self.trainable_params.copy(),\n            metadata=self.metadata.copy(),\n        )\n\n    def visualize(\n        self,\n        x_range: Tuple[float, float] = (-5, 5),\n        n_points: int = 500,\n        interactive: bool = False,\n        **kwargs: Any,\n    ):\n        \"\"\"\n        Visualize this expression.\n\n        Args:\n            x_range: Range for x-axis\n            n_points: Number of points\n            interactive: Use interactive plotly plot\n            **kwargs: Additional plot parameters\n        \"\"\"\n        from neurogebra.viz.plotting import plot_expression\n\n        return plot_expression(self, x_range=x_range, n_points=n_points, **kwargs)\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"Allow expression to be called like a function.\"\"\"\n        return self.eval(*args, **kwargs)\n\n    def __add__(self, other):\n        \"\"\"Add two expressions.\"\"\"\n        if isinstance(other, Expression):\n            new_expr = self.symbolic_expr + other.symbolic_expr\n            new_name = f\"({self.name}+{other.name})\"\n            new_params = {**self.params, **other.params}\n        else:\n            new_expr = self.symbolic_expr + other\n            new_name = f\"({self.name}+{other})\"\n            new_params = self.params.copy()\n\n        return Expression(new_name, new_expr, new_params)\n\n    def __radd__(self, other):\n        \"\"\"Right addition.\"\"\"\n        if isinstance(other, (int, float)):\n            new_expr = other + self.symbolic_expr\n            new_name = f\"({other}+{self.name})\"\n            return Expression(new_name, new_expr, self.params.copy())\n        return NotImplemented\n\n    def __sub__(self, other):\n        \"\"\"Subtract two expressions.\"\"\"\n        if isinstance(other, Expression):\n            new_expr = self.symbolic_expr - other.symbolic_expr\n            new_name = f\"({self.name}-{other.name})\"\n            new_params = {**self.params, **other.params}\n        else:\n            new_expr = self.symbolic_expr - other\n            new_name = f\"({self.name}-{other})\"\n            new_params = self.params.copy()\n\n        return Expression(new_name, new_expr, new_params)\n\n    def __mul__(self, other):\n        \"\"\"Multiply two expressions.\"\"\"\n        if isinstance(other, Expression):\n            new_expr = self.symbolic_expr * other.symbolic_expr\n            new_name = f\"({self.name}*{other.name})\"\n            new_params = {**self.params, **other.params}\n        else:\n            new_expr = self.symbolic_expr * other\n            new_name = f\"({self.name}*{other})\"\n            new_params = self.params.copy()\n\n        return Expression(new_name, new_expr, new_params)\n\n    def __rmul__(self, other):\n        \"\"\"Right multiplication.\"\"\"\n        if isinstance(other, (int, float)):\n            new_expr = other * self.symbolic_expr\n            new_name = f\"({other}*{self.name})\"\n            return Expression(new_name, new_expr, self.params.copy())\n        return NotImplemented\n\n    def __truediv__(self, other):\n        \"\"\"Divide two expressions.\"\"\"\n        if isinstance(other, Expression):\n            new_expr = self.symbolic_expr / other.symbolic_expr\n            new_name = f\"({self.name}/{other.name})\"\n            new_params = {**self.params, **other.params}\n        else:\n            new_expr = self.symbolic_expr / other\n            new_name = f\"({self.name}/{other})\"\n            new_params = self.params.copy()\n\n        return Expression(new_name, new_expr, new_params)\n\n    def __pow__(self, other):\n        \"\"\"Power of expression.\"\"\"\n        if isinstance(other, Expression):\n            new_expr = self.symbolic_expr ** other.symbolic_expr\n            new_name = f\"({self.name}**{other.name})\"\n            new_params = {**self.params, **other.params}\n        else:\n            new_expr = self.symbolic_expr ** other\n            new_name = f\"({self.name}**{other})\"\n            new_params = self.params.copy()\n\n        return Expression(new_name, new_expr, new_params)\n\n    def __neg__(self):\n        \"\"\"Negate expression.\"\"\"\n        return Expression(\n            f\"(-{self.name})\", -self.symbolic_expr, self.params.copy()\n        )\n\n    def __repr__(self):\n        return f\"Expression('{self.name}': {self.symbolic_expr})\"\n\n    def __str__(self):\n        return str(self.symbolic_expr)\n\n    @property\n    def formula(self) -&gt; str:\n        \"\"\"Get LaTeX representation of the expression.\"\"\"\n        return sp.latex(self.symbolic_expr)\n\n    def simplify(self) -&gt; \"Expression\":\n        \"\"\"\n        Return a simplified version of the expression.\n\n        Returns:\n            New simplified Expression\n        \"\"\"\n        simplified = sp.simplify(self.symbolic_expr)\n        return Expression(\n            name=f\"simplified({self.name})\",\n            symbolic_expr=simplified,\n            params=self.params.copy(),\n            trainable_params=self.trainable_params.copy(),\n            metadata=self.metadata.copy(),\n        )\n\n    def expand(self) -&gt; \"Expression\":\n        \"\"\"\n        Return an expanded version of the expression.\n\n        Returns:\n            New expanded Expression\n        \"\"\"\n        expanded = sp.expand(self.symbolic_expr)\n        return Expression(\n            name=f\"expanded({self.name})\",\n            symbolic_expr=expanded,\n            params=self.params.copy(),\n            trainable_params=self.trainable_params.copy(),\n            metadata=self.metadata.copy(),\n        )\n\n    def integrate(self, var: Union[str, Symbol]) -&gt; \"Expression\":\n        \"\"\"\n        Compute symbolic integral with respect to a variable.\n\n        Args:\n            var: Variable to integrate with respect to\n\n        Returns:\n            New Expression representing the integral\n        \"\"\"\n        if isinstance(var, str):\n            var = Symbol(var)\n\n        integral_expr = sp.integrate(self.symbolic_expr, var)\n\n        return Expression(\n            name=f\"\u222b({self.name})d{var}\",\n            symbolic_expr=integral_expr,\n            params=self.params.copy(),\n            metadata={\"parent\": self.name, \"integral_var\": str(var)},\n        )\n\n    def explain(self, level: str = \"intermediate\") -&gt; str:\n        \"\"\"\n        Provide explanation of the expression.\n\n        Args:\n            level: Explanation level ('beginner', 'intermediate', 'advanced')\n\n        Returns:\n            Explanatory text\n        \"\"\"\n        explanation = f\"Expression: {self.name}\\n\"\n        explanation += f\"Formula: {self.formula}\\n\"\n        explanation += f\"Variables: {[str(v) for v in self.variables]}\\n\"\n\n        if self.params:\n            explanation += f\"Parameters: {self.params}\\n\"\n\n        if self.trainable_params:\n            explanation += f\"Trainable: {self.trainable_params}\\n\"\n\n        if \"description\" in self.metadata:\n            explanation += f\"\\nDescription: {self.metadata['description']}\\n\"\n\n        if \"usage\" in self.metadata:\n            explanation += f\"\\nUsage: {self.metadata['usage']}\\n\"\n\n        if level in (\"intermediate\", \"advanced\"):\n            if \"pros\" in self.metadata:\n                explanation += f\"\\nPros: {', '.join(self.metadata['pros'])}\\n\"\n            if \"cons\" in self.metadata:\n                explanation += f\"Cons: {', '.join(self.metadata['cons'])}\\n\"\n\n        if level == \"advanced\":\n            # Show gradient information\n            for var in self.variables:\n                grad = sp.diff(self.symbolic_expr, var)\n                explanation += f\"\\n\u2202/\u2202{var} = {grad}\\n\"\n\n        return explanation\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression-attributes","title":"Attributes","text":""},{"location":"api/reference/#neurogebra.core.expression.Expression.formula","title":"<code>formula</code>  <code>property</code>","text":"<p>Get LaTeX representation of the expression.</p>"},{"location":"api/reference/#neurogebra.core.expression.Expression-functions","title":"Functions","text":""},{"location":"api/reference/#neurogebra.core.expression.Expression.__init__","title":"<code>__init__(name, symbolic_expr, params=None, trainable_params=None, metadata=None)</code>","text":"<p>Initialize an Expression.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name identifier for the expression</p> required <code>symbolic_expr</code> <code>Union[str, Expr]</code> <p>Symbolic mathematical expression (string or SymPy)</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of parameter values</p> <code>None</code> <code>trainable_params</code> <code>Optional[List[str]]</code> <p>List of parameters that can be trained</p> <code>None</code> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional information (description, usage, etc.)</p> <code>None</code> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    symbolic_expr: Union[str, sp.Expr],\n    params: Optional[Dict[str, Any]] = None,\n    trainable_params: Optional[List[str]] = None,\n    metadata: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"\n    Initialize an Expression.\n\n    Args:\n        name: Name identifier for the expression\n        symbolic_expr: Symbolic mathematical expression (string or SymPy)\n        params: Dictionary of parameter values\n        trainable_params: List of parameters that can be trained\n        metadata: Additional information (description, usage, etc.)\n    \"\"\"\n    self.name = name\n    self.params = params or {}\n    self.trainable_params = trainable_params or []\n    self.metadata = metadata or {}\n\n    # Convert string to SymPy expression\n    if isinstance(symbolic_expr, str):\n        self.symbolic_expr = sympify(symbolic_expr)\n    else:\n        self.symbolic_expr = symbolic_expr\n\n    # Extract variables from symbolic expression\n    self.variables = sorted(\n        list(self.symbolic_expr.free_symbols), key=lambda s: s.name\n    )\n\n    # Create numerical function\n    self._numerical_func = None\n    self._compile_numerical()\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.eval","title":"<code>eval(*args, **kwargs)</code>","text":"<p>Evaluate the expression numerically.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments for variables</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for variables</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Numerical result (float or numpy array)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expr = Expression(\"quadratic\", \"a*x**2 + b*x + c\")\n&gt;&gt;&gt; result = expr.eval(x=2, a=1, b=2, c=1)\n</code></pre> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def eval(self, *args: Any, **kwargs: Any) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Evaluate the expression numerically.\n\n    Args:\n        *args: Positional arguments for variables\n        **kwargs: Keyword arguments for variables\n\n    Returns:\n        Numerical result (float or numpy array)\n\n    Examples:\n        &gt;&gt;&gt; expr = Expression(\"quadratic\", \"a*x**2 + b*x + c\")\n        &gt;&gt;&gt; result = expr.eval(x=2, a=1, b=2, c=1)\n    \"\"\"\n    # Substitute parameters into expression\n    expr_with_params = self.symbolic_expr.subs(self.params)\n\n    # Determine remaining free symbols after parameter substitution\n    remaining_vars = sorted(\n        list(expr_with_params.free_symbols), key=lambda s: s.name\n    )\n\n    if not remaining_vars:\n        return float(expr_with_params)\n\n    # Create function with current parameters\n    func = lambdify(remaining_vars, expr_with_params, modules=[\"numpy\"])\n\n    # Handle both positional and keyword arguments\n    if args:\n        return func(*args)\n    elif kwargs:\n        ordered_args = [kwargs.get(str(var), 0) for var in remaining_vars]\n        return func(*ordered_args)\n    else:\n        return func()\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.gradient","title":"<code>gradient(var)</code>","text":"<p>Compute symbolic gradient with respect to a variable.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>Union[str, Symbol]</code> <p>Variable to differentiate with respect to</p> required <p>Returns:</p> Type Description <code>Expression</code> <p>New Expression representing the gradient</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def gradient(self, var: Union[str, Symbol]) -&gt; \"Expression\":\n    \"\"\"\n    Compute symbolic gradient with respect to a variable.\n\n    Args:\n        var: Variable to differentiate with respect to\n\n    Returns:\n        New Expression representing the gradient\n    \"\"\"\n    if isinstance(var, str):\n        var = Symbol(var)\n\n    grad_expr = sp.diff(self.symbolic_expr, var)\n\n    return Expression(\n        name=f\"d({self.name})/d({var})\",\n        symbolic_expr=grad_expr,\n        params=self.params.copy(),\n        metadata={\"parent\": self.name, \"gradient_var\": str(var)},\n    )\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.compose","title":"<code>compose(other)</code>","text":"<p>Compose two expressions: self(other(x)).</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Expression</code> <p>Expression to compose with</p> required <p>Returns:</p> Type Description <code>Expression</code> <p>New composed Expression</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def compose(self, other: \"Expression\") -&gt; \"Expression\":\n    \"\"\"\n    Compose two expressions: self(other(x)).\n\n    Args:\n        other: Expression to compose with\n\n    Returns:\n        New composed Expression\n    \"\"\"\n    # Simple composition: self(other)\n    if not self.variables:\n        return Expression(\n            name=f\"{self.name}\u2218{other.name}\",\n            symbolic_expr=self.symbolic_expr,\n            params={**self.params, **other.params},\n            metadata={\"composition\": [self.name, other.name]},\n        )\n\n    composed_expr = self.symbolic_expr.subs(\n        {self.variables[0]: other.symbolic_expr}\n    )\n\n    return Expression(\n        name=f\"{self.name}\u2218{other.name}\",\n        symbolic_expr=composed_expr,\n        params={**self.params, **other.params},\n        metadata={\"composition\": [self.name, other.name]},\n    )\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.clone","title":"<code>clone()</code>","text":"<p>Create a deep copy of this expression.</p> <p>Returns:</p> Type Description <code>Expression</code> <p>New Expression with copied attributes</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def clone(self) -&gt; \"Expression\":\n    \"\"\"\n    Create a deep copy of this expression.\n\n    Returns:\n        New Expression with copied attributes\n    \"\"\"\n    return Expression(\n        name=self.name,\n        symbolic_expr=self.symbolic_expr,\n        params=self.params.copy(),\n        trainable_params=self.trainable_params.copy(),\n        metadata=self.metadata.copy(),\n    )\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.visualize","title":"<code>visualize(x_range=(-5, 5), n_points=500, interactive=False, **kwargs)</code>","text":"<p>Visualize this expression.</p> <p>Parameters:</p> Name Type Description Default <code>x_range</code> <code>Tuple[float, float]</code> <p>Range for x-axis</p> <code>(-5, 5)</code> <code>n_points</code> <code>int</code> <p>Number of points</p> <code>500</code> <code>interactive</code> <code>bool</code> <p>Use interactive plotly plot</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional plot parameters</p> <code>{}</code> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def visualize(\n    self,\n    x_range: Tuple[float, float] = (-5, 5),\n    n_points: int = 500,\n    interactive: bool = False,\n    **kwargs: Any,\n):\n    \"\"\"\n    Visualize this expression.\n\n    Args:\n        x_range: Range for x-axis\n        n_points: Number of points\n        interactive: Use interactive plotly plot\n        **kwargs: Additional plot parameters\n    \"\"\"\n    from neurogebra.viz.plotting import plot_expression\n\n    return plot_expression(self, x_range=x_range, n_points=n_points, **kwargs)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Allow expression to be called like a function.</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def __call__(self, *args, **kwargs):\n    \"\"\"Allow expression to be called like a function.\"\"\"\n    return self.eval(*args, **kwargs)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.__add__","title":"<code>__add__(other)</code>","text":"<p>Add two expressions.</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def __add__(self, other):\n    \"\"\"Add two expressions.\"\"\"\n    if isinstance(other, Expression):\n        new_expr = self.symbolic_expr + other.symbolic_expr\n        new_name = f\"({self.name}+{other.name})\"\n        new_params = {**self.params, **other.params}\n    else:\n        new_expr = self.symbolic_expr + other\n        new_name = f\"({self.name}+{other})\"\n        new_params = self.params.copy()\n\n    return Expression(new_name, new_expr, new_params)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.__radd__","title":"<code>__radd__(other)</code>","text":"<p>Right addition.</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def __radd__(self, other):\n    \"\"\"Right addition.\"\"\"\n    if isinstance(other, (int, float)):\n        new_expr = other + self.symbolic_expr\n        new_name = f\"({other}+{self.name})\"\n        return Expression(new_name, new_expr, self.params.copy())\n    return NotImplemented\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.__sub__","title":"<code>__sub__(other)</code>","text":"<p>Subtract two expressions.</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def __sub__(self, other):\n    \"\"\"Subtract two expressions.\"\"\"\n    if isinstance(other, Expression):\n        new_expr = self.symbolic_expr - other.symbolic_expr\n        new_name = f\"({self.name}-{other.name})\"\n        new_params = {**self.params, **other.params}\n    else:\n        new_expr = self.symbolic_expr - other\n        new_name = f\"({self.name}-{other})\"\n        new_params = self.params.copy()\n\n    return Expression(new_name, new_expr, new_params)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.__mul__","title":"<code>__mul__(other)</code>","text":"<p>Multiply two expressions.</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def __mul__(self, other):\n    \"\"\"Multiply two expressions.\"\"\"\n    if isinstance(other, Expression):\n        new_expr = self.symbolic_expr * other.symbolic_expr\n        new_name = f\"({self.name}*{other.name})\"\n        new_params = {**self.params, **other.params}\n    else:\n        new_expr = self.symbolic_expr * other\n        new_name = f\"({self.name}*{other})\"\n        new_params = self.params.copy()\n\n    return Expression(new_name, new_expr, new_params)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.__rmul__","title":"<code>__rmul__(other)</code>","text":"<p>Right multiplication.</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def __rmul__(self, other):\n    \"\"\"Right multiplication.\"\"\"\n    if isinstance(other, (int, float)):\n        new_expr = other * self.symbolic_expr\n        new_name = f\"({other}*{self.name})\"\n        return Expression(new_name, new_expr, self.params.copy())\n    return NotImplemented\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.__truediv__","title":"<code>__truediv__(other)</code>","text":"<p>Divide two expressions.</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def __truediv__(self, other):\n    \"\"\"Divide two expressions.\"\"\"\n    if isinstance(other, Expression):\n        new_expr = self.symbolic_expr / other.symbolic_expr\n        new_name = f\"({self.name}/{other.name})\"\n        new_params = {**self.params, **other.params}\n    else:\n        new_expr = self.symbolic_expr / other\n        new_name = f\"({self.name}/{other})\"\n        new_params = self.params.copy()\n\n    return Expression(new_name, new_expr, new_params)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.__pow__","title":"<code>__pow__(other)</code>","text":"<p>Power of expression.</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def __pow__(self, other):\n    \"\"\"Power of expression.\"\"\"\n    if isinstance(other, Expression):\n        new_expr = self.symbolic_expr ** other.symbolic_expr\n        new_name = f\"({self.name}**{other.name})\"\n        new_params = {**self.params, **other.params}\n    else:\n        new_expr = self.symbolic_expr ** other\n        new_name = f\"({self.name}**{other})\"\n        new_params = self.params.copy()\n\n    return Expression(new_name, new_expr, new_params)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.__neg__","title":"<code>__neg__()</code>","text":"<p>Negate expression.</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def __neg__(self):\n    \"\"\"Negate expression.\"\"\"\n    return Expression(\n        f\"(-{self.name})\", -self.symbolic_expr, self.params.copy()\n    )\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.simplify","title":"<code>simplify()</code>","text":"<p>Return a simplified version of the expression.</p> <p>Returns:</p> Type Description <code>Expression</code> <p>New simplified Expression</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def simplify(self) -&gt; \"Expression\":\n    \"\"\"\n    Return a simplified version of the expression.\n\n    Returns:\n        New simplified Expression\n    \"\"\"\n    simplified = sp.simplify(self.symbolic_expr)\n    return Expression(\n        name=f\"simplified({self.name})\",\n        symbolic_expr=simplified,\n        params=self.params.copy(),\n        trainable_params=self.trainable_params.copy(),\n        metadata=self.metadata.copy(),\n    )\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.expand","title":"<code>expand()</code>","text":"<p>Return an expanded version of the expression.</p> <p>Returns:</p> Type Description <code>Expression</code> <p>New expanded Expression</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def expand(self) -&gt; \"Expression\":\n    \"\"\"\n    Return an expanded version of the expression.\n\n    Returns:\n        New expanded Expression\n    \"\"\"\n    expanded = sp.expand(self.symbolic_expr)\n    return Expression(\n        name=f\"expanded({self.name})\",\n        symbolic_expr=expanded,\n        params=self.params.copy(),\n        trainable_params=self.trainable_params.copy(),\n        metadata=self.metadata.copy(),\n    )\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.integrate","title":"<code>integrate(var)</code>","text":"<p>Compute symbolic integral with respect to a variable.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>Union[str, Symbol]</code> <p>Variable to integrate with respect to</p> required <p>Returns:</p> Type Description <code>Expression</code> <p>New Expression representing the integral</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def integrate(self, var: Union[str, Symbol]) -&gt; \"Expression\":\n    \"\"\"\n    Compute symbolic integral with respect to a variable.\n\n    Args:\n        var: Variable to integrate with respect to\n\n    Returns:\n        New Expression representing the integral\n    \"\"\"\n    if isinstance(var, str):\n        var = Symbol(var)\n\n    integral_expr = sp.integrate(self.symbolic_expr, var)\n\n    return Expression(\n        name=f\"\u222b({self.name})d{var}\",\n        symbolic_expr=integral_expr,\n        params=self.params.copy(),\n        metadata={\"parent\": self.name, \"integral_var\": str(var)},\n    )\n</code></pre>"},{"location":"api/reference/#neurogebra.core.expression.Expression.explain","title":"<code>explain(level='intermediate')</code>","text":"<p>Provide explanation of the expression.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Explanation level ('beginner', 'intermediate', 'advanced')</p> <code>'intermediate'</code> <p>Returns:</p> Type Description <code>str</code> <p>Explanatory text</p> Source code in <code>src/neurogebra/core/expression.py</code> <pre><code>def explain(self, level: str = \"intermediate\") -&gt; str:\n    \"\"\"\n    Provide explanation of the expression.\n\n    Args:\n        level: Explanation level ('beginner', 'intermediate', 'advanced')\n\n    Returns:\n        Explanatory text\n    \"\"\"\n    explanation = f\"Expression: {self.name}\\n\"\n    explanation += f\"Formula: {self.formula}\\n\"\n    explanation += f\"Variables: {[str(v) for v in self.variables]}\\n\"\n\n    if self.params:\n        explanation += f\"Parameters: {self.params}\\n\"\n\n    if self.trainable_params:\n        explanation += f\"Trainable: {self.trainable_params}\\n\"\n\n    if \"description\" in self.metadata:\n        explanation += f\"\\nDescription: {self.metadata['description']}\\n\"\n\n    if \"usage\" in self.metadata:\n        explanation += f\"\\nUsage: {self.metadata['usage']}\\n\"\n\n    if level in (\"intermediate\", \"advanced\"):\n        if \"pros\" in self.metadata:\n            explanation += f\"\\nPros: {', '.join(self.metadata['pros'])}\\n\"\n        if \"cons\" in self.metadata:\n            explanation += f\"Cons: {', '.join(self.metadata['cons'])}\\n\"\n\n    if level == \"advanced\":\n        # Show gradient information\n        for var in self.variables:\n            grad = sp.diff(self.symbolic_expr, var)\n            explanation += f\"\\n\u2202/\u2202{var} = {grad}\\n\"\n\n    return explanation\n</code></pre>"},{"location":"api/reference/#mathforge","title":"MathForge","text":"<p>Your gateway to 50+ pre-built mathematical expressions organized by category.</p>"},{"location":"api/reference/#neurogebra.core.forge.MathForge","title":"<code>neurogebra.core.forge.MathForge</code>","text":"<p>Central hub for accessing mathematical expressions.</p> <p>MathForge provides a unified interface to: - Get pre-built expressions - Create custom expressions - Search for expressions - Compose expressions - Train expressions</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; forge = MathForge()\n&gt;&gt;&gt; relu = forge.get(\"relu\")\n&gt;&gt;&gt; result = relu.eval(x=-5)\n&gt;&gt;&gt; print(result)  # 0\n</code></pre> Source code in <code>src/neurogebra/core/forge.py</code> <pre><code>class MathForge:\n    \"\"\"\n    Central hub for accessing mathematical expressions.\n\n    MathForge provides a unified interface to:\n    - Get pre-built expressions\n    - Create custom expressions\n    - Search for expressions\n    - Compose expressions\n    - Train expressions\n\n    Examples:\n        &gt;&gt;&gt; forge = MathForge()\n        &gt;&gt;&gt; relu = forge.get(\"relu\")\n        &gt;&gt;&gt; result = relu.eval(x=-5)\n        &gt;&gt;&gt; print(result)  # 0\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize MathForge with expression repository.\"\"\"\n        self._repository: Dict[str, Expression] = {}\n        self._load_repository()\n\n    def _load_repository(self):\n        \"\"\"Load all expressions from repository modules.\"\"\"\n        # Load activations\n        self._repository.update(activations.get_activations())\n\n        # Load losses\n        self._repository.update(losses.get_losses())\n\n        # Load regularizers\n        self._repository.update(regularizers.get_regularizers())\n\n        # Load algebra\n        self._repository.update(algebra.get_algebra_expressions())\n\n        # Load calculus\n        self._repository.update(calculus.get_calculus_expressions())\n\n        # Load statistics\n        self._repository.update(statistics.get_statistics_expressions())\n\n        # Load linear algebra\n        self._repository.update(linalg.get_linalg_expressions())\n\n        # Load optimization\n        self._repository.update(optimization.get_optimization_expressions())\n\n        # Load metrics\n        self._repository.update(metrics.get_metrics_expressions())\n\n        # Load transforms\n        self._repository.update(transforms.get_transforms_expressions())\n\n    def get(\n        self,\n        name: str,\n        params: Optional[Dict[str, Any]] = None,\n        trainable: bool = False,\n    ) -&gt; Expression:\n        \"\"\"\n        Get an expression by name.\n\n        Args:\n            name: Name of the expression\n            params: Parameter overrides\n            trainable: Whether to make parameters trainable\n\n        Returns:\n            Expression instance\n\n        Raises:\n            KeyError: If expression not found\n\n        Examples:\n            &gt;&gt;&gt; forge = MathForge()\n            &gt;&gt;&gt; sigmoid = forge.get(\"sigmoid\")\n            &gt;&gt;&gt; custom = forge.get(\"leaky_relu\", params={\"alpha\": 0.2})\n        \"\"\"\n        if name not in self._repository:\n            available = \", \".join(list(self._repository.keys())[:10])\n            raise KeyError(\n                f\"Expression '{name}' not found. \"\n                f\"Available: {available}...\"\n            )\n\n        expr_template = self._repository[name]\n\n        # Clone and customize\n        expr = Expression(\n            name=expr_template.name,\n            symbolic_expr=expr_template.symbolic_expr,\n            params=expr_template.params.copy(),\n            trainable_params=(\n                list(expr_template.params.keys())\n                if trainable\n                else expr_template.trainable_params.copy()\n            ),\n            metadata=expr_template.metadata.copy(),\n        )\n\n        # Override parameters\n        if params:\n            expr.params.update(params)\n\n        return expr\n\n    def register(self, name: str, expression: Expression) -&gt; None:\n        \"\"\"\n        Register a custom expression in the repository.\n\n        Args:\n            name: Name for the expression\n            expression: Expression instance to register\n        \"\"\"\n        self._repository[name] = expression\n\n    def search(self, query: str) -&gt; List[str]:\n        \"\"\"\n        Search for expressions by name or description.\n\n        Args:\n            query: Search string\n\n        Returns:\n            List of matching expression names\n        \"\"\"\n        query_lower = query.lower()\n        results = []\n\n        for name, expr in self._repository.items():\n            # Search in name\n            if query_lower in name.lower():\n                results.append(name)\n                continue\n\n            # Search in metadata\n            if \"description\" in expr.metadata:\n                if query_lower in expr.metadata[\"description\"].lower():\n                    results.append(name)\n                    continue\n\n            # Search in category\n            if \"category\" in expr.metadata:\n                if query_lower in expr.metadata[\"category\"].lower():\n                    results.append(name)\n\n        return results\n\n    def list_all(self, category: Optional[str] = None) -&gt; List[str]:\n        \"\"\"\n        List all available expressions.\n\n        Args:\n            category: Filter by category (e.g., 'activation', 'loss')\n\n        Returns:\n            List of expression names\n        \"\"\"\n        if category is None:\n            return list(self._repository.keys())\n\n        return [\n            name\n            for name, expr in self._repository.items()\n            if expr.metadata.get(\"category\") == category\n        ]\n\n    def compose(self, expression_str: str, **params: Any) -&gt; Expression:\n        \"\"\"\n        Compose expressions using string notation.\n\n        Args:\n            expression_str: Expression like \"mse + 0.1*l2\"\n            **params: Parameters for composed expression\n\n        Returns:\n            Composed Expression\n\n        Examples:\n            &gt;&gt;&gt; forge = MathForge()\n            &gt;&gt;&gt; loss = forge.compose(\"mse + 0.1*mae\")\n        \"\"\"\n        parts = expression_str.split(\"+\")\n\n        result = None\n        for part in parts:\n            part = part.strip()\n\n            # Handle scalar multiplication\n            if \"*\" in part:\n                scalar_str, expr_name = part.split(\"*\", 1)\n                scalar = float(scalar_str.strip())\n                expr = self.get(expr_name.strip())\n                expr = expr * scalar\n            else:\n                expr = self.get(part)\n\n            if result is None:\n                result = expr\n            else:\n                result = result + expr\n\n        if result is None:\n            raise ValueError(f\"Could not parse expression: {expression_str}\")\n\n        return result\n\n    def explain(self, name: str, level: str = \"intermediate\") -&gt; str:\n        \"\"\"\n        Get explanation for an expression.\n\n        Args:\n            name: Name of the expression\n            level: Detail level ('beginner', 'intermediate', 'advanced')\n\n        Returns:\n            Explanation string\n        \"\"\"\n        expr = self.get(name)\n        return expr.explain(level=level)\n\n    def compare(self, names: List[str]) -&gt; str:\n        \"\"\"\n        Compare multiple expressions side by side.\n\n        Args:\n            names: List of expression names to compare\n\n        Returns:\n            Comparison table as string\n        \"\"\"\n        lines = [f\"{'Name':&lt;20} {'Formula':&lt;40} {'Category':&lt;15}\"]\n        lines.append(\"-\" * 75)\n\n        for name in names:\n            if name in self._repository:\n                expr = self._repository[name]\n                formula = str(expr.symbolic_expr)[:38]\n                category = expr.metadata.get(\"category\", \"N/A\")\n                lines.append(f\"{name:&lt;20} {formula:&lt;40} {category:&lt;15}\")\n\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.forge.MathForge-functions","title":"Functions","text":""},{"location":"api/reference/#neurogebra.core.forge.MathForge.__init__","title":"<code>__init__()</code>","text":"<p>Initialize MathForge with expression repository.</p> Source code in <code>src/neurogebra/core/forge.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize MathForge with expression repository.\"\"\"\n    self._repository: Dict[str, Expression] = {}\n    self._load_repository()\n</code></pre>"},{"location":"api/reference/#neurogebra.core.forge.MathForge.get","title":"<code>get(name, params=None, trainable=False)</code>","text":"<p>Get an expression by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the expression</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Parameter overrides</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether to make parameters trainable</p> <code>False</code> <p>Returns:</p> Type Description <code>Expression</code> <p>Expression instance</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If expression not found</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; forge = MathForge()\n&gt;&gt;&gt; sigmoid = forge.get(\"sigmoid\")\n&gt;&gt;&gt; custom = forge.get(\"leaky_relu\", params={\"alpha\": 0.2})\n</code></pre> Source code in <code>src/neurogebra/core/forge.py</code> <pre><code>def get(\n    self,\n    name: str,\n    params: Optional[Dict[str, Any]] = None,\n    trainable: bool = False,\n) -&gt; Expression:\n    \"\"\"\n    Get an expression by name.\n\n    Args:\n        name: Name of the expression\n        params: Parameter overrides\n        trainable: Whether to make parameters trainable\n\n    Returns:\n        Expression instance\n\n    Raises:\n        KeyError: If expression not found\n\n    Examples:\n        &gt;&gt;&gt; forge = MathForge()\n        &gt;&gt;&gt; sigmoid = forge.get(\"sigmoid\")\n        &gt;&gt;&gt; custom = forge.get(\"leaky_relu\", params={\"alpha\": 0.2})\n    \"\"\"\n    if name not in self._repository:\n        available = \", \".join(list(self._repository.keys())[:10])\n        raise KeyError(\n            f\"Expression '{name}' not found. \"\n            f\"Available: {available}...\"\n        )\n\n    expr_template = self._repository[name]\n\n    # Clone and customize\n    expr = Expression(\n        name=expr_template.name,\n        symbolic_expr=expr_template.symbolic_expr,\n        params=expr_template.params.copy(),\n        trainable_params=(\n            list(expr_template.params.keys())\n            if trainable\n            else expr_template.trainable_params.copy()\n        ),\n        metadata=expr_template.metadata.copy(),\n    )\n\n    # Override parameters\n    if params:\n        expr.params.update(params)\n\n    return expr\n</code></pre>"},{"location":"api/reference/#neurogebra.core.forge.MathForge.register","title":"<code>register(name, expression)</code>","text":"<p>Register a custom expression in the repository.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name for the expression</p> required <code>expression</code> <code>Expression</code> <p>Expression instance to register</p> required Source code in <code>src/neurogebra/core/forge.py</code> <pre><code>def register(self, name: str, expression: Expression) -&gt; None:\n    \"\"\"\n    Register a custom expression in the repository.\n\n    Args:\n        name: Name for the expression\n        expression: Expression instance to register\n    \"\"\"\n    self._repository[name] = expression\n</code></pre>"},{"location":"api/reference/#neurogebra.core.forge.MathForge.search","title":"<code>search(query)</code>","text":"<p>Search for expressions by name or description.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search string</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of matching expression names</p> Source code in <code>src/neurogebra/core/forge.py</code> <pre><code>def search(self, query: str) -&gt; List[str]:\n    \"\"\"\n    Search for expressions by name or description.\n\n    Args:\n        query: Search string\n\n    Returns:\n        List of matching expression names\n    \"\"\"\n    query_lower = query.lower()\n    results = []\n\n    for name, expr in self._repository.items():\n        # Search in name\n        if query_lower in name.lower():\n            results.append(name)\n            continue\n\n        # Search in metadata\n        if \"description\" in expr.metadata:\n            if query_lower in expr.metadata[\"description\"].lower():\n                results.append(name)\n                continue\n\n        # Search in category\n        if \"category\" in expr.metadata:\n            if query_lower in expr.metadata[\"category\"].lower():\n                results.append(name)\n\n    return results\n</code></pre>"},{"location":"api/reference/#neurogebra.core.forge.MathForge.list_all","title":"<code>list_all(category=None)</code>","text":"<p>List all available expressions.</p> <p>Parameters:</p> Name Type Description Default <code>category</code> <code>Optional[str]</code> <p>Filter by category (e.g., 'activation', 'loss')</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of expression names</p> Source code in <code>src/neurogebra/core/forge.py</code> <pre><code>def list_all(self, category: Optional[str] = None) -&gt; List[str]:\n    \"\"\"\n    List all available expressions.\n\n    Args:\n        category: Filter by category (e.g., 'activation', 'loss')\n\n    Returns:\n        List of expression names\n    \"\"\"\n    if category is None:\n        return list(self._repository.keys())\n\n    return [\n        name\n        for name, expr in self._repository.items()\n        if expr.metadata.get(\"category\") == category\n    ]\n</code></pre>"},{"location":"api/reference/#neurogebra.core.forge.MathForge.compose","title":"<code>compose(expression_str, **params)</code>","text":"<p>Compose expressions using string notation.</p> <p>Parameters:</p> Name Type Description Default <code>expression_str</code> <code>str</code> <p>Expression like \"mse + 0.1*l2\"</p> required <code>**params</code> <code>Any</code> <p>Parameters for composed expression</p> <code>{}</code> <p>Returns:</p> Type Description <code>Expression</code> <p>Composed Expression</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; forge = MathForge()\n&gt;&gt;&gt; loss = forge.compose(\"mse + 0.1*mae\")\n</code></pre> Source code in <code>src/neurogebra/core/forge.py</code> <pre><code>def compose(self, expression_str: str, **params: Any) -&gt; Expression:\n    \"\"\"\n    Compose expressions using string notation.\n\n    Args:\n        expression_str: Expression like \"mse + 0.1*l2\"\n        **params: Parameters for composed expression\n\n    Returns:\n        Composed Expression\n\n    Examples:\n        &gt;&gt;&gt; forge = MathForge()\n        &gt;&gt;&gt; loss = forge.compose(\"mse + 0.1*mae\")\n    \"\"\"\n    parts = expression_str.split(\"+\")\n\n    result = None\n    for part in parts:\n        part = part.strip()\n\n        # Handle scalar multiplication\n        if \"*\" in part:\n            scalar_str, expr_name = part.split(\"*\", 1)\n            scalar = float(scalar_str.strip())\n            expr = self.get(expr_name.strip())\n            expr = expr * scalar\n        else:\n            expr = self.get(part)\n\n        if result is None:\n            result = expr\n        else:\n            result = result + expr\n\n    if result is None:\n        raise ValueError(f\"Could not parse expression: {expression_str}\")\n\n    return result\n</code></pre>"},{"location":"api/reference/#neurogebra.core.forge.MathForge.explain","title":"<code>explain(name, level='intermediate')</code>","text":"<p>Get explanation for an expression.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the expression</p> required <code>level</code> <code>str</code> <p>Detail level ('beginner', 'intermediate', 'advanced')</p> <code>'intermediate'</code> <p>Returns:</p> Type Description <code>str</code> <p>Explanation string</p> Source code in <code>src/neurogebra/core/forge.py</code> <pre><code>def explain(self, name: str, level: str = \"intermediate\") -&gt; str:\n    \"\"\"\n    Get explanation for an expression.\n\n    Args:\n        name: Name of the expression\n        level: Detail level ('beginner', 'intermediate', 'advanced')\n\n    Returns:\n        Explanation string\n    \"\"\"\n    expr = self.get(name)\n    return expr.explain(level=level)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.forge.MathForge.compare","title":"<code>compare(names)</code>","text":"<p>Compare multiple expressions side by side.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>List of expression names to compare</p> required <p>Returns:</p> Type Description <code>str</code> <p>Comparison table as string</p> Source code in <code>src/neurogebra/core/forge.py</code> <pre><code>def compare(self, names: List[str]) -&gt; str:\n    \"\"\"\n    Compare multiple expressions side by side.\n\n    Args:\n        names: List of expression names to compare\n\n    Returns:\n        Comparison table as string\n    \"\"\"\n    lines = [f\"{'Name':&lt;20} {'Formula':&lt;40} {'Category':&lt;15}\"]\n    lines.append(\"-\" * 75)\n\n    for name in names:\n        if name in self._repository:\n            expr = self._repository[name]\n            formula = str(expr.symbolic_expr)[:38]\n            category = expr.metadata.get(\"category\", \"N/A\")\n            lines.append(f\"{name:&lt;20} {formula:&lt;40} {category:&lt;15}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/reference/#trainer","title":"Trainer","text":"<p>Fits trainable expressions to data using SGD or Adam optimization.</p>"},{"location":"api/reference/#neurogebra.core.trainer.Trainer","title":"<code>neurogebra.core.trainer.Trainer</code>","text":"<p>Trainer for optimizing expression parameters.</p> <p>Supports various optimization algorithms for fitting expressions to data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from neurogebra.core.expression import Expression\n&gt;&gt;&gt; expr = Expression(\"linear\", \"a*x + b\",\n...                   params={\"a\": 0.0, \"b\": 0.0},\n...                   trainable_params=[\"a\", \"b\"])\n&gt;&gt;&gt; trainer = Trainer(expr, learning_rate=0.01)\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5])\n&gt;&gt;&gt; y = np.array([3, 5, 7, 9, 11])  # y = 2x + 1\n&gt;&gt;&gt; history = trainer.fit(X, y, epochs=100)\n</code></pre> Source code in <code>src/neurogebra/core/trainer.py</code> <pre><code>class Trainer:\n    \"\"\"\n    Trainer for optimizing expression parameters.\n\n    Supports various optimization algorithms for fitting expressions to data.\n\n    Examples:\n        &gt;&gt;&gt; from neurogebra.core.expression import Expression\n        &gt;&gt;&gt; expr = Expression(\"linear\", \"a*x + b\",\n        ...                   params={\"a\": 0.0, \"b\": 0.0},\n        ...                   trainable_params=[\"a\", \"b\"])\n        &gt;&gt;&gt; trainer = Trainer(expr, learning_rate=0.01)\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; y = np.array([3, 5, 7, 9, 11])  # y = 2x + 1\n        &gt;&gt;&gt; history = trainer.fit(X, y, epochs=100)\n    \"\"\"\n\n    def __init__(\n        self,\n        expression: Expression,\n        learning_rate: float = 0.01,\n        optimizer: str = \"sgd\",\n    ):\n        \"\"\"\n        Initialize Trainer.\n\n        Args:\n            expression: Expression to train\n            learning_rate: Learning rate for optimization\n            optimizer: Optimization algorithm ('sgd', 'adam')\n        \"\"\"\n        self.expression = expression\n        self.learning_rate = learning_rate\n        self.optimizer = optimizer\n        self.history: Dict[str, List] = {\"loss\": [], \"params\": []}\n\n        # Adam optimizer state\n        self._m: Dict[str, float] = {}\n        self._v: Dict[str, float] = {}\n        self._t = 0\n\n        for param_name in self.expression.trainable_params:\n            self._m[param_name] = 0.0\n            self._v[param_name] = 0.0\n\n    def fit(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        epochs: int = 100,\n        batch_size: Optional[int] = None,\n        loss_fn: str = \"mse\",\n        verbose: bool = True,\n        callback: Optional[Callable] = None,\n    ) -&gt; Dict[str, List]:\n        \"\"\"\n        Fit expression to data.\n\n        Args:\n            X: Input data (N,) or (N, D)\n            y: Target data (N,)\n            epochs: Number of training epochs\n            batch_size: Mini-batch size (None = full batch)\n            loss_fn: Loss function ('mse', 'mae')\n            verbose: Print training progress\n            callback: Optional callback function called each epoch\n\n        Returns:\n            Training history with loss and parameter values per epoch\n        \"\"\"\n        X = np.asarray(X, dtype=np.float64)\n        y = np.asarray(y, dtype=np.float64)\n\n        if batch_size is None:\n            batch_size = len(X)\n\n        for epoch in range(epochs):\n            # Shuffle data\n            indices = np.random.permutation(len(X))\n            X_shuffled = X[indices]\n            y_shuffled = y[indices]\n\n            epoch_loss = 0.0\n            n_batches = 0\n\n            # Mini-batch training\n            for i in range(0, len(X), batch_size):\n                X_batch = X_shuffled[i : i + batch_size]\n                y_batch = y_shuffled[i : i + batch_size]\n\n                # Forward pass\n                predictions = self._forward(X_batch)\n\n                # Compute loss\n                loss = self._compute_loss(predictions, y_batch, loss_fn)\n                epoch_loss += loss\n                n_batches += 1\n\n                # Backward pass (numerical gradients for MVP)\n                self._backward(X_batch, y_batch, loss_fn)\n\n            # Record history\n            avg_loss = epoch_loss / n_batches\n            self.history[\"loss\"].append(avg_loss)\n            self.history[\"params\"].append(self.expression.params.copy())\n\n            if verbose and (epoch % max(1, epochs // 10) == 0 or epoch == epochs - 1):\n                print(f\"Epoch {epoch:&gt;4d}/{epochs}: Loss = {avg_loss:.6f}\")\n\n            if callback is not None:\n                callback(epoch, avg_loss, self.expression.params.copy())\n\n        return self.history\n\n    def _forward(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Forward pass through expression.\"\"\"\n        results = []\n        for x in X:\n            result = self.expression.eval(x=float(x))\n            results.append(float(result))\n        return np.array(results)\n\n    def _compute_loss(\n        self, predictions: np.ndarray, targets: np.ndarray, loss_fn: str\n    ) -&gt; float:\n        \"\"\"Compute loss value.\"\"\"\n        if loss_fn == \"mse\":\n            return float(np.mean((predictions - targets) ** 2))\n        elif loss_fn == \"mae\":\n            return float(np.mean(np.abs(predictions - targets)))\n        elif loss_fn == \"huber\":\n            delta = 1.0\n            diff = np.abs(predictions - targets)\n            return float(\n                np.mean(\n                    np.where(\n                        diff &lt;= delta,\n                        0.5 * diff**2,\n                        delta * diff - 0.5 * delta**2,\n                    )\n                )\n            )\n        else:\n            raise ValueError(f\"Unknown loss function: {loss_fn}\")\n\n    def _backward(self, X: np.ndarray, y: np.ndarray, loss_fn: str):\n        \"\"\"\n        Backward pass using numerical gradients.\n\n        For MVP, use finite differences. Later versions can use\n        symbolic differentiation or autograd.\n        \"\"\"\n        epsilon = 1e-5\n\n        for param_name in self.expression.trainable_params:\n            if param_name not in self.expression.params:\n                continue\n\n            original_value = self.expression.params[param_name]\n\n            # Compute gradient via central finite difference\n            self.expression.params[param_name] = original_value + epsilon\n            pred_plus = self._forward(X)\n            loss_plus = self._compute_loss(pred_plus, y, loss_fn)\n\n            self.expression.params[param_name] = original_value - epsilon\n            pred_minus = self._forward(X)\n            loss_minus = self._compute_loss(pred_minus, y, loss_fn)\n\n            # Restore original\n            self.expression.params[param_name] = original_value\n\n            # Gradient\n            gradient = (loss_plus - loss_minus) / (2 * epsilon)\n\n            # Update parameter based on optimizer\n            if self.optimizer == \"adam\":\n                self._adam_update(param_name, gradient)\n            else:\n                # SGD\n                self.expression.params[param_name] = (\n                    original_value - self.learning_rate * gradient\n                )\n\n    def _adam_update(\n        self,\n        param_name: str,\n        gradient: float,\n        beta1: float = 0.9,\n        beta2: float = 0.999,\n        eps: float = 1e-8,\n    ):\n        \"\"\"Adam optimizer update step.\"\"\"\n        self._t += 1\n\n        self._m[param_name] = beta1 * self._m[param_name] + (1 - beta1) * gradient\n        self._v[param_name] = beta2 * self._v[param_name] + (1 - beta2) * gradient**2\n\n        m_hat = self._m[param_name] / (1 - beta1**self._t)\n        v_hat = self._v[param_name] / (1 - beta2**self._t)\n\n        self.expression.params[param_name] -= (\n            self.learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n        )\n\n    def reset(self):\n        \"\"\"Reset trainer state.\"\"\"\n        self.history = {\"loss\": [], \"params\": []}\n        self._t = 0\n        for param_name in self.expression.trainable_params:\n            self._m[param_name] = 0.0\n            self._v[param_name] = 0.0\n</code></pre>"},{"location":"api/reference/#neurogebra.core.trainer.Trainer-functions","title":"Functions","text":""},{"location":"api/reference/#neurogebra.core.trainer.Trainer.__init__","title":"<code>__init__(expression, learning_rate=0.01, optimizer='sgd')</code>","text":"<p>Initialize Trainer.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>Expression</code> <p>Expression to train</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate for optimization</p> <code>0.01</code> <code>optimizer</code> <code>str</code> <p>Optimization algorithm ('sgd', 'adam')</p> <code>'sgd'</code> Source code in <code>src/neurogebra/core/trainer.py</code> <pre><code>def __init__(\n    self,\n    expression: Expression,\n    learning_rate: float = 0.01,\n    optimizer: str = \"sgd\",\n):\n    \"\"\"\n    Initialize Trainer.\n\n    Args:\n        expression: Expression to train\n        learning_rate: Learning rate for optimization\n        optimizer: Optimization algorithm ('sgd', 'adam')\n    \"\"\"\n    self.expression = expression\n    self.learning_rate = learning_rate\n    self.optimizer = optimizer\n    self.history: Dict[str, List] = {\"loss\": [], \"params\": []}\n\n    # Adam optimizer state\n    self._m: Dict[str, float] = {}\n    self._v: Dict[str, float] = {}\n    self._t = 0\n\n    for param_name in self.expression.trainable_params:\n        self._m[param_name] = 0.0\n        self._v[param_name] = 0.0\n</code></pre>"},{"location":"api/reference/#neurogebra.core.trainer.Trainer.fit","title":"<code>fit(X, y, epochs=100, batch_size=None, loss_fn='mse', verbose=True, callback=None)</code>","text":"<p>Fit expression to data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data (N,) or (N, D)</p> required <code>y</code> <code>ndarray</code> <p>Target data (N,)</p> required <code>epochs</code> <code>int</code> <p>Number of training epochs</p> <code>100</code> <code>batch_size</code> <code>Optional[int]</code> <p>Mini-batch size (None = full batch)</p> <code>None</code> <code>loss_fn</code> <code>str</code> <p>Loss function ('mse', 'mae')</p> <code>'mse'</code> <code>verbose</code> <code>bool</code> <p>Print training progress</p> <code>True</code> <code>callback</code> <code>Optional[Callable]</code> <p>Optional callback function called each epoch</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, List]</code> <p>Training history with loss and parameter values per epoch</p> Source code in <code>src/neurogebra/core/trainer.py</code> <pre><code>def fit(\n    self,\n    X: np.ndarray,\n    y: np.ndarray,\n    epochs: int = 100,\n    batch_size: Optional[int] = None,\n    loss_fn: str = \"mse\",\n    verbose: bool = True,\n    callback: Optional[Callable] = None,\n) -&gt; Dict[str, List]:\n    \"\"\"\n    Fit expression to data.\n\n    Args:\n        X: Input data (N,) or (N, D)\n        y: Target data (N,)\n        epochs: Number of training epochs\n        batch_size: Mini-batch size (None = full batch)\n        loss_fn: Loss function ('mse', 'mae')\n        verbose: Print training progress\n        callback: Optional callback function called each epoch\n\n    Returns:\n        Training history with loss and parameter values per epoch\n    \"\"\"\n    X = np.asarray(X, dtype=np.float64)\n    y = np.asarray(y, dtype=np.float64)\n\n    if batch_size is None:\n        batch_size = len(X)\n\n    for epoch in range(epochs):\n        # Shuffle data\n        indices = np.random.permutation(len(X))\n        X_shuffled = X[indices]\n        y_shuffled = y[indices]\n\n        epoch_loss = 0.0\n        n_batches = 0\n\n        # Mini-batch training\n        for i in range(0, len(X), batch_size):\n            X_batch = X_shuffled[i : i + batch_size]\n            y_batch = y_shuffled[i : i + batch_size]\n\n            # Forward pass\n            predictions = self._forward(X_batch)\n\n            # Compute loss\n            loss = self._compute_loss(predictions, y_batch, loss_fn)\n            epoch_loss += loss\n            n_batches += 1\n\n            # Backward pass (numerical gradients for MVP)\n            self._backward(X_batch, y_batch, loss_fn)\n\n        # Record history\n        avg_loss = epoch_loss / n_batches\n        self.history[\"loss\"].append(avg_loss)\n        self.history[\"params\"].append(self.expression.params.copy())\n\n        if verbose and (epoch % max(1, epochs // 10) == 0 or epoch == epochs - 1):\n            print(f\"Epoch {epoch:&gt;4d}/{epochs}: Loss = {avg_loss:.6f}\")\n\n        if callback is not None:\n            callback(epoch, avg_loss, self.expression.params.copy())\n\n    return self.history\n</code></pre>"},{"location":"api/reference/#neurogebra.core.trainer.Trainer.reset","title":"<code>reset()</code>","text":"<p>Reset trainer state.</p> Source code in <code>src/neurogebra/core/trainer.py</code> <pre><code>def reset(self):\n    \"\"\"Reset trainer state.\"\"\"\n    self.history = {\"loss\": [], \"params\": []}\n    self._t = 0\n    for param_name in self.expression.trainable_params:\n        self._m[param_name] = 0.0\n        self._v[param_name] = 0.0\n</code></pre>"},{"location":"api/reference/#value-autograd","title":"Value (Autograd)","text":"<p>Scalar value with automatic differentiation \u2014 the engine behind backpropagation.</p>"},{"location":"api/reference/#neurogebra.core.autograd.Value","title":"<code>neurogebra.core.autograd.Value</code>","text":"<p>Scalar value with automatic differentiation support.</p> <p>Inspired by micrograd, this class wraps numerical values and tracks computational graphs for backpropagation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; a = Value(2.0)\n&gt;&gt;&gt; b = Value(3.0)\n&gt;&gt;&gt; c = a * b + a\n&gt;&gt;&gt; c.backward()\n&gt;&gt;&gt; print(a.grad)  # dc/da = b + 1 = 4.0\n</code></pre> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>class Value:\n    \"\"\"\n    Scalar value with automatic differentiation support.\n\n    Inspired by micrograd, this class wraps numerical values and tracks\n    computational graphs for backpropagation.\n\n    Examples:\n        &gt;&gt;&gt; a = Value(2.0)\n        &gt;&gt;&gt; b = Value(3.0)\n        &gt;&gt;&gt; c = a * b + a\n        &gt;&gt;&gt; c.backward()\n        &gt;&gt;&gt; print(a.grad)  # dc/da = b + 1 = 4.0\n    \"\"\"\n\n    def __init__(self, data: float, _children: Tuple = (), _op: str = \"\"):\n        \"\"\"\n        Initialize a Value node.\n\n        Args:\n            data: Numerical value\n            _children: Parent nodes in computation graph\n            _op: Operation that created this node\n        \"\"\"\n        self.data = float(data)\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        \"\"\"Addition with gradient tracking.\"\"\"\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), \"+\")\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        \"\"\"Multiplication with gradient tracking.\"\"\"\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), \"*\")\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def __pow__(self, other):\n        \"\"\"Power operation with gradient tracking.\"\"\"\n        assert isinstance(other, (int, float)), \"only int/float powers supported\"\n        out = Value(self.data**other, (self,), f\"**{other}\")\n\n        def _backward():\n            self.grad += other * (self.data ** (other - 1)) * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        \"\"\"ReLU activation with gradient.\"\"\"\n        out = Value(max(0, self.data), (self,), \"ReLU\")\n\n        def _backward():\n            self.grad += (out.data &gt; 0) * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def sigmoid(self):\n        \"\"\"Sigmoid activation with gradient.\"\"\"\n        sig = 1.0 / (1.0 + np.exp(-self.data))\n        out = Value(sig, (self,), \"sigmoid\")\n\n        def _backward():\n            self.grad += sig * (1 - sig) * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def tanh(self):\n        \"\"\"Tanh activation with gradient.\"\"\"\n        t = np.tanh(self.data)\n        out = Value(t, (self,), \"tanh\")\n\n        def _backward():\n            self.grad += (1 - t**2) * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def exp(self):\n        \"\"\"Exponential with gradient.\"\"\"\n        e = np.exp(self.data)\n        out = Value(e, (self,), \"exp\")\n\n        def _backward():\n            self.grad += e * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def log(self):\n        \"\"\"Natural logarithm with gradient.\"\"\"\n        out = Value(np.log(self.data), (self,), \"log\")\n\n        def _backward():\n            self.grad += (1.0 / self.data) * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        \"\"\"\n        Compute gradients via backpropagation.\n\n        Performs topological sort and calls _backward on each node.\n        \"\"\"\n        # Build topological order\n        topo: List[Value] = []\n        visited: Set[int] = set()\n\n        def build_topo(v: Value):\n            if id(v) not in visited:\n                visited.add(id(v))\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n\n        build_topo(self)\n\n        # Backpropagate\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n    def zero_grad(self):\n        \"\"\"Reset gradient to zero.\"\"\"\n        self.grad = 0.0\n\n    def __neg__(self):\n        return self * -1\n\n    def __radd__(self, other):\n        return self + other\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __rsub__(self, other):\n        return other + (-self)\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __truediv__(self, other):\n        return self * other**-1\n\n    def __rtruediv__(self, other):\n        return other * self**-1\n\n    def __lt__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        return self.data &lt; other.data\n\n    def __gt__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        return self.data &gt; other.data\n\n    def __eq__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        return self.data == other.data\n\n    def __hash__(self):\n        return id(self)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Value-functions","title":"Functions","text":""},{"location":"api/reference/#neurogebra.core.autograd.Value.__init__","title":"<code>__init__(data, _children=(), _op='')</code>","text":"<p>Initialize a Value node.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>float</code> <p>Numerical value</p> required <code>_children</code> <code>Tuple</code> <p>Parent nodes in computation graph</p> <code>()</code> <code>_op</code> <code>str</code> <p>Operation that created this node</p> <code>''</code> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def __init__(self, data: float, _children: Tuple = (), _op: str = \"\"):\n    \"\"\"\n    Initialize a Value node.\n\n    Args:\n        data: Numerical value\n        _children: Parent nodes in computation graph\n        _op: Operation that created this node\n    \"\"\"\n    self.data = float(data)\n    self.grad = 0.0\n    self._backward = lambda: None\n    self._prev = set(_children)\n    self._op = _op\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Value.__add__","title":"<code>__add__(other)</code>","text":"<p>Addition with gradient tracking.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def __add__(self, other):\n    \"\"\"Addition with gradient tracking.\"\"\"\n    other = other if isinstance(other, Value) else Value(other)\n    out = Value(self.data + other.data, (self, other), \"+\")\n\n    def _backward():\n        self.grad += out.grad\n        other.grad += out.grad\n\n    out._backward = _backward\n\n    return out\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Value.__mul__","title":"<code>__mul__(other)</code>","text":"<p>Multiplication with gradient tracking.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def __mul__(self, other):\n    \"\"\"Multiplication with gradient tracking.\"\"\"\n    other = other if isinstance(other, Value) else Value(other)\n    out = Value(self.data * other.data, (self, other), \"*\")\n\n    def _backward():\n        self.grad += other.data * out.grad\n        other.grad += self.data * out.grad\n\n    out._backward = _backward\n\n    return out\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Value.__pow__","title":"<code>__pow__(other)</code>","text":"<p>Power operation with gradient tracking.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def __pow__(self, other):\n    \"\"\"Power operation with gradient tracking.\"\"\"\n    assert isinstance(other, (int, float)), \"only int/float powers supported\"\n    out = Value(self.data**other, (self,), f\"**{other}\")\n\n    def _backward():\n        self.grad += other * (self.data ** (other - 1)) * out.grad\n\n    out._backward = _backward\n\n    return out\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Value.relu","title":"<code>relu()</code>","text":"<p>ReLU activation with gradient.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def relu(self):\n    \"\"\"ReLU activation with gradient.\"\"\"\n    out = Value(max(0, self.data), (self,), \"ReLU\")\n\n    def _backward():\n        self.grad += (out.data &gt; 0) * out.grad\n\n    out._backward = _backward\n\n    return out\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Value.sigmoid","title":"<code>sigmoid()</code>","text":"<p>Sigmoid activation with gradient.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def sigmoid(self):\n    \"\"\"Sigmoid activation with gradient.\"\"\"\n    sig = 1.0 / (1.0 + np.exp(-self.data))\n    out = Value(sig, (self,), \"sigmoid\")\n\n    def _backward():\n        self.grad += sig * (1 - sig) * out.grad\n\n    out._backward = _backward\n\n    return out\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Value.tanh","title":"<code>tanh()</code>","text":"<p>Tanh activation with gradient.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def tanh(self):\n    \"\"\"Tanh activation with gradient.\"\"\"\n    t = np.tanh(self.data)\n    out = Value(t, (self,), \"tanh\")\n\n    def _backward():\n        self.grad += (1 - t**2) * out.grad\n\n    out._backward = _backward\n\n    return out\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Value.exp","title":"<code>exp()</code>","text":"<p>Exponential with gradient.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def exp(self):\n    \"\"\"Exponential with gradient.\"\"\"\n    e = np.exp(self.data)\n    out = Value(e, (self,), \"exp\")\n\n    def _backward():\n        self.grad += e * out.grad\n\n    out._backward = _backward\n\n    return out\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Value.log","title":"<code>log()</code>","text":"<p>Natural logarithm with gradient.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def log(self):\n    \"\"\"Natural logarithm with gradient.\"\"\"\n    out = Value(np.log(self.data), (self,), \"log\")\n\n    def _backward():\n        self.grad += (1.0 / self.data) * out.grad\n\n    out._backward = _backward\n\n    return out\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Value.backward","title":"<code>backward()</code>","text":"<p>Compute gradients via backpropagation.</p> <p>Performs topological sort and calls _backward on each node.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def backward(self):\n    \"\"\"\n    Compute gradients via backpropagation.\n\n    Performs topological sort and calls _backward on each node.\n    \"\"\"\n    # Build topological order\n    topo: List[Value] = []\n    visited: Set[int] = set()\n\n    def build_topo(v: Value):\n        if id(v) not in visited:\n            visited.add(id(v))\n            for child in v._prev:\n                build_topo(child)\n            topo.append(v)\n\n    build_topo(self)\n\n    # Backpropagate\n    self.grad = 1.0\n    for node in reversed(topo):\n        node._backward()\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Value.zero_grad","title":"<code>zero_grad()</code>","text":"<p>Reset gradient to zero.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def zero_grad(self):\n    \"\"\"Reset gradient to zero.\"\"\"\n    self.grad = 0.0\n</code></pre>"},{"location":"api/reference/#tensor-autograd","title":"Tensor (Autograd)","text":"<p>Multi-dimensional array with gradient tracking for batch operations.</p>"},{"location":"api/reference/#neurogebra.core.autograd.Tensor","title":"<code>neurogebra.core.autograd.Tensor</code>","text":"<p>Multi-dimensional array with autograd support.</p> <p>Extends Value concept to tensors for mini-batch training.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; t = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n&gt;&gt;&gt; result = t.sum()\n&gt;&gt;&gt; result.backward()\n&gt;&gt;&gt; print(t.grad)  # [1.0, 1.0, 1.0]\n</code></pre> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>class Tensor:\n    \"\"\"\n    Multi-dimensional array with autograd support.\n\n    Extends Value concept to tensors for mini-batch training.\n\n    Examples:\n        &gt;&gt;&gt; t = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n        &gt;&gt;&gt; result = t.sum()\n        &gt;&gt;&gt; result.backward()\n        &gt;&gt;&gt; print(t.grad)  # [1.0, 1.0, 1.0]\n    \"\"\"\n\n    def __init__(self, data: Union[List, np.ndarray, Any], requires_grad: bool = False):\n        \"\"\"\n        Initialize a Tensor.\n\n        Args:\n            data: Array-like data (list, numpy array, etc.)\n            requires_grad: Whether to track gradients\n        \"\"\"\n        self.data = np.array(data, dtype=np.float64)\n        self.grad: Optional[np.ndarray] = None\n        self.requires_grad = requires_grad\n        self._grad_fn = None\n        self._prev: Set[\"Tensor\"] = set()\n\n        if requires_grad:\n            self.grad = np.zeros_like(self.data)\n\n    @property\n    def shape(self) -&gt; Tuple:\n        \"\"\"Return the shape of the tensor.\"\"\"\n        return self.data.shape\n\n    @property\n    def ndim(self) -&gt; int:\n        \"\"\"Return the number of dimensions.\"\"\"\n        return self.data.ndim\n\n    def backward(self, gradient: Optional[np.ndarray] = None):\n        \"\"\"\n        Compute gradients via backpropagation.\n\n        Args:\n            gradient: Upstream gradient. If None, uses ones.\n        \"\"\"\n        if not self.requires_grad:\n            return\n\n        if gradient is None:\n            gradient = np.ones_like(self.data)\n\n        if self.grad is not None:\n            self.grad += gradient\n        else:\n            self.grad = gradient.copy()\n\n        if self._grad_fn is not None:\n            self._grad_fn(gradient)\n\n    def zero_grad(self):\n        \"\"\"Reset gradient to zero.\"\"\"\n        if self.requires_grad:\n            self.grad = np.zeros_like(self.data)\n\n    def sum(self) -&gt; \"Tensor\":\n        \"\"\"Sum all elements.\"\"\"\n        out = Tensor(np.sum(self.data), requires_grad=self.requires_grad)\n        out._prev = {self}\n\n        def _grad_fn(gradient):\n            self.backward(gradient * np.ones_like(self.data))\n\n        out._grad_fn = _grad_fn\n        return out\n\n    def mean(self) -&gt; \"Tensor\":\n        \"\"\"Mean of all elements.\"\"\"\n        n = self.data.size\n        out = Tensor(np.mean(self.data), requires_grad=self.requires_grad)\n        out._prev = {self}\n\n        def _grad_fn(gradient):\n            self.backward(gradient * np.ones_like(self.data) / n)\n\n        out._grad_fn = _grad_fn\n        return out\n\n    def __add__(self, other):\n        \"\"\"Element-wise addition.\"\"\"\n        if isinstance(other, Tensor):\n            out = Tensor(\n                self.data + other.data,\n                requires_grad=self.requires_grad or other.requires_grad,\n            )\n            out._prev = {self, other}\n\n            def _grad_fn(gradient):\n                if self.requires_grad:\n                    self.backward(gradient)\n                if other.requires_grad:\n                    other.backward(gradient)\n\n            out._grad_fn = _grad_fn\n        else:\n            out = Tensor(self.data + other, requires_grad=self.requires_grad)\n            out._prev = {self}\n\n            def _grad_fn(gradient):\n                if self.requires_grad:\n                    self.backward(gradient)\n\n            out._grad_fn = _grad_fn\n\n        return out\n\n    def __mul__(self, other):\n        \"\"\"Element-wise multiplication.\"\"\"\n        if isinstance(other, Tensor):\n            out = Tensor(\n                self.data * other.data,\n                requires_grad=self.requires_grad or other.requires_grad,\n            )\n            out._prev = {self, other}\n\n            def _grad_fn(gradient):\n                if self.requires_grad:\n                    self.backward(gradient * other.data)\n                if other.requires_grad:\n                    other.backward(gradient * self.data)\n\n            out._grad_fn = _grad_fn\n        else:\n            out = Tensor(self.data * other, requires_grad=self.requires_grad)\n            out._prev = {self}\n\n            def _grad_fn(gradient):\n                if self.requires_grad:\n                    self.backward(gradient * other)\n\n            out._grad_fn = _grad_fn\n\n        return out\n\n    def __sub__(self, other):\n        \"\"\"Element-wise subtraction.\"\"\"\n        if isinstance(other, Tensor):\n            return self + Tensor(-other.data, requires_grad=other.requires_grad)\n        return self + (-other)\n\n    def __neg__(self):\n        \"\"\"Negate tensor.\"\"\"\n        return self * (-1)\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __radd__(self, other):\n        return self + other\n\n    def __pow__(self, power):\n        \"\"\"Element-wise power.\"\"\"\n        out = Tensor(self.data**power, requires_grad=self.requires_grad)\n        out._prev = {self}\n\n        def _grad_fn(gradient):\n            if self.requires_grad:\n                self.backward(gradient * power * self.data ** (power - 1))\n\n        out._grad_fn = _grad_fn\n        return out\n\n    def __repr__(self):\n        return f\"Tensor(shape={self.data.shape}, requires_grad={self.requires_grad})\"\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Tensor-attributes","title":"Attributes","text":""},{"location":"api/reference/#neurogebra.core.autograd.Tensor.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Return the shape of the tensor.</p>"},{"location":"api/reference/#neurogebra.core.autograd.Tensor.ndim","title":"<code>ndim</code>  <code>property</code>","text":"<p>Return the number of dimensions.</p>"},{"location":"api/reference/#neurogebra.core.autograd.Tensor-functions","title":"Functions","text":""},{"location":"api/reference/#neurogebra.core.autograd.Tensor.__init__","title":"<code>__init__(data, requires_grad=False)</code>","text":"<p>Initialize a Tensor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[List, ndarray, Any]</code> <p>Array-like data (list, numpy array, etc.)</p> required <code>requires_grad</code> <code>bool</code> <p>Whether to track gradients</p> <code>False</code> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def __init__(self, data: Union[List, np.ndarray, Any], requires_grad: bool = False):\n    \"\"\"\n    Initialize a Tensor.\n\n    Args:\n        data: Array-like data (list, numpy array, etc.)\n        requires_grad: Whether to track gradients\n    \"\"\"\n    self.data = np.array(data, dtype=np.float64)\n    self.grad: Optional[np.ndarray] = None\n    self.requires_grad = requires_grad\n    self._grad_fn = None\n    self._prev: Set[\"Tensor\"] = set()\n\n    if requires_grad:\n        self.grad = np.zeros_like(self.data)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Tensor.backward","title":"<code>backward(gradient=None)</code>","text":"<p>Compute gradients via backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>gradient</code> <code>Optional[ndarray]</code> <p>Upstream gradient. If None, uses ones.</p> <code>None</code> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def backward(self, gradient: Optional[np.ndarray] = None):\n    \"\"\"\n    Compute gradients via backpropagation.\n\n    Args:\n        gradient: Upstream gradient. If None, uses ones.\n    \"\"\"\n    if not self.requires_grad:\n        return\n\n    if gradient is None:\n        gradient = np.ones_like(self.data)\n\n    if self.grad is not None:\n        self.grad += gradient\n    else:\n        self.grad = gradient.copy()\n\n    if self._grad_fn is not None:\n        self._grad_fn(gradient)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Tensor.zero_grad","title":"<code>zero_grad()</code>","text":"<p>Reset gradient to zero.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def zero_grad(self):\n    \"\"\"Reset gradient to zero.\"\"\"\n    if self.requires_grad:\n        self.grad = np.zeros_like(self.data)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Tensor.sum","title":"<code>sum()</code>","text":"<p>Sum all elements.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def sum(self) -&gt; \"Tensor\":\n    \"\"\"Sum all elements.\"\"\"\n    out = Tensor(np.sum(self.data), requires_grad=self.requires_grad)\n    out._prev = {self}\n\n    def _grad_fn(gradient):\n        self.backward(gradient * np.ones_like(self.data))\n\n    out._grad_fn = _grad_fn\n    return out\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Tensor.mean","title":"<code>mean()</code>","text":"<p>Mean of all elements.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def mean(self) -&gt; \"Tensor\":\n    \"\"\"Mean of all elements.\"\"\"\n    n = self.data.size\n    out = Tensor(np.mean(self.data), requires_grad=self.requires_grad)\n    out._prev = {self}\n\n    def _grad_fn(gradient):\n        self.backward(gradient * np.ones_like(self.data) / n)\n\n    out._grad_fn = _grad_fn\n    return out\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Tensor.__add__","title":"<code>__add__(other)</code>","text":"<p>Element-wise addition.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def __add__(self, other):\n    \"\"\"Element-wise addition.\"\"\"\n    if isinstance(other, Tensor):\n        out = Tensor(\n            self.data + other.data,\n            requires_grad=self.requires_grad or other.requires_grad,\n        )\n        out._prev = {self, other}\n\n        def _grad_fn(gradient):\n            if self.requires_grad:\n                self.backward(gradient)\n            if other.requires_grad:\n                other.backward(gradient)\n\n        out._grad_fn = _grad_fn\n    else:\n        out = Tensor(self.data + other, requires_grad=self.requires_grad)\n        out._prev = {self}\n\n        def _grad_fn(gradient):\n            if self.requires_grad:\n                self.backward(gradient)\n\n        out._grad_fn = _grad_fn\n\n    return out\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Tensor.__mul__","title":"<code>__mul__(other)</code>","text":"<p>Element-wise multiplication.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def __mul__(self, other):\n    \"\"\"Element-wise multiplication.\"\"\"\n    if isinstance(other, Tensor):\n        out = Tensor(\n            self.data * other.data,\n            requires_grad=self.requires_grad or other.requires_grad,\n        )\n        out._prev = {self, other}\n\n        def _grad_fn(gradient):\n            if self.requires_grad:\n                self.backward(gradient * other.data)\n            if other.requires_grad:\n                other.backward(gradient * self.data)\n\n        out._grad_fn = _grad_fn\n    else:\n        out = Tensor(self.data * other, requires_grad=self.requires_grad)\n        out._prev = {self}\n\n        def _grad_fn(gradient):\n            if self.requires_grad:\n                self.backward(gradient * other)\n\n        out._grad_fn = _grad_fn\n\n    return out\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Tensor.__sub__","title":"<code>__sub__(other)</code>","text":"<p>Element-wise subtraction.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def __sub__(self, other):\n    \"\"\"Element-wise subtraction.\"\"\"\n    if isinstance(other, Tensor):\n        return self + Tensor(-other.data, requires_grad=other.requires_grad)\n    return self + (-other)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Tensor.__neg__","title":"<code>__neg__()</code>","text":"<p>Negate tensor.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def __neg__(self):\n    \"\"\"Negate tensor.\"\"\"\n    return self * (-1)\n</code></pre>"},{"location":"api/reference/#neurogebra.core.autograd.Tensor.__pow__","title":"<code>__pow__(power)</code>","text":"<p>Element-wise power.</p> Source code in <code>src/neurogebra/core/autograd.py</code> <pre><code>def __pow__(self, power):\n    \"\"\"Element-wise power.\"\"\"\n    out = Tensor(self.data**power, requires_grad=self.requires_grad)\n    out._prev = {self}\n\n    def _grad_fn(gradient):\n        if self.requires_grad:\n            self.backward(gradient * power * self.data ** (power - 1))\n\n    out._grad_fn = _grad_fn\n    return out\n</code></pre>"},{"location":"api/reference/#builders","title":"Builders","text":""},{"location":"api/reference/#modelbuilder","title":"ModelBuilder","text":"<p>Keras-like interface for building neural network architectures layer by layer.</p>"},{"location":"api/reference/#neurogebra.builders.model_builder.ModelBuilder","title":"<code>neurogebra.builders.model_builder.ModelBuilder</code>","text":"<p>Build neural networks with an educational, intuitive interface.</p> <p>ModelBuilder makes it easy for beginners to: - Understand what they're building - Get guidance on architecture choices - See what each layer does - Learn best practices</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from neurogebra.builders import ModelBuilder\n&gt;&gt;&gt; builder = ModelBuilder()\n&gt;&gt;&gt; model = builder.Sequential([\n...     builder.Dense(128, activation=\"relu\"),\n...     builder.Dropout(0.2),\n...     builder.Dense(10, activation=\"softmax\")\n... ])\n&gt;&gt;&gt; model.summary()\n</code></pre> Source code in <code>src/neurogebra/builders/model_builder.py</code> <pre><code>class ModelBuilder:\n    \"\"\"\n    Build neural networks with an educational, intuitive interface.\n\n    ModelBuilder makes it easy for beginners to:\n    - Understand what they're building\n    - Get guidance on architecture choices\n    - See what each layer does\n    - Learn best practices\n\n    Examples:\n        &gt;&gt;&gt; from neurogebra.builders import ModelBuilder\n        &gt;&gt;&gt; builder = ModelBuilder()\n        &gt;&gt;&gt; model = builder.Sequential([\n        ...     builder.Dense(128, activation=\"relu\"),\n        ...     builder.Dropout(0.2),\n        ...     builder.Dense(10, activation=\"softmax\")\n        ... ])\n        &gt;&gt;&gt; model.summary()\n    \"\"\"\n\n    def __init__(self, craft: Optional[Any] = None):\n        \"\"\"\n        Initialize ModelBuilder.\n\n        Args:\n            craft: NeuroCraft instance for expression access.\n                   If None, a default one is created internally.\n        \"\"\"\n        self.craft = craft\n        self._templates = self._load_templates()\n\n    def _get_craft(self):\n        \"\"\"Lazily initialize craft if needed.\"\"\"\n        if self.craft is None:\n            from neurogebra.core.neurocraft import NeuroCraft\n\n            self.craft = NeuroCraft(educational_mode=False)\n        return self.craft\n\n    def _load_templates(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Load pre-built model templates for common tasks.\"\"\"\n        return {\n            \"simple_classifier\": {\n                \"description\": \"Basic neural network for classification\",\n                \"layers\": [\n                    {\"type\": \"dense\", \"units\": 128, \"activation\": \"relu\"},\n                    {\"type\": \"dropout\", \"rate\": 0.2},\n                    {\"type\": \"dense\", \"units\": 64, \"activation\": \"relu\"},\n                    {\"type\": \"dense\", \"units\": 10, \"activation\": \"softmax\"},\n                ],\n                \"good_for\": [\n                    \"MNIST\",\n                    \"CIFAR-10\",\n                    \"Simple tabular data\",\n                ],\n            },\n            \"image_classifier\": {\n                \"description\": \"CNN for image classification\",\n                \"layers\": [\n                    {\n                        \"type\": \"conv2d\",\n                        \"filters\": 32,\n                        \"kernel_size\": 3,\n                        \"activation\": \"relu\",\n                    },\n                    {\"type\": \"maxpool2d\", \"pool_size\": 2},\n                    {\n                        \"type\": \"conv2d\",\n                        \"filters\": 64,\n                        \"kernel_size\": 3,\n                        \"activation\": \"relu\",\n                    },\n                    {\"type\": \"maxpool2d\", \"pool_size\": 2},\n                    {\"type\": \"flatten\"},\n                    {\"type\": \"dense\", \"units\": 128, \"activation\": \"relu\"},\n                    {\"type\": \"dense\", \"units\": 10, \"activation\": \"softmax\"},\n                ],\n                \"good_for\": [\n                    \"Image recognition\",\n                    \"Object classification\",\n                ],\n            },\n            \"regression\": {\n                \"description\": \"Network for predicting continuous values\",\n                \"layers\": [\n                    {\"type\": \"dense\", \"units\": 64, \"activation\": \"relu\"},\n                    {\"type\": \"dense\", \"units\": 32, \"activation\": \"relu\"},\n                    {\"type\": \"dense\", \"units\": 1, \"activation\": \"linear\"},\n                ],\n                \"good_for\": [\n                    \"House price prediction\",\n                    \"Stock prices\",\n                    \"Any regression task\",\n                ],\n            },\n            \"binary_classifier\": {\n                \"description\": \"Network for binary yes/no classification\",\n                \"layers\": [\n                    {\"type\": \"dense\", \"units\": 64, \"activation\": \"relu\"},\n                    {\"type\": \"dropout\", \"rate\": 0.3},\n                    {\"type\": \"dense\", \"units\": 32, \"activation\": \"relu\"},\n                    {\"type\": \"dense\", \"units\": 1, \"activation\": \"sigmoid\"},\n                ],\n                \"good_for\": [\n                    \"Spam detection\",\n                    \"Medical diagnosis\",\n                    \"Sentiment analysis\",\n                ],\n            },\n        }\n\n    # ---- Layer creation methods ----\n\n    def Dense(\n        self,\n        units: int,\n        activation: Optional[str] = None,\n        input_shape: Optional[tuple] = None,\n        **kwargs,\n    ) -&gt; Layer:\n        \"\"\"\n        Create a fully connected (dense) layer.\n\n        Args:\n            units: Number of neurons\n            activation: Activation function name\n            input_shape: Shape of input (only for first layer)\n\n        Returns:\n            Layer instance\n\n        Examples:\n            &gt;&gt;&gt; builder = ModelBuilder()\n            &gt;&gt;&gt; layer = builder.Dense(128, activation=\"relu\")\n            &gt;&gt;&gt; layer.explain()  # Learn what it does\n        \"\"\"\n        extra = {**kwargs}\n        if input_shape is not None:\n            extra[\"input_shape\"] = input_shape\n        return Layer(\"dense\", units=units, activation=activation, **extra)\n\n    def Conv2D(\n        self,\n        filters: int,\n        kernel_size: int = 3,\n        activation: Optional[str] = None,\n        **kwargs,\n    ) -&gt; Layer:\n        \"\"\"\n        Create a 2D convolutional layer for images.\n\n        Args:\n            filters: Number of filters / feature detectors\n            kernel_size: Size of the filter window\n            activation: Activation function\n\n        Returns:\n            Layer instance\n        \"\"\"\n        return Layer(\n            \"conv2d\",\n            units=filters,\n            activation=activation,\n            kernel_size=kernel_size,\n            **kwargs,\n        )\n\n    def Dropout(self, rate: float = 0.2) -&gt; Layer:\n        \"\"\"\n        Create a dropout layer for regularization.\n\n        Args:\n            rate: Fraction of neurons to drop (0.0 to 1.0)\n\n        Returns:\n            Layer instance\n        \"\"\"\n        return Layer(\"dropout\", rate=rate)\n\n    def BatchNorm(self) -&gt; Layer:\n        \"\"\"Create a batch normalization layer.\"\"\"\n        return Layer(\"batchnorm\")\n\n    def MaxPooling2D(self, pool_size: int = 2) -&gt; Layer:\n        \"\"\"Create a max pooling layer.\"\"\"\n        return Layer(\"maxpool2d\", pool_size=pool_size)\n\n    def Flatten(self) -&gt; Layer:\n        \"\"\"Create a flatten layer.\"\"\"\n        return Layer(\"flatten\")\n\n    # ---- Model construction ----\n\n    def Sequential(\n        self, layers: List[Layer], name: Optional[str] = None\n    ) -&gt; \"Model\":\n        \"\"\"\n        Build a sequential model (layers stacked one after another).\n\n        Args:\n            layers: List of Layer instances\n            name: Optional name for the model\n\n        Returns:\n            Model instance ready for compilation and training\n\n        Examples:\n            &gt;&gt;&gt; builder = ModelBuilder()\n            &gt;&gt;&gt; model = builder.Sequential([\n            ...     builder.Dense(128, activation=\"relu\"),\n            ...     builder.Dropout(0.2),\n            ...     builder.Dense(10, activation=\"softmax\")\n            ... ])\n            &gt;&gt;&gt; model.summary()\n        \"\"\"\n        return Model(layers=layers, name=name, craft=self._get_craft())\n\n    def from_template(\n        self,\n        template_name: str,\n        customize: Optional[Dict] = None,\n    ) -&gt; \"Model\":\n        \"\"\"\n        Create a model from a pre-built template.\n\n        Args:\n            template_name: Name of template (e.g. 'simple_classifier')\n            customize: Optional customizations (not yet implemented)\n\n        Returns:\n            Model instance\n\n        Examples:\n            &gt;&gt;&gt; builder = ModelBuilder()\n            &gt;&gt;&gt; model = builder.from_template(\"simple_classifier\")\n            &gt;&gt;&gt; model.explain_architecture()\n        \"\"\"\n        if template_name not in self._templates:\n            available = \", \".join(self._templates.keys())\n            raise ValueError(\n                f\"Template '{template_name}' not found.\\n\"\n                f\"Available templates: {available}\\n\"\n                f\"Use builder.list_templates() to see details.\"\n            )\n\n        template = self._templates[template_name]\n\n        # Deep-copy so repeated calls don't mutate the template\n        layer_specs = copy.deepcopy(template[\"layers\"])\n\n        layers = []\n        for layer_spec in layer_specs:\n            layer_type = layer_spec.pop(\"type\")\n\n            if layer_type == \"dense\":\n                layers.append(self.Dense(**layer_spec))\n            elif layer_type == \"conv2d\":\n                layers.append(self.Conv2D(**layer_spec))\n            elif layer_type == \"dropout\":\n                layers.append(self.Dropout(**layer_spec))\n            elif layer_type == \"maxpool2d\":\n                layers.append(self.MaxPooling2D(**layer_spec))\n            elif layer_type == \"flatten\":\n                layers.append(self.Flatten())\n            elif layer_type == \"batchnorm\":\n                layers.append(self.BatchNorm())\n\n        model = self.Sequential(layers, name=template_name)\n        model.template_info = template\n\n        return model\n\n    def list_templates(self):\n        \"\"\"Show all available model templates with descriptions.\"\"\"\n        print(\"\\n\ud83c\udfd7\ufe0f  Available Model Templates:\\n\")\n\n        for name, template in self._templates.items():\n            print(f\"  \ud83d\udce6 {name}\")\n            print(f\"     {template['description']}\")\n            print(f\"     Good for: {', '.join(template['good_for'])}\")\n            print()\n\n    def suggest_architecture(\n        self,\n        task: str,\n        input_shape: tuple,\n        output_size: int,\n    ):\n        \"\"\"\n        Get architecture suggestions based on your task.\n\n        Args:\n            task: 'classification', 'regression', 'image_classification', etc.\n            input_shape: Shape of your input data\n            output_size: Number of outputs\n\n        Examples:\n            &gt;&gt;&gt; builder = ModelBuilder()\n            &gt;&gt;&gt; builder.suggest_architecture(\n            ...     task=\"image_classification\",\n            ...     input_shape=(28, 28, 1),\n            ...     output_size=10\n            ... )\n        \"\"\"\n        print(f\"\\n\ud83d\udca1 Architecture Suggestions for {task}:\\n\")\n\n        if task in (\"classification\", \"image_classification\"):\n            if len(input_shape) == 3:\n                print(\"   Recommended: Convolutional Neural Network (CNN)\")\n                print(\n                    \"   Why: CNNs are designed to detect patterns in images\"\n                )\n                print(\"\\n   Suggested architecture:\")\n                print(\"   1. Conv2D(32, kernel_size=3) + ReLU\")\n                print(\"   2. MaxPooling2D()\")\n                print(\"   3. Conv2D(64, kernel_size=3) + ReLU\")\n                print(\"   4. MaxPooling2D()\")\n                print(\"   5. Flatten()\")\n                print(\"   6. Dense(128) + ReLU\")\n                print(\"   7. Dropout(0.3)\")\n                print(f\"   8. Dense({output_size}) + Softmax\")\n                print(\"\\n   Use: builder.from_template('image_classifier')\")\n            else:\n                print(\"   Recommended: Simple Feedforward Network\")\n                print(\"   Why: Efficient for structured/tabular data\")\n                print(\"\\n   Suggested architecture:\")\n                print(\"   1. Dense(128) + ReLU\")\n                print(\"   2. Dropout(0.2)\")\n                print(\"   3. Dense(64) + ReLU\")\n                print(f\"   4. Dense({output_size}) + Softmax\")\n                print(\n                    \"\\n   Use: builder.from_template('simple_classifier')\"\n                )\n\n        elif task == \"regression\":\n            print(\"   Recommended: Regression Network\")\n            print(\"   Why: Predicts continuous values\")\n            print(\"\\n   Suggested architecture:\")\n            print(\"   1. Dense(64) + ReLU\")\n            print(\"   2. Dense(32) + ReLU\")\n            print(\"   3. Dense(1) + Linear (no activation)\")\n            print(\"\\n   Use: builder.from_template('regression')\")\n\n        elif task == \"binary_classification\":\n            print(\"   Recommended: Binary Classifier\")\n            print(\"   Why: Two-class output with sigmoid\")\n            print(\"\\n   Suggested architecture:\")\n            print(\"   1. Dense(64) + ReLU\")\n            print(\"   2. Dropout(0.3)\")\n            print(\"   3. Dense(32) + ReLU\")\n            print(\"   4. Dense(1) + Sigmoid\")\n            print(\n                \"\\n   Use: builder.from_template('binary_classifier')\"\n            )\n\n        print(\n            \"\\n   \ud83d\udca1 Tip: Start with these suggestions, then experiment!\"\n        )\n</code></pre>"},{"location":"api/reference/#neurogebra.builders.model_builder.ModelBuilder-functions","title":"Functions","text":""},{"location":"api/reference/#neurogebra.builders.model_builder.ModelBuilder.__init__","title":"<code>__init__(craft=None)</code>","text":"<p>Initialize ModelBuilder.</p> <p>Parameters:</p> Name Type Description Default <code>craft</code> <code>Optional[Any]</code> <p>NeuroCraft instance for expression access.    If None, a default one is created internally.</p> <code>None</code> Source code in <code>src/neurogebra/builders/model_builder.py</code> <pre><code>def __init__(self, craft: Optional[Any] = None):\n    \"\"\"\n    Initialize ModelBuilder.\n\n    Args:\n        craft: NeuroCraft instance for expression access.\n               If None, a default one is created internally.\n    \"\"\"\n    self.craft = craft\n    self._templates = self._load_templates()\n</code></pre>"},{"location":"api/reference/#neurogebra.builders.model_builder.ModelBuilder.Dense","title":"<code>Dense(units, activation=None, input_shape=None, **kwargs)</code>","text":"<p>Create a fully connected (dense) layer.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>int</code> <p>Number of neurons</p> required <code>activation</code> <code>Optional[str]</code> <p>Activation function name</p> <code>None</code> <code>input_shape</code> <code>Optional[tuple]</code> <p>Shape of input (only for first layer)</p> <code>None</code> <p>Returns:</p> Type Description <code>Layer</code> <p>Layer instance</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; builder = ModelBuilder()\n&gt;&gt;&gt; layer = builder.Dense(128, activation=\"relu\")\n&gt;&gt;&gt; layer.explain()  # Learn what it does\n</code></pre> Source code in <code>src/neurogebra/builders/model_builder.py</code> <pre><code>def Dense(\n    self,\n    units: int,\n    activation: Optional[str] = None,\n    input_shape: Optional[tuple] = None,\n    **kwargs,\n) -&gt; Layer:\n    \"\"\"\n    Create a fully connected (dense) layer.\n\n    Args:\n        units: Number of neurons\n        activation: Activation function name\n        input_shape: Shape of input (only for first layer)\n\n    Returns:\n        Layer instance\n\n    Examples:\n        &gt;&gt;&gt; builder = ModelBuilder()\n        &gt;&gt;&gt; layer = builder.Dense(128, activation=\"relu\")\n        &gt;&gt;&gt; layer.explain()  # Learn what it does\n    \"\"\"\n    extra = {**kwargs}\n    if input_shape is not None:\n        extra[\"input_shape\"] = input_shape\n    return Layer(\"dense\", units=units, activation=activation, **extra)\n</code></pre>"},{"location":"api/reference/#neurogebra.builders.model_builder.ModelBuilder.Conv2D","title":"<code>Conv2D(filters, kernel_size=3, activation=None, **kwargs)</code>","text":"<p>Create a 2D convolutional layer for images.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>int</code> <p>Number of filters / feature detectors</p> required <code>kernel_size</code> <code>int</code> <p>Size of the filter window</p> <code>3</code> <code>activation</code> <code>Optional[str]</code> <p>Activation function</p> <code>None</code> <p>Returns:</p> Type Description <code>Layer</code> <p>Layer instance</p> Source code in <code>src/neurogebra/builders/model_builder.py</code> <pre><code>def Conv2D(\n    self,\n    filters: int,\n    kernel_size: int = 3,\n    activation: Optional[str] = None,\n    **kwargs,\n) -&gt; Layer:\n    \"\"\"\n    Create a 2D convolutional layer for images.\n\n    Args:\n        filters: Number of filters / feature detectors\n        kernel_size: Size of the filter window\n        activation: Activation function\n\n    Returns:\n        Layer instance\n    \"\"\"\n    return Layer(\n        \"conv2d\",\n        units=filters,\n        activation=activation,\n        kernel_size=kernel_size,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/reference/#neurogebra.builders.model_builder.ModelBuilder.Dropout","title":"<code>Dropout(rate=0.2)</code>","text":"<p>Create a dropout layer for regularization.</p> <p>Parameters:</p> Name Type Description Default <code>rate</code> <code>float</code> <p>Fraction of neurons to drop (0.0 to 1.0)</p> <code>0.2</code> <p>Returns:</p> Type Description <code>Layer</code> <p>Layer instance</p> Source code in <code>src/neurogebra/builders/model_builder.py</code> <pre><code>def Dropout(self, rate: float = 0.2) -&gt; Layer:\n    \"\"\"\n    Create a dropout layer for regularization.\n\n    Args:\n        rate: Fraction of neurons to drop (0.0 to 1.0)\n\n    Returns:\n        Layer instance\n    \"\"\"\n    return Layer(\"dropout\", rate=rate)\n</code></pre>"},{"location":"api/reference/#neurogebra.builders.model_builder.ModelBuilder.BatchNorm","title":"<code>BatchNorm()</code>","text":"<p>Create a batch normalization layer.</p> Source code in <code>src/neurogebra/builders/model_builder.py</code> <pre><code>def BatchNorm(self) -&gt; Layer:\n    \"\"\"Create a batch normalization layer.\"\"\"\n    return Layer(\"batchnorm\")\n</code></pre>"},{"location":"api/reference/#neurogebra.builders.model_builder.ModelBuilder.MaxPooling2D","title":"<code>MaxPooling2D(pool_size=2)</code>","text":"<p>Create a max pooling layer.</p> Source code in <code>src/neurogebra/builders/model_builder.py</code> <pre><code>def MaxPooling2D(self, pool_size: int = 2) -&gt; Layer:\n    \"\"\"Create a max pooling layer.\"\"\"\n    return Layer(\"maxpool2d\", pool_size=pool_size)\n</code></pre>"},{"location":"api/reference/#neurogebra.builders.model_builder.ModelBuilder.Flatten","title":"<code>Flatten()</code>","text":"<p>Create a flatten layer.</p> Source code in <code>src/neurogebra/builders/model_builder.py</code> <pre><code>def Flatten(self) -&gt; Layer:\n    \"\"\"Create a flatten layer.\"\"\"\n    return Layer(\"flatten\")\n</code></pre>"},{"location":"api/reference/#neurogebra.builders.model_builder.ModelBuilder.Sequential","title":"<code>Sequential(layers, name=None)</code>","text":"<p>Build a sequential model (layers stacked one after another).</p> <p>Parameters:</p> Name Type Description Default <code>layers</code> <code>List[Layer]</code> <p>List of Layer instances</p> required <code>name</code> <code>Optional[str]</code> <p>Optional name for the model</p> <code>None</code> <p>Returns:</p> Type Description <code>'Model'</code> <p>Model instance ready for compilation and training</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; builder = ModelBuilder()\n&gt;&gt;&gt; model = builder.Sequential([\n...     builder.Dense(128, activation=\"relu\"),\n...     builder.Dropout(0.2),\n...     builder.Dense(10, activation=\"softmax\")\n... ])\n&gt;&gt;&gt; model.summary()\n</code></pre> Source code in <code>src/neurogebra/builders/model_builder.py</code> <pre><code>def Sequential(\n    self, layers: List[Layer], name: Optional[str] = None\n) -&gt; \"Model\":\n    \"\"\"\n    Build a sequential model (layers stacked one after another).\n\n    Args:\n        layers: List of Layer instances\n        name: Optional name for the model\n\n    Returns:\n        Model instance ready for compilation and training\n\n    Examples:\n        &gt;&gt;&gt; builder = ModelBuilder()\n        &gt;&gt;&gt; model = builder.Sequential([\n        ...     builder.Dense(128, activation=\"relu\"),\n        ...     builder.Dropout(0.2),\n        ...     builder.Dense(10, activation=\"softmax\")\n        ... ])\n        &gt;&gt;&gt; model.summary()\n    \"\"\"\n    return Model(layers=layers, name=name, craft=self._get_craft())\n</code></pre>"},{"location":"api/reference/#neurogebra.builders.model_builder.ModelBuilder.from_template","title":"<code>from_template(template_name, customize=None)</code>","text":"<p>Create a model from a pre-built template.</p> <p>Parameters:</p> Name Type Description Default <code>template_name</code> <code>str</code> <p>Name of template (e.g. 'simple_classifier')</p> required <code>customize</code> <code>Optional[Dict]</code> <p>Optional customizations (not yet implemented)</p> <code>None</code> <p>Returns:</p> Type Description <code>'Model'</code> <p>Model instance</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; builder = ModelBuilder()\n&gt;&gt;&gt; model = builder.from_template(\"simple_classifier\")\n&gt;&gt;&gt; model.explain_architecture()\n</code></pre> Source code in <code>src/neurogebra/builders/model_builder.py</code> <pre><code>def from_template(\n    self,\n    template_name: str,\n    customize: Optional[Dict] = None,\n) -&gt; \"Model\":\n    \"\"\"\n    Create a model from a pre-built template.\n\n    Args:\n        template_name: Name of template (e.g. 'simple_classifier')\n        customize: Optional customizations (not yet implemented)\n\n    Returns:\n        Model instance\n\n    Examples:\n        &gt;&gt;&gt; builder = ModelBuilder()\n        &gt;&gt;&gt; model = builder.from_template(\"simple_classifier\")\n        &gt;&gt;&gt; model.explain_architecture()\n    \"\"\"\n    if template_name not in self._templates:\n        available = \", \".join(self._templates.keys())\n        raise ValueError(\n            f\"Template '{template_name}' not found.\\n\"\n            f\"Available templates: {available}\\n\"\n            f\"Use builder.list_templates() to see details.\"\n        )\n\n    template = self._templates[template_name]\n\n    # Deep-copy so repeated calls don't mutate the template\n    layer_specs = copy.deepcopy(template[\"layers\"])\n\n    layers = []\n    for layer_spec in layer_specs:\n        layer_type = layer_spec.pop(\"type\")\n\n        if layer_type == \"dense\":\n            layers.append(self.Dense(**layer_spec))\n        elif layer_type == \"conv2d\":\n            layers.append(self.Conv2D(**layer_spec))\n        elif layer_type == \"dropout\":\n            layers.append(self.Dropout(**layer_spec))\n        elif layer_type == \"maxpool2d\":\n            layers.append(self.MaxPooling2D(**layer_spec))\n        elif layer_type == \"flatten\":\n            layers.append(self.Flatten())\n        elif layer_type == \"batchnorm\":\n            layers.append(self.BatchNorm())\n\n    model = self.Sequential(layers, name=template_name)\n    model.template_info = template\n\n    return model\n</code></pre>"},{"location":"api/reference/#neurogebra.builders.model_builder.ModelBuilder.list_templates","title":"<code>list_templates()</code>","text":"<p>Show all available model templates with descriptions.</p> Source code in <code>src/neurogebra/builders/model_builder.py</code> <pre><code>def list_templates(self):\n    \"\"\"Show all available model templates with descriptions.\"\"\"\n    print(\"\\n\ud83c\udfd7\ufe0f  Available Model Templates:\\n\")\n\n    for name, template in self._templates.items():\n        print(f\"  \ud83d\udce6 {name}\")\n        print(f\"     {template['description']}\")\n        print(f\"     Good for: {', '.join(template['good_for'])}\")\n        print()\n</code></pre>"},{"location":"api/reference/#neurogebra.builders.model_builder.ModelBuilder.suggest_architecture","title":"<code>suggest_architecture(task, input_shape, output_size)</code>","text":"<p>Get architecture suggestions based on your task.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>'classification', 'regression', 'image_classification', etc.</p> required <code>input_shape</code> <code>tuple</code> <p>Shape of your input data</p> required <code>output_size</code> <code>int</code> <p>Number of outputs</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; builder = ModelBuilder()\n&gt;&gt;&gt; builder.suggest_architecture(\n...     task=\"image_classification\",\n...     input_shape=(28, 28, 1),\n...     output_size=10\n... )\n</code></pre> Source code in <code>src/neurogebra/builders/model_builder.py</code> <pre><code>def suggest_architecture(\n    self,\n    task: str,\n    input_shape: tuple,\n    output_size: int,\n):\n    \"\"\"\n    Get architecture suggestions based on your task.\n\n    Args:\n        task: 'classification', 'regression', 'image_classification', etc.\n        input_shape: Shape of your input data\n        output_size: Number of outputs\n\n    Examples:\n        &gt;&gt;&gt; builder = ModelBuilder()\n        &gt;&gt;&gt; builder.suggest_architecture(\n        ...     task=\"image_classification\",\n        ...     input_shape=(28, 28, 1),\n        ...     output_size=10\n        ... )\n    \"\"\"\n    print(f\"\\n\ud83d\udca1 Architecture Suggestions for {task}:\\n\")\n\n    if task in (\"classification\", \"image_classification\"):\n        if len(input_shape) == 3:\n            print(\"   Recommended: Convolutional Neural Network (CNN)\")\n            print(\n                \"   Why: CNNs are designed to detect patterns in images\"\n            )\n            print(\"\\n   Suggested architecture:\")\n            print(\"   1. Conv2D(32, kernel_size=3) + ReLU\")\n            print(\"   2. MaxPooling2D()\")\n            print(\"   3. Conv2D(64, kernel_size=3) + ReLU\")\n            print(\"   4. MaxPooling2D()\")\n            print(\"   5. Flatten()\")\n            print(\"   6. Dense(128) + ReLU\")\n            print(\"   7. Dropout(0.3)\")\n            print(f\"   8. Dense({output_size}) + Softmax\")\n            print(\"\\n   Use: builder.from_template('image_classifier')\")\n        else:\n            print(\"   Recommended: Simple Feedforward Network\")\n            print(\"   Why: Efficient for structured/tabular data\")\n            print(\"\\n   Suggested architecture:\")\n            print(\"   1. Dense(128) + ReLU\")\n            print(\"   2. Dropout(0.2)\")\n            print(\"   3. Dense(64) + ReLU\")\n            print(f\"   4. Dense({output_size}) + Softmax\")\n            print(\n                \"\\n   Use: builder.from_template('simple_classifier')\"\n            )\n\n    elif task == \"regression\":\n        print(\"   Recommended: Regression Network\")\n        print(\"   Why: Predicts continuous values\")\n        print(\"\\n   Suggested architecture:\")\n        print(\"   1. Dense(64) + ReLU\")\n        print(\"   2. Dense(32) + ReLU\")\n        print(\"   3. Dense(1) + Linear (no activation)\")\n        print(\"\\n   Use: builder.from_template('regression')\")\n\n    elif task == \"binary_classification\":\n        print(\"   Recommended: Binary Classifier\")\n        print(\"   Why: Two-class output with sigmoid\")\n        print(\"\\n   Suggested architecture:\")\n        print(\"   1. Dense(64) + ReLU\")\n        print(\"   2. Dropout(0.3)\")\n        print(\"   3. Dense(32) + ReLU\")\n        print(\"   4. Dense(1) + Sigmoid\")\n        print(\n            \"\\n   Use: builder.from_template('binary_classifier')\"\n        )\n\n    print(\n        \"\\n   \ud83d\udca1 Tip: Start with these suggestions, then experiment!\"\n    )\n</code></pre>"},{"location":"api/reference/#repository","title":"Repository","text":"<p>Pre-built expression collections organized by category.</p>"},{"location":"api/reference/#activations","title":"Activations","text":""},{"location":"api/reference/#neurogebra.repository.activations.get_activations","title":"<code>neurogebra.repository.activations.get_activations()</code>","text":"<p>Get dictionary of activation function expressions.</p> <p>Returns:</p> Type Description <code>Dict[str, Expression]</code> <p>Dictionary mapping names to Expression instances</p> Source code in <code>src/neurogebra/repository/activations.py</code> <pre><code>def get_activations() -&gt; Dict[str, Expression]:\n    \"\"\"\n    Get dictionary of activation function expressions.\n\n    Returns:\n        Dictionary mapping names to Expression instances\n    \"\"\"\n    acts: Dict[str, Expression] = {}\n\n    # ReLU\n    acts[\"relu\"] = Expression(\n        name=\"relu\",\n        symbolic_expr=\"Max(0, x)\",\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"Rectified Linear Unit - outputs x if x &gt; 0, else 0\",\n            \"usage\": \"Most common activation for hidden layers\",\n            \"pros\": [\"Fast computation\", \"No vanishing gradient for positive values\"],\n            \"cons\": [\"Dead neurons for negative values\", \"Not zero-centered\"],\n        },\n    )\n\n    # Sigmoid\n    acts[\"sigmoid\"] = Expression(\n        name=\"sigmoid\",\n        symbolic_expr=\"1 / (1 + exp(-x))\",\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"Sigmoid function - maps input to (0, 1)\",\n            \"usage\": \"Binary classification output layer\",\n            \"pros\": [\"Smooth gradient\", \"Bounded output (0, 1)\"],\n            \"cons\": [\"Vanishing gradient\", \"Not zero-centered\", \"Expensive exp()\"],\n        },\n    )\n\n    # Tanh\n    acts[\"tanh\"] = Expression(\n        name=\"tanh\",\n        symbolic_expr=\"tanh(x)\",\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"Hyperbolic tangent - maps input to (-1, 1)\",\n            \"usage\": \"Hidden layers when zero-centered outputs preferred\",\n            \"pros\": [\"Zero-centered\", \"Smooth gradient\"],\n            \"cons\": [\"Vanishing gradient for large values\"],\n        },\n    )\n\n    # Leaky ReLU\n    acts[\"leaky_relu\"] = Expression(\n        name=\"leaky_relu\",\n        symbolic_expr=\"Max(alpha*x, x)\",\n        params={\"alpha\": 0.01},\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"ReLU with small slope for negative values\",\n            \"usage\": \"Alternative to ReLU to prevent dead neurons\",\n            \"pros\": [\"Prevents dead neurons\", \"Fast computation\"],\n            \"cons\": [\"Inconsistent predictions for negative values\"],\n        },\n    )\n\n    # Swish (SiLU)\n    acts[\"swish\"] = Expression(\n        name=\"swish\",\n        symbolic_expr=\"x / (1 + exp(-x))\",\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"Self-gated activation function (x * sigmoid(x))\",\n            \"usage\": \"Modern deep networks, often outperforms ReLU\",\n            \"pros\": [\"Smooth\", \"Non-monotonic\", \"Self-gating property\"],\n            \"cons\": [\"More expensive than ReLU\"],\n        },\n    )\n\n    # GELU\n    acts[\"gelu\"] = Expression(\n        name=\"gelu\",\n        symbolic_expr=\"0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715*x**3)))\",\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"Gaussian Error Linear Unit\",\n            \"usage\": \"Transformers, BERT, and GPT models\",\n            \"pros\": [\"Smooth\", \"Probabilistic interpretation\", \"State-of-the-art\"],\n            \"cons\": [\"Expensive computation\"],\n        },\n    )\n\n    # Softplus\n    acts[\"softplus\"] = Expression(\n        name=\"softplus\",\n        symbolic_expr=\"log(1 + exp(x))\",\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"Smooth approximation of ReLU\",\n            \"usage\": \"When a smooth differentiable version of ReLU is needed\",\n            \"pros\": [\"Smooth everywhere\", \"Always positive output\"],\n            \"cons\": [\"Slower than ReLU\"],\n        },\n    )\n\n    # ELU\n    acts[\"elu\"] = Expression(\n        name=\"elu\",\n        symbolic_expr=\"Piecewise((x, x &gt; 0), (alpha*(exp(x) - 1), True))\",\n        params={\"alpha\": 1.0},\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"Exponential Linear Unit\",\n            \"usage\": \"When negative outputs improve robustness\",\n            \"pros\": [\"Zero-centered for negative inputs\", \"Smooth\"],\n            \"cons\": [\"Expensive exp() for negative values\"],\n        },\n    )\n\n    # SELU (Scaled ELU)\n    acts[\"selu\"] = Expression(\n        name=\"selu\",\n        symbolic_expr=(\n            \"1.0507009873554805 * \"\n            \"Piecewise((x, x &gt; 0), \"\n            \"(1.6732632423543772*(exp(x) - 1), True))\"\n        ),\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"Scaled Exponential Linear Unit - self-normalizing\",\n            \"usage\": \"Self-normalizing neural networks (SNNs)\",\n            \"pros\": [\"Self-normalizing\", \"Avoids vanishing/exploding gradients\"],\n            \"cons\": [\"Only effective with specific architectures\"],\n        },\n    )\n\n    # Mish\n    acts[\"mish\"] = Expression(\n        name=\"mish\",\n        symbolic_expr=\"x * tanh(log(1 + exp(x)))\",\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"Mish activation - smooth, non-monotonic\",\n            \"usage\": \"Computer vision models, YOLOv4\",\n            \"pros\": [\"Smooth\", \"Non-monotonic\", \"Unbounded above\"],\n            \"cons\": [\"Computationally expensive\"],\n        },\n    )\n\n    # Hard Sigmoid\n    acts[\"hard_sigmoid\"] = Expression(\n        name=\"hard_sigmoid\",\n        symbolic_expr=\"Max(0, Min(1, (x + 3) / 6))\",\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"Piecewise linear approximation of sigmoid\",\n            \"usage\": \"Mobile/embedded models for efficiency\",\n            \"pros\": [\"Very fast\", \"Good approximation of sigmoid\"],\n            \"cons\": [\"Not smooth at transitions\"],\n        },\n    )\n\n    # Hard Swish\n    acts[\"hard_swish\"] = Expression(\n        name=\"hard_swish\",\n        symbolic_expr=\"x * Max(0, Min(1, (x + 3) / 6))\",\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"Efficient approximation of Swish\",\n            \"usage\": \"MobileNetV3 and efficient mobile architectures\",\n            \"pros\": [\"Fast\", \"Good approximation of Swish\"],\n            \"cons\": [\"Not smooth at transitions\"],\n        },\n    )\n\n    # Softsign\n    acts[\"softsign\"] = Expression(\n        name=\"softsign\",\n        symbolic_expr=\"x / (1 + Abs(x))\",\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"Maps input to (-1, 1), similar to tanh but lighter tails\",\n            \"usage\": \"Alternative to tanh for lighter-tail distribution\",\n            \"pros\": [\"Lighter tails than tanh\", \"Smooth\"],\n            \"cons\": [\"Slower convergence than tanh\"],\n        },\n    )\n\n    # Identity / Linear\n    acts[\"linear\"] = Expression(\n        name=\"linear\",\n        symbolic_expr=\"x\",\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"Identity/linear activation - no transformation\",\n            \"usage\": \"Output layer for regression tasks\",\n            \"pros\": [\"Simple\", \"No information loss\"],\n            \"cons\": [\"No nonlinearity\"],\n        },\n    )\n\n    # Square\n    acts[\"square\"] = Expression(\n        name=\"square\",\n        symbolic_expr=\"x**2\",\n        metadata={\n            \"category\": \"activation\",\n            \"description\": \"Square activation function\",\n            \"usage\": \"Polynomial networks, kernel approximation\",\n            \"pros\": [\"Simple nonlinearity\"],\n            \"cons\": [\"Unbounded\", \"Not monotonic\"],\n        },\n    )\n\n    return acts\n</code></pre>"},{"location":"api/reference/#losses","title":"Losses","text":""},{"location":"api/reference/#neurogebra.repository.losses.get_losses","title":"<code>neurogebra.repository.losses.get_losses()</code>","text":"<p>Get dictionary of loss function expressions.</p> <p>Returns:</p> Type Description <code>Dict[str, Expression]</code> <p>Dictionary mapping names to Expression instances</p> Source code in <code>src/neurogebra/repository/losses.py</code> <pre><code>def get_losses() -&gt; Dict[str, Expression]:\n    \"\"\"\n    Get dictionary of loss function expressions.\n\n    Returns:\n        Dictionary mapping names to Expression instances\n    \"\"\"\n    loss_fns: Dict[str, Expression] = {}\n\n    # Mean Squared Error\n    loss_fns[\"mse\"] = Expression(\n        name=\"mse\",\n        symbolic_expr=\"(y_pred - y_true)**2\",\n        metadata={\n            \"category\": \"loss\",\n            \"description\": \"Mean Squared Error - standard regression loss\",\n            \"usage\": \"Regression tasks\",\n            \"pros\": [\"Smooth and convex\", \"Well-understood gradients\"],\n            \"cons\": [\"Sensitive to outliers\"],\n        },\n    )\n\n    # Mean Absolute Error\n    loss_fns[\"mae\"] = Expression(\n        name=\"mae\",\n        symbolic_expr=\"Abs(y_pred - y_true)\",\n        metadata={\n            \"category\": \"loss\",\n            \"description\": \"Mean Absolute Error - robust regression loss\",\n            \"usage\": \"Regression with outliers\",\n            \"pros\": [\"Robust to outliers\", \"Interpretable\"],\n            \"cons\": [\"Not differentiable at zero\", \"Slower convergence\"],\n        },\n    )\n\n    # Binary Cross-Entropy\n    loss_fns[\"binary_crossentropy\"] = Expression(\n        name=\"binary_crossentropy\",\n        symbolic_expr=\"-y_true*log(y_pred) - (1-y_true)*log(1-y_pred)\",\n        metadata={\n            \"category\": \"loss\",\n            \"description\": \"Binary cross-entropy for binary classification\",\n            \"usage\": \"Binary classification with sigmoid output\",\n            \"pros\": [\"Probabilistic interpretation\", \"Strong gradients\"],\n            \"cons\": [\"Requires probability inputs (0, 1)\"],\n        },\n    )\n\n    # Huber Loss\n    loss_fns[\"huber\"] = Expression(\n        name=\"huber\",\n        symbolic_expr=(\n            \"Piecewise(\"\n            \"(0.5*(y_pred - y_true)**2, Abs(y_pred - y_true) &lt;= delta), \"\n            \"(delta*Abs(y_pred - y_true) - 0.5*delta**2, True)\"\n            \")\"\n        ),\n        params={\"delta\": 1.0},\n        metadata={\n            \"category\": \"loss\",\n            \"description\": \"Huber loss - MSE for small errors, MAE for large errors\",\n            \"usage\": \"Robust regression, reinforcement learning\",\n            \"pros\": [\"Best of MSE and MAE\", \"Differentiable everywhere\"],\n            \"cons\": [\"Extra hyperparameter (delta)\"],\n        },\n    )\n\n    # Log-Cosh Loss\n    loss_fns[\"log_cosh\"] = Expression(\n        name=\"log_cosh\",\n        symbolic_expr=\"log(cosh(y_pred - y_true))\",\n        metadata={\n            \"category\": \"loss\",\n            \"description\": \"Logarithm of hyperbolic cosine loss\",\n            \"usage\": \"Smooth alternative to Huber loss\",\n            \"pros\": [\"Smooth\", \"Approximately MSE for small errors\"],\n            \"cons\": [\"Less commonly used\"],\n        },\n    )\n\n    # Hinge Loss\n    loss_fns[\"hinge\"] = Expression(\n        name=\"hinge\",\n        symbolic_expr=\"Max(0, 1 - y_true * y_pred)\",\n        metadata={\n            \"category\": \"loss\",\n            \"description\": \"Hinge loss for SVM-style classification\",\n            \"usage\": \"Support vector machines, maximum margin classifiers\",\n            \"pros\": [\"Encourages margin\", \"Sparse solutions\"],\n            \"cons\": [\"Not differentiable at hinge point\"],\n        },\n    )\n\n    # Squared Hinge Loss\n    loss_fns[\"squared_hinge\"] = Expression(\n        name=\"squared_hinge\",\n        symbolic_expr=\"Max(0, 1 - y_true * y_pred)**2\",\n        metadata={\n            \"category\": \"loss\",\n            \"description\": \"Squared hinge loss - smoother version of hinge\",\n            \"usage\": \"Smooth SVM classification\",\n            \"pros\": [\"Smooth\", \"Differentiable\"],\n            \"cons\": [\"Penalizes outliers more\"],\n        },\n    )\n\n    # Quantile Loss\n    loss_fns[\"quantile\"] = Expression(\n        name=\"quantile\",\n        symbolic_expr=(\n            \"Piecewise(\"\n            \"(q * (y_true - y_pred), y_true &gt;= y_pred), \"\n            \"((1 - q) * (y_pred - y_true), True)\"\n            \")\"\n        ),\n        params={\"q\": 0.5},\n        metadata={\n            \"category\": \"loss\",\n            \"description\": \"Quantile loss for quantile regression\",\n            \"usage\": \"Predicting specific quantiles of distribution\",\n            \"pros\": [\"Asymmetric penalty\", \"Quantile estimation\"],\n            \"cons\": [\"Not differentiable at zero\"],\n        },\n    )\n\n    return loss_fns\n</code></pre>"},{"location":"api/reference/#regularizers","title":"Regularizers","text":""},{"location":"api/reference/#neurogebra.repository.regularizers.get_regularizers","title":"<code>neurogebra.repository.regularizers.get_regularizers()</code>","text":"<p>Get dictionary of regularization expressions.</p> <p>Returns:</p> Type Description <code>Dict[str, Expression]</code> <p>Dictionary mapping names to Expression instances</p> Source code in <code>src/neurogebra/repository/regularizers.py</code> <pre><code>def get_regularizers() -&gt; Dict[str, Expression]:\n    \"\"\"\n    Get dictionary of regularization expressions.\n\n    Returns:\n        Dictionary mapping names to Expression instances\n    \"\"\"\n    regs: Dict[str, Expression] = {}\n\n    # ----------------------------------------------------------------\n    # Classic Norm-Based Regularizers\n    # ----------------------------------------------------------------\n\n    regs[\"l1\"] = Expression(\n        name=\"l1\",\n        symbolic_expr=\"lambda_reg * Abs(w)\",\n        params={\"lambda_reg\": 0.01},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"L1 regularization (Lasso) - promotes sparsity\",\n            \"usage\": \"Feature selection, sparse models\",\n            \"pros\": [\"Promotes sparsity\", \"Feature selection\"],\n            \"cons\": [\"Not differentiable at zero\"],\n            \"formula_latex\": r\"\\lambda |w|\",\n        },\n    )\n\n    regs[\"l2\"] = Expression(\n        name=\"l2\",\n        symbolic_expr=\"lambda_reg * w**2\",\n        params={\"lambda_reg\": 0.01},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"L2 regularization (Ridge/Weight Decay)\",\n            \"usage\": \"Prevent overfitting, weight decay\",\n            \"pros\": [\"Smooth\", \"Well-understood\", \"Convex\"],\n            \"cons\": [\"Does not promote sparsity\"],\n            \"formula_latex\": r\"\\lambda w^2\",\n        },\n    )\n\n    regs[\"elastic_net\"] = Expression(\n        name=\"elastic_net\",\n        symbolic_expr=(\n            \"alpha * lambda_reg * Abs(w) \"\n            \"+ (1 - alpha) * lambda_reg * w**2\"\n        ),\n        params={\"lambda_reg\": 0.01, \"alpha\": 0.5},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Elastic Net - combination of L1 and L2\",\n            \"usage\": \"When both sparsity and grouping are desired\",\n            \"pros\": [\"Combines L1 and L2 benefits\"],\n            \"cons\": [\"Extra hyperparameter\"],\n        },\n    )\n\n    regs[\"weight_decay\"] = Expression(\n        name=\"weight_decay\",\n        symbolic_expr=\"0.5 * lambda_reg * w**2\",\n        params={\"lambda_reg\": 0.01},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Weight decay - direct weight shrinkage (\u00bd\u03bbw\u00b2)\",\n            \"usage\": \"AdamW optimizer, modern training\",\n            \"pros\": [\"Decoupled from loss\", \"Stable training\"],\n            \"cons\": [\"Equivalent to L2 for SGD\"],\n        },\n    )\n\n    regs[\"l1_l2\"] = Expression(\n        name=\"l1_l2\",\n        symbolic_expr=\"l1_reg * Abs(w) + l2_reg * w**2\",\n        params={\"l1_reg\": 0.01, \"l2_reg\": 0.01},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Independent L1 + L2 with separate coefficients\",\n            \"usage\": \"Fine-grained control over sparsity &amp; smoothness\",\n            \"pros\": [\"Independent tuning of each term\"],\n            \"cons\": [\"Two hyperparameters\"],\n        },\n    )\n\n    # ----------------------------------------------------------------\n    # Smooth Sparsity Regularizers\n    # ----------------------------------------------------------------\n\n    regs[\"log_barrier\"] = Expression(\n        name=\"log_barrier\",\n        symbolic_expr=\"-lambda_reg * log(1 - w**2 + epsilon)\",\n        params={\"lambda_reg\": 0.01, \"epsilon\": 1e-8},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Log-barrier penalty - keeps weights bounded\",\n            \"usage\": \"Constrained optimization, bounded weights\",\n            \"pros\": [\"Smooth\", \"Enforces weight bounds\"],\n            \"cons\": [\"Undefined outside bounds\"],\n        },\n    )\n\n    regs[\"sqrt_reg\"] = Expression(\n        name=\"sqrt_reg\",\n        symbolic_expr=\"lambda_reg * sqrt(w**2 + epsilon)\",\n        params={\"lambda_reg\": 0.01, \"epsilon\": 1e-8},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Smooth L1 approximation via \u221a(w\u00b2 + \u03b5)\",\n            \"usage\": \"Differentiable sparsity, compressed sensing\",\n            \"pros\": [\"Smooth everywhere\", \"Approximates L1\"],\n            \"cons\": [\"Less sparse than true L1\"],\n        },\n    )\n\n    regs[\"cauchy_reg\"] = Expression(\n        name=\"cauchy_reg\",\n        symbolic_expr=\"lambda_reg * log(1 + (w / sigma)**2)\",\n        params={\"lambda_reg\": 0.01, \"sigma\": 1.0},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Cauchy (Lorentzian) penalty - robust sparsity\",\n            \"usage\": \"Robust regression, outlier-tolerant sparsity\",\n            \"pros\": [\"Non-convex\", \"Strongly promotes sparsity\"],\n            \"cons\": [\"Non-convex, harder to optimize\"],\n        },\n    )\n\n    # ----------------------------------------------------------------\n    # Information-Theoretic Regularizers\n    # ----------------------------------------------------------------\n\n    regs[\"entropy_reg\"] = Expression(\n        name=\"entropy_reg\",\n        symbolic_expr=(\n            \"-lambda_reg * (p * log(p + epsilon) \"\n            \"+ (1 - p) * log(1 - p + epsilon))\"\n        ),\n        params={\"lambda_reg\": 0.01, \"epsilon\": 1e-8},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Entropy regularization - encourages confidence\",\n            \"usage\": \"Reinforcement learning, semi-supervised learning\",\n            \"pros\": [\"Encourages confident predictions\"],\n            \"cons\": [\"Can reduce exploration\"],\n        },\n    )\n\n    regs[\"kl_divergence_reg\"] = Expression(\n        name=\"kl_divergence_reg\",\n        symbolic_expr=(\n            \"lambda_reg * (p * log((p + epsilon)/(q + epsilon)) \"\n            \"+ (1 - p) * log((1 - p + epsilon)/(1 - q + epsilon)))\"\n        ),\n        params={\"lambda_reg\": 0.01, \"epsilon\": 1e-8},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"KL divergence penalty - keep distribution close to prior\",\n            \"usage\": \"Variational autoencoders (VAE), Bayesian learning\",\n            \"pros\": [\"Principled Bayesian regularization\"],\n            \"cons\": [\"Asymmetric\", \"Requires prior q\"],\n        },\n    )\n\n    # ----------------------------------------------------------------\n    # Gradient / Smoothness Regularizers\n    # ----------------------------------------------------------------\n\n    regs[\"gradient_penalty\"] = Expression(\n        name=\"gradient_penalty\",\n        symbolic_expr=\"lambda_reg * (grad_norm - 1)**2\",\n        params={\"lambda_reg\": 10.0},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Gradient penalty - enforce Lipschitz constraint\",\n            \"usage\": \"WGAN-GP, Lipschitz-constrained networks\",\n            \"pros\": [\"Stabilizes GAN training\", \"Enforces smoothness\"],\n            \"cons\": [\"Expensive to compute\"],\n        },\n    )\n\n    regs[\"total_variation\"] = Expression(\n        name=\"total_variation\",\n        symbolic_expr=\"lambda_reg * Abs(w_i - w_j)\",\n        params={\"lambda_reg\": 0.01},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Total variation - promote spatial smoothness\",\n            \"usage\": \"Image denoising, signal smoothing\",\n            \"pros\": [\"Preserves edges\", \"Removes noise\"],\n            \"cons\": [\"Staircase artifacts\"],\n        },\n    )\n\n    regs[\"tikhonov\"] = Expression(\n        name=\"tikhonov\",\n        symbolic_expr=\"lambda_reg * (w - w_prior)**2\",\n        params={\"lambda_reg\": 0.01, \"w_prior\": 0.0},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Tikhonov regularization - shrink toward prior\",\n            \"usage\": \"Ill-posed inverse problems, Bayesian MAP\",\n            \"pros\": [\"Principled\", \"Centers weights around prior\"],\n            \"cons\": [\"Requires choosing prior\"],\n        },\n    )\n\n    # ----------------------------------------------------------------\n    # Sparsity-Inducing Penalty Functions\n    # ----------------------------------------------------------------\n\n    regs[\"group_lasso\"] = Expression(\n        name=\"group_lasso\",\n        symbolic_expr=\"lambda_reg * sqrt(w1**2 + w2**2 + epsilon)\",\n        params={\"lambda_reg\": 0.01, \"epsilon\": 1e-8},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Group Lasso - sets entire feature groups to zero\",\n            \"usage\": \"Feature group selection, structured sparsity\",\n            \"pros\": [\"Structured sparsity\", \"Group selection\"],\n            \"cons\": [\"Requires grouping definition\"],\n        },\n    )\n\n    regs[\"scad\"] = Expression(\n        name=\"scad\",\n        symbolic_expr=(\n            \"Piecewise(\"\n            \"(lambda_reg * Abs(w), Abs(w) &lt;= lambda_reg), \"\n            \"(-(w**2 - 2*a*lambda_reg*Abs(w) + lambda_reg**2) \"\n            \"/ (2*(a - 1)), Abs(w) &lt;= a*lambda_reg), \"\n            \"(lambda_reg**2 * (a + 1) / 2, True)\"\n            \")\"\n        ),\n        params={\"lambda_reg\": 0.01, \"a\": 3.7},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"SCAD penalty - Smoothly Clipped Absolute Deviation\",\n            \"usage\": \"Variable selection, unbiased estimation\",\n            \"pros\": [\"Unbiased for large weights\", \"Oracle property\"],\n            \"cons\": [\"Non-convex\", \"Extra hyperparameter a\"],\n        },\n    )\n\n    regs[\"mcp\"] = Expression(\n        name=\"mcp\",\n        symbolic_expr=(\n            \"Piecewise(\"\n            \"(lambda_reg * Abs(w) - w**2 / (2*gamma_mcp), \"\n            \"Abs(w) &lt;= gamma_mcp*lambda_reg), \"\n            \"(gamma_mcp*lambda_reg**2 / 2, True)\"\n            \")\"\n        ),\n        params={\"lambda_reg\": 0.01, \"gamma_mcp\": 3.0},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Minimax Concave Penalty (MCP)\",\n            \"usage\": \"High-dimensional variable selection\",\n            \"pros\": [\"Nearly unbiased\", \"Sparser than Lasso\"],\n            \"cons\": [\"Non-convex\", \"Harder optimization\"],\n        },\n    )\n\n    # ----------------------------------------------------------------\n    # Other Useful Regularizers\n    # ----------------------------------------------------------------\n\n    regs[\"max_norm\"] = Expression(\n        name=\"max_norm\",\n        symbolic_expr=\"Max(0, w**2 - max_val**2)\",\n        params={\"max_val\": 3.0},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Max-norm constraint penalty\",\n            \"usage\": \"Dropout companion, bounded activations\",\n            \"pros\": [\"Bounds weight magnitude\", \"Works well with dropout\"],\n            \"cons\": [\"Non-smooth at boundary\"],\n        },\n    )\n\n    regs[\"orthogonal_reg\"] = Expression(\n        name=\"orthogonal_reg\",\n        symbolic_expr=\"lambda_reg * (w1*w2)**2\",\n        params={\"lambda_reg\": 0.01},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Orthogonal regularization - encourage decorrelated weights\",\n            \"usage\": \"Improving gradient flow, preventing mode collapse\",\n            \"pros\": [\"Better gradient flow\", \"Decorrelated features\"],\n            \"cons\": [\"Pairwise computation O(n\u00b2)\"],\n        },\n    )\n\n    regs[\"label_smoothing\"] = Expression(\n        name=\"label_smoothing\",\n        symbolic_expr=\"(1 - epsilon_smooth) * y + epsilon_smooth / K\",\n        params={\"epsilon_smooth\": 0.1, \"K\": 10},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Label smoothing - soften hard targets\",\n            \"usage\": \"Classification, knowledge distillation\",\n            \"pros\": [\"Prevents overconfidence\", \"Improves calibration\"],\n            \"cons\": [\"Slightly lower peak accuracy\"],\n        },\n    )\n\n    regs[\"confidence_penalty\"] = Expression(\n        name=\"confidence_penalty\",\n        symbolic_expr=\"-lambda_reg * p * log(p + epsilon)\",\n        params={\"lambda_reg\": 0.1, \"epsilon\": 1e-8},\n        metadata={\n            \"category\": \"regularizer\",\n            \"description\": \"Confidence penalty - discourages overconfident outputs\",\n            \"usage\": \"Well-calibrated models, uncertainty estimation\",\n            \"pros\": [\"Better calibrated probabilities\"],\n            \"cons\": [\"Slightly lower accuracy\"],\n        },\n    )\n\n    return regs\n</code></pre>"},{"location":"api/reference/#algebra","title":"Algebra","text":""},{"location":"api/reference/#neurogebra.repository.algebra.get_algebra_expressions","title":"<code>neurogebra.repository.algebra.get_algebra_expressions()</code>","text":"<p>Get dictionary of algebraic expressions.</p> <p>Returns:</p> Type Description <code>Dict[str, Expression]</code> <p>Dictionary mapping names to Expression instances</p> Source code in <code>src/neurogebra/repository/algebra.py</code> <pre><code>def get_algebra_expressions() -&gt; Dict[str, Expression]:\n    \"\"\"\n    Get dictionary of algebraic expressions.\n\n    Returns:\n        Dictionary mapping names to Expression instances\n    \"\"\"\n    algebra: Dict[str, Expression] = {}\n\n    # ================================================================\n    # Polynomial Functions\n    # ================================================================\n\n    algebra[\"linear_eq\"] = Expression(\n        name=\"linear_eq\",\n        symbolic_expr=\"m*x + b\",\n        params={\"m\": 1, \"b\": 0},\n        trainable_params=[\"m\", \"b\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Linear equation: y = mx + b\",\n            \"usage\": \"Linear regression, line fitting\",\n        },\n    )\n\n    algebra[\"quadratic\"] = Expression(\n        name=\"quadratic\",\n        symbolic_expr=\"a*x**2 + b*x + c\",\n        params={\"a\": 1, \"b\": 0, \"c\": 0},\n        trainable_params=[\"a\", \"b\", \"c\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Quadratic polynomial: ax\u00b2 + bx + c\",\n            \"usage\": \"Curve fitting, parabolic approximation\",\n        },\n    )\n\n    algebra[\"cubic\"] = Expression(\n        name=\"cubic\",\n        symbolic_expr=\"a*x**3 + b*x**2 + c*x + d\",\n        params={\"a\": 1, \"b\": 0, \"c\": 0, \"d\": 0},\n        trainable_params=[\"a\", \"b\", \"c\", \"d\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Cubic polynomial: ax\u00b3 + bx\u00b2 + cx + d\",\n            \"usage\": \"Curve fitting, interpolation\",\n        },\n    )\n\n    algebra[\"quartic\"] = Expression(\n        name=\"quartic\",\n        symbolic_expr=\"a*x**4 + b*x**3 + c*x**2 + d*x + e\",\n        params={\"a\": 1, \"b\": 0, \"c\": 0, \"d\": 0, \"e\": 0},\n        trainable_params=[\"a\", \"b\", \"c\", \"d\", \"e\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Quartic polynomial: ax\u2074 + bx\u00b3 + cx\u00b2 + dx + e\",\n            \"usage\": \"Higher-order curve fitting\",\n        },\n    )\n\n    algebra[\"monomial\"] = Expression(\n        name=\"monomial\",\n        symbolic_expr=\"a * x**n\",\n        params={\"a\": 1.0, \"n\": 2.0},\n        trainable_params=[\"a\", \"n\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Monomial: a\u00b7x\u207f, single-term power expression\",\n            \"usage\": \"Basis for polynomial approximation\",\n        },\n    )\n\n    # ================================================================\n    # Power &amp; Root Functions\n    # ================================================================\n\n    algebra[\"power_law\"] = Expression(\n        name=\"power_law\",\n        symbolic_expr=\"a * x**n\",\n        params={\"a\": 1.0, \"n\": 2.0},\n        trainable_params=[\"a\", \"n\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Power law: a\u00b7x\u207f\",\n            \"usage\": \"Scaling laws, Zipf's law, allometry\",\n        },\n    )\n\n    algebra[\"inverse_square\"] = Expression(\n        name=\"inverse_square\",\n        symbolic_expr=\"a / (x**2 + epsilon)\",\n        params={\"a\": 1.0, \"epsilon\": 1e-8},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Inverse square law: a/x\u00b2\",\n            \"usage\": \"Gravity, electric fields, light intensity\",\n        },\n    )\n\n    algebra[\"square_root\"] = Expression(\n        name=\"square_root\",\n        symbolic_expr=\"a * sqrt(Abs(x) + epsilon)\",\n        params={\"a\": 1.0, \"epsilon\": 1e-8},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Square root function: a\u00b7\u221a|x|\",\n            \"usage\": \"Sublinear growth, diminishing returns\",\n        },\n    )\n\n    algebra[\"nth_root\"] = Expression(\n        name=\"nth_root\",\n        symbolic_expr=\"(Abs(x) + epsilon)**(1/n)\",\n        params={\"n\": 3.0, \"epsilon\": 1e-8},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Nth root: x^(1/n)\",\n            \"usage\": \"Root extraction, power normalization\",\n        },\n    )\n\n    # ================================================================\n    # Exponential &amp; Logarithmic Functions\n    # ================================================================\n\n    algebra[\"exp_decay\"] = Expression(\n        name=\"exp_decay\",\n        symbolic_expr=\"A * exp(-k * x)\",\n        params={\"A\": 1.0, \"k\": 1.0},\n        trainable_params=[\"A\", \"k\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Exponential decay: A\u00b7e^(\u2212kx)\",\n            \"usage\": \"Radioactive decay, learning rate schedules\",\n        },\n    )\n\n    algebra[\"exp_growth\"] = Expression(\n        name=\"exp_growth\",\n        symbolic_expr=\"A * exp(k * x)\",\n        params={\"A\": 1.0, \"k\": 0.1},\n        trainable_params=[\"A\", \"k\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Exponential growth: A\u00b7e^(kx)\",\n            \"usage\": \"Population growth, compound interest\",\n        },\n    )\n\n    algebra[\"double_exponential\"] = Expression(\n        name=\"double_exponential\",\n        symbolic_expr=\"A * exp(-Abs(x - mu) / b)\",\n        params={\"A\": 1.0, \"mu\": 0.0, \"b\": 1.0},\n        trainable_params=[\"A\", \"mu\", \"b\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Laplace distribution / double exponential\",\n            \"usage\": \"Robust statistics, L1 regularization prior\",\n        },\n    )\n\n    algebra[\"logarithmic\"] = Expression(\n        name=\"logarithmic\",\n        symbolic_expr=\"a * log(x + epsilon) + b\",\n        params={\"a\": 1.0, \"b\": 0.0, \"epsilon\": 1e-8},\n        trainable_params=[\"a\", \"b\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Logarithmic function: a\u00b7ln(x) + b\",\n            \"usage\": \"Diminishing returns, Weber-Fechner law\",\n        },\n    )\n\n    algebra[\"log_linear\"] = Expression(\n        name=\"log_linear\",\n        symbolic_expr=\"a * log(1 + exp(b * x))\",\n        params={\"a\": 1.0, \"b\": 1.0},\n        trainable_params=[\"a\", \"b\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Log-linear (softplus family): a\u00b7log(1+e^(bx))\",\n            \"usage\": \"Smooth ramp functions, positive outputs\",\n        },\n    )\n\n    # ================================================================\n    # Sigmoid &amp; Growth Curves\n    # ================================================================\n\n    algebra[\"logistic\"] = Expression(\n        name=\"logistic\",\n        symbolic_expr=\"L / (1 + exp(-k * (x - x0)))\",\n        params={\"L\": 1.0, \"k\": 1.0, \"x0\": 0.0},\n        trainable_params=[\"L\", \"k\", \"x0\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Logistic growth curve (S-curve)\",\n            \"usage\": \"Population growth, adoption curves\",\n        },\n    )\n\n    algebra[\"gompertz\"] = Expression(\n        name=\"gompertz\",\n        symbolic_expr=\"a * exp(-b * exp(-c * x))\",\n        params={\"a\": 1.0, \"b\": 1.0, \"c\": 1.0},\n        trainable_params=[\"a\", \"b\", \"c\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Gompertz curve - asymmetric sigmoid growth\",\n            \"usage\": \"Tumor growth, product adoption, mortality\",\n        },\n    )\n\n    algebra[\"richards_curve\"] = Expression(\n        name=\"richards_curve\",\n        symbolic_expr=\"K / (1 + exp(-r * (x - x0)))**(1/nu)\",\n        params={\"K\": 1.0, \"r\": 1.0, \"x0\": 0.0, \"nu\": 1.0},\n        trainable_params=[\"K\", \"r\", \"x0\", \"nu\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Richards / generalized logistic curve\",\n            \"usage\": \"Flexible growth modeling, epidemiology\",\n        },\n    )\n\n    algebra[\"probit\"] = Expression(\n        name=\"probit\",\n        symbolic_expr=\"0.5 * (1 + erf(x / sqrt(2)))\",\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Probit function (Gaussian CDF \u03a6(x))\",\n            \"usage\": \"Probit regression, dose-response curves\",\n        },\n    )\n\n    algebra[\"hill_equation\"] = Expression(\n        name=\"hill_equation\",\n        symbolic_expr=\"V_max * x**n / (K**n + x**n)\",\n        params={\"V_max\": 1.0, \"K\": 0.5, \"n\": 2.0},\n        trainable_params=[\"V_max\", \"K\", \"n\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Hill equation - cooperative binding sigmoid\",\n            \"usage\": \"Biochemistry, pharmacology, dose-response\",\n        },\n    )\n\n    algebra[\"michaelis_menten\"] = Expression(\n        name=\"michaelis_menten\",\n        symbolic_expr=\"V_max * x / (K_m + x)\",\n        params={\"V_max\": 1.0, \"K_m\": 0.5},\n        trainable_params=[\"V_max\", \"K_m\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Michaelis-Menten enzyme kinetics\",\n            \"usage\": \"Enzyme kinetics, saturation modeling\",\n        },\n    )\n\n    # ================================================================\n    # Probability Distribution Functions\n    # ================================================================\n\n    algebra[\"gaussian\"] = Expression(\n        name=\"gaussian\",\n        symbolic_expr=\"A * exp(-(x - mu)**2 / (2 * sigma**2))\",\n        params={\"A\": 1.0, \"mu\": 0.0, \"sigma\": 1.0},\n        trainable_params=[\"A\", \"mu\", \"sigma\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Gaussian (normal) distribution bell curve\",\n            \"usage\": \"Probability density, Gaussian processes, RBF\",\n        },\n    )\n\n    algebra[\"cauchy_distribution\"] = Expression(\n        name=\"cauchy_distribution\",\n        symbolic_expr=\"1 / (pi * gamma_param * (1 + ((x - x0) / gamma_param)**2))\",\n        params={\"x0\": 0.0, \"gamma_param\": 1.0},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Cauchy (Lorentzian) distribution - heavy tails\",\n            \"usage\": \"Robust statistics, spectral line shapes\",\n        },\n    )\n\n    algebra[\"student_t\"] = Expression(\n        name=\"student_t\",\n        symbolic_expr=(\n            \"(1 + x**2/nu)**(-(nu + 1)/2) \"\n            \"* gamma_func((nu + 1)/2) \"\n            \"/ (sqrt(nu * pi) * gamma_func(nu/2))\"\n        ),\n        params={\"nu\": 5.0},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Student's t-distribution kernel (unnormalized)\",\n            \"usage\": \"Small sample inference, robust regression\",\n        },\n    )\n\n    algebra[\"rayleigh\"] = Expression(\n        name=\"rayleigh\",\n        symbolic_expr=\"(x / sigma**2) * exp(-x**2 / (2 * sigma**2))\",\n        params={\"sigma\": 1.0},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Rayleigh distribution density\",\n            \"usage\": \"Signal processing, wind speed modeling\",\n        },\n    )\n\n    algebra[\"laplace_distribution\"] = Expression(\n        name=\"laplace_distribution\",\n        symbolic_expr=\"(1/(2*b)) * exp(-Abs(x - mu)/b)\",\n        params={\"mu\": 0.0, \"b\": 1.0},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Laplace distribution density\",\n            \"usage\": \"Sparse priors, L1 loss connection, robustness\",\n        },\n    )\n\n    algebra[\"beta_distribution\"] = Expression(\n        name=\"beta_distribution\",\n        symbolic_expr=\"x**(alpha - 1) * (1 - x)**(beta_param - 1)\",\n        params={\"alpha\": 2.0, \"beta_param\": 5.0},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Beta distribution kernel (unnormalized)\",\n            \"usage\": \"Probability on [0,1], Bayesian conjugate prior\",\n        },\n    )\n\n    # ================================================================\n    # Trigonometric &amp; Periodic Functions\n    # ================================================================\n\n    algebra[\"sinusoidal\"] = Expression(\n        name=\"sinusoidal\",\n        symbolic_expr=\"A * sin(omega * x + phi)\",\n        params={\"A\": 1.0, \"omega\": 1.0, \"phi\": 0.0},\n        trainable_params=[\"A\", \"omega\", \"phi\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Sinusoidal wave: A\u00b7sin(\u03c9x + \u03c6)\",\n            \"usage\": \"Signal processing, periodic data fitting\",\n        },\n    )\n\n    algebra[\"damped_oscillation\"] = Expression(\n        name=\"damped_oscillation\",\n        symbolic_expr=\"A * exp(-gamma_d * x) * cos(omega * x + phi)\",\n        params={\"A\": 1.0, \"gamma_d\": 0.1, \"omega\": 1.0, \"phi\": 0.0},\n        trainable_params=[\"A\", \"gamma_d\", \"omega\", \"phi\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Damped oscillation: A\u00b7e^(-\u03b3x)\u00b7cos(\u03c9x+\u03c6)\",\n            \"usage\": \"Spring-mass systems, RLC circuits, decay\",\n        },\n    )\n\n    algebra[\"fourier_term\"] = Expression(\n        name=\"fourier_term\",\n        symbolic_expr=\"a0 + a1*cos(x) + b1*sin(x) + a2*cos(2*x) + b2*sin(2*x)\",\n        params={\"a0\": 0.0, \"a1\": 1.0, \"b1\": 0.0, \"a2\": 0.0, \"b2\": 0.0},\n        trainable_params=[\"a0\", \"a1\", \"b1\", \"a2\", \"b2\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Fourier series (2 terms): a\u2080 + \u03a3(a\u2099cos(nx) + b\u2099sin(nx))\",\n            \"usage\": \"Periodic function approximation, spectral analysis\",\n        },\n    )\n\n    algebra[\"sawtooth_approx\"] = Expression(\n        name=\"sawtooth_approx\",\n        symbolic_expr=(\n            \"0.5 - (1/pi) * (sin(x) + sin(2*x)/2 + sin(3*x)/3)\"\n        ),\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Sawtooth wave (Fourier approx, 3 terms)\",\n            \"usage\": \"Signal processing, waveform generation\",\n        },\n    )\n\n    algebra[\"square_wave_approx\"] = Expression(\n        name=\"square_wave_approx\",\n        symbolic_expr=(\n            \"(4/pi) * (sin(x) + sin(3*x)/3 + sin(5*x)/5)\"\n        ),\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Square wave (Fourier approx, 3 terms)\",\n            \"usage\": \"Digital signals, pulse train approximation\",\n        },\n    )\n\n    # ================================================================\n    # Kernel Functions (ML / Gaussian Processes)\n    # ================================================================\n\n    algebra[\"rbf_kernel\"] = Expression(\n        name=\"rbf_kernel\",\n        symbolic_expr=\"exp(-gamma_k * (x - y)**2)\",\n        params={\"gamma_k\": 1.0},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"RBF / Gaussian kernel: exp(\u2212\u03b3(x\u2212y)\u00b2)\",\n            \"usage\": \"SVM, Gaussian processes, kernel methods\",\n        },\n    )\n\n    algebra[\"polynomial_kernel\"] = Expression(\n        name=\"polynomial_kernel\",\n        symbolic_expr=\"(alpha_k * x * y + c_k)**d_k\",\n        params={\"alpha_k\": 1.0, \"c_k\": 1.0, \"d_k\": 3.0},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Polynomial kernel: (\u03b1xy + c)^d\",\n            \"usage\": \"SVM, polynomial feature maps\",\n        },\n    )\n\n    algebra[\"laplacian_kernel\"] = Expression(\n        name=\"laplacian_kernel\",\n        symbolic_expr=\"exp(-gamma_k * Abs(x - y))\",\n        params={\"gamma_k\": 1.0},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Laplacian kernel: exp(\u2212\u03b3|x\u2212y|)\",\n            \"usage\": \"SVM, non-smooth data, Gaussian processes\",\n        },\n    )\n\n    algebra[\"rational_quadratic_kernel\"] = Expression(\n        name=\"rational_quadratic_kernel\",\n        symbolic_expr=\"(1 + (x - y)**2 / (2*alpha_rq * length_scale**2))**(-alpha_rq)\",\n        params={\"alpha_rq\": 1.0, \"length_scale\": 1.0},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Rational quadratic kernel (infinite RBF mixture)\",\n            \"usage\": \"Gaussian processes, multi-scale modeling\",\n        },\n    )\n\n    algebra[\"matern_12_kernel\"] = Expression(\n        name=\"matern_12_kernel\",\n        symbolic_expr=\"sigma_k**2 * exp(-Abs(x - y) / length_scale)\",\n        params={\"sigma_k\": 1.0, \"length_scale\": 1.0},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Mat\u00e9rn kernel (\u03bd=1/2) - equivalent to Laplacian\",\n            \"usage\": \"Gaussian processes for rough functions\",\n        },\n    )\n\n    algebra[\"matern_32_kernel\"] = Expression(\n        name=\"matern_32_kernel\",\n        symbolic_expr=(\n            \"sigma_k**2 * (1 + sqrt(3)*Abs(x - y)/length_scale) \"\n            \"* exp(-sqrt(3)*Abs(x - y)/length_scale)\"\n        ),\n        params={\"sigma_k\": 1.0, \"length_scale\": 1.0},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Mat\u00e9rn kernel (\u03bd=3/2) - once differentiable\",\n            \"usage\": \"Gaussian processes, moderate smoothness\",\n        },\n    )\n\n    algebra[\"periodic_kernel\"] = Expression(\n        name=\"periodic_kernel\",\n        symbolic_expr=\"sigma_k**2 * exp(-2 * sin(pi * Abs(x - y) / period)**2 / length_scale**2)\",\n        params={\"sigma_k\": 1.0, \"length_scale\": 1.0, \"period\": 1.0},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Periodic kernel for repeating patterns\",\n            \"usage\": \"Gaussian processes on periodic data\",\n        },\n    )\n\n    # ================================================================\n    # Rational Functions\n    # ================================================================\n\n    algebra[\"rational\"] = Expression(\n        name=\"rational\",\n        symbolic_expr=\"(a*x + b) / (c*x + d + epsilon)\",\n        params={\"a\": 1.0, \"b\": 0.0, \"c\": 0.0, \"d\": 1.0, \"epsilon\": 1e-8},\n        trainable_params=[\"a\", \"b\", \"c\", \"d\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Rational function (ax+b)/(cx+d)\",\n            \"usage\": \"Pad\u00e9 approximation, M\u00f6bius transforms\",\n        },\n    )\n\n    algebra[\"lorentzian\"] = Expression(\n        name=\"lorentzian\",\n        symbolic_expr=\"A / (1 + ((x - x0)/gamma_l)**2)\",\n        params={\"A\": 1.0, \"x0\": 0.0, \"gamma_l\": 1.0},\n        trainable_params=[\"A\", \"x0\", \"gamma_l\"],\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Lorentzian / Cauchy peak function\",\n            \"usage\": \"Spectral line fitting, resonance curves\",\n        },\n    )\n\n    # ================================================================\n    # Special Functions &amp; Other\n    # ================================================================\n\n    algebra[\"heaviside\"] = Expression(\n        name=\"heaviside\",\n        symbolic_expr=\"Piecewise((0, x &lt; 0), (1, True))\",\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Heaviside step function H(x)\",\n            \"usage\": \"Signal processing, threshold operations\",\n        },\n    )\n\n    algebra[\"ramp\"] = Expression(\n        name=\"ramp\",\n        symbolic_expr=\"Max(0, x)\",\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Ramp function (same as ReLU)\",\n            \"usage\": \"Piecewise linear modeling, half-wave rectifier\",\n        },\n    )\n\n    algebra[\"absolute_value\"] = Expression(\n        name=\"absolute_value\",\n        symbolic_expr=\"Abs(x)\",\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Absolute value |x|\",\n            \"usage\": \"Distance, L1 norm, error magnitude\",\n        },\n    )\n\n    algebra[\"sign_function\"] = Expression(\n        name=\"sign_function\",\n        symbolic_expr=\"Piecewise((-1, x &lt; 0), (0, Eq(x, 0)), (1, True))\",\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Sign / signum function: sgn(x)\",\n            \"usage\": \"Direction indicator, binary quantization\",\n        },\n    )\n\n    algebra[\"clamp\"] = Expression(\n        name=\"clamp\",\n        symbolic_expr=\"Max(low, Min(high, x))\",\n        params={\"low\": 0.0, \"high\": 1.0},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Clamp function: clip x to [low, high]\",\n            \"usage\": \"Gradient clipping, bounded outputs\",\n        },\n    )\n\n    algebra[\"lerp\"] = Expression(\n        name=\"lerp\",\n        symbolic_expr=\"a_val + t * (b_val - a_val)\",\n        params={\"a_val\": 0.0, \"b_val\": 1.0},\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Linear interpolation: a + t(b \u2212 a)\",\n            \"usage\": \"Animation, EMA, parameter mixing\",\n        },\n    )\n\n    algebra[\"smoothstep\"] = Expression(\n        name=\"smoothstep\",\n        symbolic_expr=\"3*t**2 - 2*t**3\",\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Smoothstep (Hermite interpolation): 3t\u00b2 \u2212 2t\u00b3\",\n            \"usage\": \"Smooth transitions, animation easing\",\n        },\n    )\n\n    algebra[\"smootherstep\"] = Expression(\n        name=\"smootherstep\",\n        symbolic_expr=\"6*t**5 - 15*t**4 + 10*t**3\",\n        metadata={\n            \"category\": \"algebra\",\n            \"description\": \"Smootherstep (Ken Perlin): 6t\u2075 \u2212 15t\u2074 + 10t\u00b3\",\n            \"usage\": \"Perlin noise, ultra-smooth transitions\",\n        },\n    )\n\n    return algebra\n</code></pre>"},{"location":"api/reference/#calculus","title":"Calculus","text":""},{"location":"api/reference/#neurogebra.repository.calculus.get_calculus_expressions","title":"<code>neurogebra.repository.calculus.get_calculus_expressions()</code>","text":"<p>Get dictionary of common calculus expressions.</p> <p>Returns:</p> Type Description <code>Dict[str, Expression]</code> <p>Dictionary mapping names to Expression instances</p> Source code in <code>src/neurogebra/repository/calculus.py</code> <pre><code>def get_calculus_expressions() -&gt; Dict[str, Expression]:\n    \"\"\"\n    Get dictionary of common calculus expressions.\n\n    Returns:\n        Dictionary mapping names to Expression instances\n    \"\"\"\n    calc: Dict[str, Expression] = {}\n\n    # ================================================================\n    # Elementary Functions\n    # ================================================================\n\n    calc[\"exp\"] = Expression(\n        name=\"exp\",\n        symbolic_expr=\"exp(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Natural exponential function e^x\",\n            \"usage\": \"Growth models, probability distributions\",\n            \"derivative\": \"exp(x)\",\n            \"integral\": \"exp(x)\",\n        },\n    )\n\n    calc[\"ln\"] = Expression(\n        name=\"ln\",\n        symbolic_expr=\"log(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Natural logarithm ln(x)\",\n            \"usage\": \"Log-scale transformations, information theory\",\n            \"derivative\": \"1/x\",\n            \"integral\": \"x*ln(x) - x\",\n        },\n    )\n\n    calc[\"log2\"] = Expression(\n        name=\"log2\",\n        symbolic_expr=\"log(x) / log(2)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Base-2 logarithm log\u2082(x)\",\n            \"usage\": \"Information theory, bits, binary entropy\",\n        },\n    )\n\n    calc[\"log10\"] = Expression(\n        name=\"log10\",\n        symbolic_expr=\"log(x) / log(10)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Base-10 (common) logarithm log\u2081\u2080(x)\",\n            \"usage\": \"Decibels, pH scale, order of magnitude\",\n        },\n    )\n\n    calc[\"reciprocal\"] = Expression(\n        name=\"reciprocal\",\n        symbolic_expr=\"1 / x\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Reciprocal function 1/x\",\n            \"usage\": \"Inverse transformations, harmonic series\",\n            \"derivative\": \"-1/x\u00b2\",\n        },\n    )\n\n    calc[\"sqrt\"] = Expression(\n        name=\"sqrt\",\n        symbolic_expr=\"sqrt(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Square root function \u221ax\",\n            \"usage\": \"Normalization, distance computation\",\n            \"derivative\": \"1/(2\u221ax)\",\n        },\n    )\n\n    calc[\"cbrt\"] = Expression(\n        name=\"cbrt\",\n        symbolic_expr=\"x**(Rational(1, 3))\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Cube root function x^(1/3)\",\n            \"usage\": \"Volume extraction, cubic equations\",\n        },\n    )\n\n    calc[\"abs_val\"] = Expression(\n        name=\"abs_val\",\n        symbolic_expr=\"Abs(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Absolute value |x|\",\n            \"usage\": \"Distance, L1 norm, magnitude\",\n        },\n    )\n\n    # ================================================================\n    # Trigonometric Functions\n    # ================================================================\n\n    calc[\"sin\"] = Expression(\n        name=\"sin\",\n        symbolic_expr=\"sin(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Sine function\",\n            \"usage\": \"Trigonometric operations, signal processing\",\n            \"derivative\": \"cos(x)\",\n            \"integral\": \"-cos(x)\",\n        },\n    )\n\n    calc[\"cos\"] = Expression(\n        name=\"cos\",\n        symbolic_expr=\"cos(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Cosine function\",\n            \"usage\": \"Trigonometric operations, positional encoding\",\n            \"derivative\": \"-sin(x)\",\n            \"integral\": \"sin(x)\",\n        },\n    )\n\n    calc[\"tan\"] = Expression(\n        name=\"tan\",\n        symbolic_expr=\"tan(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Tangent function tan(x) = sin(x)/cos(x)\",\n            \"usage\": \"Trigonometry, angle computation\",\n            \"derivative\": \"sec\u00b2(x) = 1 + tan\u00b2(x)\",\n        },\n    )\n\n    calc[\"sec\"] = Expression(\n        name=\"sec\",\n        symbolic_expr=\"1 / cos(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Secant function sec(x) = 1/cos(x)\",\n            \"usage\": \"Advanced trigonometry, integration techniques\",\n        },\n    )\n\n    calc[\"csc\"] = Expression(\n        name=\"csc\",\n        symbolic_expr=\"1 / sin(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Cosecant function csc(x) = 1/sin(x)\",\n            \"usage\": \"Advanced trigonometry\",\n        },\n    )\n\n    calc[\"cot\"] = Expression(\n        name=\"cot\",\n        symbolic_expr=\"cos(x) / sin(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Cotangent function cot(x) = cos(x)/sin(x)\",\n            \"usage\": \"Advanced trigonometry\",\n        },\n    )\n\n    # ================================================================\n    # Inverse Trigonometric Functions\n    # ================================================================\n\n    calc[\"arcsin\"] = Expression(\n        name=\"arcsin\",\n        symbolic_expr=\"asin(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Inverse sine (arcsine) function\",\n            \"usage\": \"Angle recovery, trigonometric inversion\",\n            \"derivative\": \"1/\u221a(1 \u2212 x\u00b2)\",\n        },\n    )\n\n    calc[\"arccos\"] = Expression(\n        name=\"arccos\",\n        symbolic_expr=\"acos(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Inverse cosine (arccosine) function\",\n            \"usage\": \"Angle computation from dot products\",\n            \"derivative\": \"\u22121/\u221a(1 \u2212 x\u00b2)\",\n        },\n    )\n\n    calc[\"arctan\"] = Expression(\n        name=\"arctan\",\n        symbolic_expr=\"atan(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Inverse tangent (arctangent) function\",\n            \"usage\": \"Angle computation, smooth saturation\",\n            \"derivative\": \"1/(1 + x\u00b2)\",\n        },\n    )\n\n    calc[\"arctan2\"] = Expression(\n        name=\"arctan2\",\n        symbolic_expr=\"atan2(y, x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Two-argument arctangent atan2(y, x)\",\n            \"usage\": \"Full-circle angle computation in [\u2212\u03c0, \u03c0]\",\n        },\n    )\n\n    # ================================================================\n    # Hyperbolic Functions\n    # ================================================================\n\n    calc[\"sinh\"] = Expression(\n        name=\"sinh\",\n        symbolic_expr=\"sinh(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Hyperbolic sine: (e\u02e3 \u2212 e\u207b\u02e3)/2\",\n            \"usage\": \"Catenary curves, special relativity\",\n            \"derivative\": \"cosh(x)\",\n        },\n    )\n\n    calc[\"cosh\"] = Expression(\n        name=\"cosh\",\n        symbolic_expr=\"cosh(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Hyperbolic cosine: (e\u02e3 + e\u207b\u02e3)/2\",\n            \"usage\": \"Catenary curves, distance in hyperbolic space\",\n            \"derivative\": \"sinh(x)\",\n        },\n    )\n\n    calc[\"tanh_func\"] = Expression(\n        name=\"tanh_func\",\n        symbolic_expr=\"tanh(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Hyperbolic tangent: sinh(x)/cosh(x)\",\n            \"usage\": \"Neural network activations, bounded output\",\n            \"derivative\": \"1 \u2212 tanh\u00b2(x) = sech\u00b2(x)\",\n        },\n    )\n\n    calc[\"sech\"] = Expression(\n        name=\"sech\",\n        symbolic_expr=\"1 / cosh(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Hyperbolic secant sech(x) = 1/cosh(x)\",\n            \"usage\": \"Soliton solutions, sech\u00b2 potential\",\n        },\n    )\n\n    # ================================================================\n    # Inverse Hyperbolic Functions\n    # ================================================================\n\n    calc[\"arcsinh\"] = Expression(\n        name=\"arcsinh\",\n        symbolic_expr=\"asinh(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Inverse hyperbolic sine: ln(x + \u221a(x\u00b2+1))\",\n            \"usage\": \"Smooth logarithm-like transform for all x\",\n            \"derivative\": \"1/\u221a(x\u00b2 + 1)\",\n        },\n    )\n\n    calc[\"arccosh\"] = Expression(\n        name=\"arccosh\",\n        symbolic_expr=\"acosh(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Inverse hyperbolic cosine: ln(x + \u221a(x\u00b2\u22121))\",\n            \"usage\": \"Hyperbolic geometry, distance metrics\",\n        },\n    )\n\n    calc[\"arctanh\"] = Expression(\n        name=\"arctanh\",\n        symbolic_expr=\"atanh(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Inverse hyperbolic tangent: \u00bdln((1+x)/(1\u2212x))\",\n            \"usage\": \"Fisher z-transform, correlation analysis\",\n            \"derivative\": \"1/(1 \u2212 x\u00b2)\",\n        },\n    )\n\n    # ================================================================\n    # Special Functions\n    # ================================================================\n\n    calc[\"erf\"] = Expression(\n        name=\"erf\",\n        symbolic_expr=\"erf(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Error function erf(x) = (2/\u221a\u03c0) \u222b\u2080\u02e3 e^(\u2212t\u00b2) dt\",\n            \"usage\": \"GELU activation, Gaussian CDF, statistics\",\n            \"derivative\": \"(2/\u221a\u03c0)\u00b7e^(\u2212x\u00b2)\",\n        },\n    )\n\n    calc[\"erfc\"] = Expression(\n        name=\"erfc\",\n        symbolic_expr=\"erfc(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Complementary error function: 1 \u2212 erf(x)\",\n            \"usage\": \"Tail probabilities, Q-function\",\n        },\n    )\n\n    calc[\"gamma_func\"] = Expression(\n        name=\"gamma_func\",\n        symbolic_expr=\"gamma(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Gamma function \u0393(x) - generalized factorial\",\n            \"usage\": \"Distributions, combinatorics, \u0393(n) = (n\u22121)!\",\n        },\n    )\n\n    calc[\"digamma\"] = Expression(\n        name=\"digamma\",\n        symbolic_expr=\"digamma(x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Digamma function \u03c8(x) = d/dx ln \u0393(x)\",\n            \"usage\": \"Bayesian inference, sufficient statistics\",\n        },\n    )\n\n    calc[\"beta_func\"] = Expression(\n        name=\"beta_func\",\n        symbolic_expr=\"gamma(a_param) * gamma(b_param) / gamma(a_param + b_param)\",\n        params={\"a_param\": 1.0, \"b_param\": 1.0},\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Beta function B(a,b) = \u0393(a)\u0393(b)/\u0393(a+b)\",\n            \"usage\": \"Beta distribution normalization, Bayesian priors\",\n        },\n    )\n\n    calc[\"sinc\"] = Expression(\n        name=\"sinc\",\n        symbolic_expr=\"Piecewise((1, Eq(x, 0)), (sin(pi*x)/(pi*x), True))\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Sinc function: sin(\u03c0x)/(\u03c0x)\",\n            \"usage\": \"Signal reconstruction, Fourier analysis\",\n        },\n    )\n\n    # ================================================================\n    # Series Approximations (educational)\n    # ================================================================\n\n    calc[\"taylor_exp\"] = Expression(\n        name=\"taylor_exp\",\n        symbolic_expr=\"1 + x + x**2/2 + x**3/6 + x**4/24 + x**5/120\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Taylor series for e\u02e3 (5 terms)\",\n            \"usage\": \"Educational: showing how series approximate functions\",\n        },\n    )\n\n    calc[\"taylor_sin\"] = Expression(\n        name=\"taylor_sin\",\n        symbolic_expr=\"x - x**3/6 + x**5/120 - x**7/5040\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Taylor series for sin(x) (4 terms)\",\n            \"usage\": \"Educational: polynomial approximation of sin\",\n        },\n    )\n\n    calc[\"taylor_cos\"] = Expression(\n        name=\"taylor_cos\",\n        symbolic_expr=\"1 - x**2/2 + x**4/24 - x**6/720\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Taylor series for cos(x) (4 terms)\",\n            \"usage\": \"Educational: polynomial approximation of cos\",\n        },\n    )\n\n    calc[\"taylor_ln1px\"] = Expression(\n        name=\"taylor_ln1px\",\n        symbolic_expr=\"x - x**2/2 + x**3/3 - x**4/4 + x**5/5\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Taylor series for ln(1+x) (5 terms)\",\n            \"usage\": \"Educational: approximation valid for |x| &lt; 1\",\n        },\n    )\n\n    calc[\"taylor_arctan\"] = Expression(\n        name=\"taylor_arctan\",\n        symbolic_expr=\"x - x**3/3 + x**5/5 - x**7/7\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Taylor series for atan(x) (4 terms)\",\n            \"usage\": \"Educational: Leibniz formula for \u03c0/4\",\n        },\n    )\n\n    # ================================================================\n    # Fundamental Calculus Operations\n    # ================================================================\n\n    calc[\"gradient_descent_step\"] = Expression(\n        name=\"gradient_descent_step\",\n        symbolic_expr=\"w - lr * gradient\",\n        params={\"lr\": 0.01},\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Single gradient descent update: w \u2190 w \u2212 \u03b7\u00b7\u2207L\",\n            \"usage\": \"Foundation of all neural network training\",\n        },\n    )\n\n    calc[\"chain_rule\"] = Expression(\n        name=\"chain_rule\",\n        symbolic_expr=\"df_du * du_dx\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Chain rule: d/dx f(u(x)) = f'(u)\u00b7u'(x)\",\n            \"usage\": \"Backpropagation, composite function derivatives\",\n        },\n    )\n\n    calc[\"product_rule\"] = Expression(\n        name=\"product_rule\",\n        symbolic_expr=\"f_val * dg_dx + g_val * df_dx\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Product rule: (fg)' = f\u00b7g' + g\u00b7f'\",\n            \"usage\": \"Derivatives of products, attention gradients\",\n        },\n    )\n\n    calc[\"quotient_rule\"] = Expression(\n        name=\"quotient_rule\",\n        symbolic_expr=\"(g_val * df_dx - f_val * dg_dx) / (g_val**2 + epsilon)\",\n        params={\"epsilon\": 1e-8},\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Quotient rule: (f/g)' = (g\u00b7f' \u2212 f\u00b7g')/g\u00b2\",\n            \"usage\": \"Derivatives of ratios, softmax gradients\",\n        },\n    )\n\n    calc[\"finite_difference\"] = Expression(\n        name=\"finite_difference\",\n        symbolic_expr=\"(f_plus - f_minus) / (2 * h)\",\n        params={\"h\": 1e-5},\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Central finite difference: (f(x+h) \u2212 f(x\u2212h))/(2h)\",\n            \"usage\": \"Numerical differentiation, gradient checking\",\n        },\n    )\n\n    calc[\"trapezoidal_rule\"] = Expression(\n        name=\"trapezoidal_rule\",\n        symbolic_expr=\"h * (f_a + f_b) / 2\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Trapezoidal rule: h\u00b7(f(a) + f(b))/2\",\n            \"usage\": \"Numerical integration, area approximation\",\n        },\n    )\n\n    calc[\"simpsons_rule\"] = Expression(\n        name=\"simpsons_rule\",\n        symbolic_expr=\"h * (f_a + 4*f_m + f_b) / 6\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Simpson's rule: h\u00b7(f(a) + 4f(m) + f(b))/6\",\n            \"usage\": \"Accurate numerical integration\",\n        },\n    )\n\n    # ================================================================\n    # Integral Transforms (simplified scalar forms)\n    # ================================================================\n\n    calc[\"laplace_kernel\"] = Expression(\n        name=\"laplace_kernel\",\n        symbolic_expr=\"exp(-s * t)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Laplace transform kernel e^(\u2212st)\",\n            \"usage\": \"Control theory, differential equation solutions\",\n        },\n    )\n\n    calc[\"fourier_kernel_real\"] = Expression(\n        name=\"fourier_kernel_real\",\n        symbolic_expr=\"cos(omega * t)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Real part of Fourier kernel cos(\u03c9t)\",\n            \"usage\": \"Frequency analysis, spectral decomposition\",\n        },\n    )\n\n    calc[\"fourier_kernel_imag\"] = Expression(\n        name=\"fourier_kernel_imag\",\n        symbolic_expr=\"-sin(omega * t)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Imaginary part of Fourier kernel \u2212sin(\u03c9t)\",\n            \"usage\": \"Frequency analysis, phase information\",\n        },\n    )\n\n    calc[\"wavelet_morlet\"] = Expression(\n        name=\"wavelet_morlet\",\n        symbolic_expr=\"exp(-x**2/2) * cos(5*x)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Morlet wavelet: e^(\u2212x\u00b2/2)\u00b7cos(5x)\",\n            \"usage\": \"Time-frequency analysis, wavelet transforms\",\n        },\n    )\n\n    calc[\"wavelet_mexican_hat\"] = Expression(\n        name=\"wavelet_mexican_hat\",\n        symbolic_expr=\"(2/sqrt(3)) * pi**(-Rational(1,4)) * (1 - x**2) * exp(-x**2/2)\",\n        metadata={\n            \"category\": \"calculus\",\n            \"description\": \"Mexican hat (Ricker) wavelet: (1\u2212x\u00b2)e^(\u2212x\u00b2/2)\",\n            \"usage\": \"Edge detection, seismology, wavelet analysis\",\n        },\n    )\n\n    return calc\n</code></pre>"},{"location":"api/reference/#statistics","title":"Statistics","text":""},{"location":"api/reference/#neurogebra.repository.statistics.get_statistics_expressions","title":"<code>neurogebra.repository.statistics.get_statistics_expressions()</code>","text":"<p>Get dictionary of statistical expressions.</p> <p>Returns:</p> Type Description <code>Dict[str, Expression]</code> <p>Dictionary mapping names to Expression instances</p> Source code in <code>src/neurogebra/repository/statistics.py</code> <pre><code>def get_statistics_expressions() -&gt; Dict[str, Expression]:\n    \"\"\"\n    Get dictionary of statistical expressions.\n\n    Returns:\n        Dictionary mapping names to Expression instances\n    \"\"\"\n    stats: Dict[str, Expression] = {}\n\n    # ================================================================\n    # Probability Density Functions\n    # ================================================================\n\n    stats[\"normal_pdf\"] = Expression(\n        name=\"normal_pdf\",\n        symbolic_expr=\"(1 / (sigma * sqrt(2 * pi))) * exp(-(x - mu)**2 / (2 * sigma**2))\",\n        params={\"mu\": 0, \"sigma\": 1},\n        trainable_params=[\"mu\", \"sigma\"],\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"distribution\",\n            \"description\": \"Normal (Gaussian) probability density function\",\n            \"usage\": \"Central limit theorem, Bayesian priors, noise modeling\",\n            \"formula_latex\": r\"\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\",\n        },\n    )\n\n    stats[\"standard_normal_pdf\"] = Expression(\n        name=\"standard_normal_pdf\",\n        symbolic_expr=\"(1 / sqrt(2 * pi)) * exp(-x**2 / 2)\",\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"distribution\",\n            \"description\": \"Standard normal PDF (\u03bc=0, \u03c3=1)\",\n            \"usage\": \"Z-scores, standard statistics\",\n            \"formula_latex\": r\"\\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\",\n        },\n    )\n\n    stats[\"uniform_pdf\"] = Expression(\n        name=\"uniform_pdf\",\n        symbolic_expr=\"1 / (b - a)\",\n        params={\"a\": 0, \"b\": 1},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"distribution\",\n            \"description\": \"Uniform distribution PDF on [a, b]\",\n            \"usage\": \"Random initialization, prior distributions\",\n            \"formula_latex\": r\"\\frac{1}{b - a}\",\n        },\n    )\n\n    stats[\"exponential_pdf\"] = Expression(\n        name=\"exponential_pdf\",\n        symbolic_expr=\"lambda_param * exp(-lambda_param * x)\",\n        params={\"lambda_param\": 1},\n        trainable_params=[\"lambda_param\"],\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"distribution\",\n            \"description\": \"Exponential distribution PDF\",\n            \"usage\": \"Waiting times, survival analysis, Poisson processes\",\n            \"formula_latex\": r\"\\lambda e^{-\\lambda x}\",\n        },\n    )\n\n    stats[\"laplace_pdf\"] = Expression(\n        name=\"laplace_pdf\",\n        symbolic_expr=\"(1 / (2 * b)) * exp(-Abs(x - mu) / b)\",\n        params={\"mu\": 0, \"b\": 1},\n        trainable_params=[\"mu\", \"b\"],\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"distribution\",\n            \"description\": \"Laplace distribution PDF\",\n            \"usage\": \"Robust statistics, L1 regularization prior\",\n            \"formula_latex\": r\"\\frac{1}{2b} e^{-|x-\\mu|/b}\",\n        },\n    )\n\n    stats[\"cauchy_pdf\"] = Expression(\n        name=\"cauchy_pdf\",\n        symbolic_expr=\"1 / (pi * gamma_param * (1 + ((x - x0) / gamma_param)**2))\",\n        params={\"x0\": 0, \"gamma_param\": 1},\n        trainable_params=[\"x0\", \"gamma_param\"],\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"distribution\",\n            \"description\": \"Cauchy (Lorentzian) distribution PDF\",\n            \"usage\": \"Heavy-tailed modeling, spectral lines\",\n            \"formula_latex\": r\"\\frac{1}{\\pi\\gamma[1+(\\frac{x-x_0}{\\gamma})^2]}\",\n        },\n    )\n\n    stats[\"log_normal_pdf\"] = Expression(\n        name=\"log_normal_pdf\",\n        symbolic_expr=\"(1 / (x * sigma * sqrt(2 * pi))) * exp(-(log(x) - mu)**2 / (2 * sigma**2))\",\n        params={\"mu\": 0, \"sigma\": 1},\n        trainable_params=[\"mu\", \"sigma\"],\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"distribution\",\n            \"description\": \"Log-normal distribution PDF\",\n            \"usage\": \"Income distributions, stock prices, multiplicative processes\",\n            \"formula_latex\": r\"\\frac{1}{x\\sigma\\sqrt{2\\pi}} e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}}\",\n        },\n    )\n\n    stats[\"rayleigh_pdf\"] = Expression(\n        name=\"rayleigh_pdf\",\n        symbolic_expr=\"(x / sigma**2) * exp(-x**2 / (2 * sigma**2))\",\n        params={\"sigma\": 1},\n        trainable_params=[\"sigma\"],\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"distribution\",\n            \"description\": \"Rayleigh distribution PDF\",\n            \"usage\": \"Wind speed modeling, signal amplitude\",\n            \"formula_latex\": r\"\\frac{x}{\\sigma^2} e^{-x^2/(2\\sigma^2)}\",\n        },\n    )\n\n    stats[\"gumbel_pdf\"] = Expression(\n        name=\"gumbel_pdf\",\n        symbolic_expr=\"(1 / beta_param) * exp(-(z + exp(-z)))\",\n        params={\"mu\": 0, \"beta_param\": 1},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"distribution\",\n            \"description\": \"Gumbel distribution PDF (z = (x-\u03bc)/\u03b2)\",\n            \"usage\": \"Extreme value theory, max/min modeling\",\n            \"note\": \"z = (x - mu) / beta; substitute before eval\",\n            \"formula_latex\": r\"\\frac{1}{\\beta} e^{-(z + e^{-z})}\",\n        },\n    )\n\n    stats[\"beta_pdf\"] = Expression(\n        name=\"beta_pdf\",\n        symbolic_expr=\"x**(alpha - 1) * (1 - x)**(beta_param - 1)\",\n        params={\"alpha\": 2, \"beta_param\": 5},\n        trainable_params=[\"alpha\", \"beta_param\"],\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"distribution\",\n            \"description\": \"Beta distribution PDF (unnormalized, x \u2208 [0,1])\",\n            \"usage\": \"Bayesian priors for probabilities, A/B testing\",\n            \"formula_latex\": r\"x^{\\alpha-1}(1-x)^{\\beta-1}\",\n        },\n    )\n\n    stats[\"chi_squared_kernel\"] = Expression(\n        name=\"chi_squared_kernel\",\n        symbolic_expr=\"x**(k/2 - 1) * exp(-x/2)\",\n        params={\"k\": 2},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"distribution\",\n            \"description\": \"Chi-squared distribution kernel (unnormalized)\",\n            \"usage\": \"Hypothesis testing, goodness-of-fit\",\n            \"formula_latex\": r\"x^{k/2 - 1} e^{-x/2}\",\n        },\n    )\n\n    stats[\"student_t_kernel\"] = Expression(\n        name=\"student_t_kernel\",\n        symbolic_expr=\"(1 + x**2 / nu)**(-(nu + 1) / 2)\",\n        params={\"nu\": 3},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"distribution\",\n            \"description\": \"Student's t-distribution kernel (unnormalized)\",\n            \"usage\": \"Small sample inference, robust regression\",\n            \"formula_latex\": r\"(1 + x^2/\\nu)^{-(\\nu+1)/2}\",\n        },\n    )\n\n    # ================================================================\n    # Cumulative Distribution &amp; Survival Functions\n    # ================================================================\n\n    stats[\"logistic_cdf\"] = Expression(\n        name=\"logistic_cdf\",\n        symbolic_expr=\"1 / (1 + exp(-(x - mu) / s))\",\n        params={\"mu\": 0, \"s\": 1},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"cdf\",\n            \"description\": \"Logistic CDF (sigmoid with location/scale)\",\n            \"usage\": \"Binary classification, logistic regression\",\n            \"formula_latex\": r\"\\frac{1}{1 + e^{-(x-\\mu)/s}}\",\n        },\n    )\n\n    stats[\"exponential_cdf\"] = Expression(\n        name=\"exponential_cdf\",\n        symbolic_expr=\"1 - exp(-lambda_param * x)\",\n        params={\"lambda_param\": 1},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"cdf\",\n            \"description\": \"Exponential CDF\",\n            \"usage\": \"Survival analysis, reliability engineering\",\n            \"formula_latex\": r\"1 - e^{-\\lambda x}\",\n        },\n    )\n\n    stats[\"survival_exponential\"] = Expression(\n        name=\"survival_exponential\",\n        symbolic_expr=\"exp(-lambda_param * x)\",\n        params={\"lambda_param\": 1},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"survival\",\n            \"description\": \"Exponential survival function S(x) = 1 - F(x)\",\n            \"usage\": \"Survival analysis, reliability, time-to-event\",\n            \"formula_latex\": r\"e^{-\\lambda x}\",\n        },\n    )\n\n    stats[\"weibull_survival\"] = Expression(\n        name=\"weibull_survival\",\n        symbolic_expr=\"exp(-(x / lambda_param)**k)\",\n        params={\"lambda_param\": 1, \"k\": 1.5},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"survival\",\n            \"description\": \"Weibull survival function\",\n            \"usage\": \"Failure analysis, wind speed distribution\",\n            \"formula_latex\": r\"e^{-(x/\\lambda)^k}\",\n        },\n    )\n\n    # ================================================================\n    # Information Theory\n    # ================================================================\n\n    stats[\"binary_entropy\"] = Expression(\n        name=\"binary_entropy\",\n        symbolic_expr=\"-(p * log(p + 1e-15) + (1 - p) * log(1 - p + 1e-15))\",\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"information_theory\",\n            \"description\": \"Binary entropy H(p) for Bernoulli variable\",\n            \"usage\": \"Decision trees, information gain, uncertainty\",\n            \"formula_latex\": r\"-[p\\ln p + (1-p)\\ln(1-p)]\",\n        },\n    )\n\n    stats[\"cross_entropy_elem\"] = Expression(\n        name=\"cross_entropy_elem\",\n        symbolic_expr=\"-(y * log(p + 1e-15) + (1 - y) * log(1 - p + 1e-15))\",\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"information_theory\",\n            \"description\": \"Element-wise binary cross-entropy\",\n            \"usage\": \"Classification loss, KL divergence component\",\n            \"formula_latex\": r\"-[y\\ln p + (1-y)\\ln(1-p)]\",\n        },\n    )\n\n    stats[\"kl_divergence_elem\"] = Expression(\n        name=\"kl_divergence_elem\",\n        symbolic_expr=\"p * log((p + 1e-15) / (q + 1e-15))\",\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"information_theory\",\n            \"description\": \"Element-wise KL divergence D_KL(p||q)\",\n            \"usage\": \"VAE loss, distribution matching, Bayesian inference\",\n            \"formula_latex\": r\"p \\ln\\frac{p}{q}\",\n        },\n    )\n\n    stats[\"js_divergence_elem\"] = Expression(\n        name=\"js_divergence_elem\",\n        symbolic_expr=\"(1/2) * (p * log((2*p) / (p + q + 1e-15)) + q * log((2*q) / (p + q + 1e-15)))\",\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"information_theory\",\n            \"description\": \"Element-wise Jensen-Shannon divergence\",\n            \"usage\": \"GAN training, symmetric distribution comparison\",\n            \"formula_latex\": r\"\\frac{1}{2}[p\\ln\\frac{2p}{p+q} + q\\ln\\frac{2q}{p+q}]\",\n        },\n    )\n\n    stats[\"mutual_info_bound\"] = Expression(\n        name=\"mutual_info_bound\",\n        symbolic_expr=\"log(1 + exp(x))\",\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"information_theory\",\n            \"description\": \"Donsker-Varadhan/NWJ lower bound kernel for MI\",\n            \"usage\": \"Mutual information estimation, representation learning\",\n            \"formula_latex\": r\"\\ln(1 + e^x)\",\n        },\n    )\n\n    # ================================================================\n    # Descriptive Statistics (scalar expressions)\n    # ================================================================\n\n    stats[\"z_score\"] = Expression(\n        name=\"z_score\",\n        symbolic_expr=\"(x - mu) / sigma\",\n        params={\"mu\": 0, \"sigma\": 1},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"descriptive\",\n            \"description\": \"Z-score standardization\",\n            \"usage\": \"Normalization, hypothesis testing, outlier detection\",\n            \"formula_latex\": r\"z = \\frac{x - \\mu}{\\sigma}\",\n        },\n    )\n\n    stats[\"t_statistic\"] = Expression(\n        name=\"t_statistic\",\n        symbolic_expr=\"(x - mu) / (s / sqrt(n))\",\n        params={\"mu\": 0, \"s\": 1, \"n\": 30},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"descriptive\",\n            \"description\": \"t-statistic for one-sample t-test\",\n            \"usage\": \"Hypothesis testing, confidence intervals\",\n            \"formula_latex\": r\"t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}}\",\n        },\n    )\n\n    stats[\"coefficient_of_variation\"] = Expression(\n        name=\"coefficient_of_variation\",\n        symbolic_expr=\"sigma / mu\",\n        params={\"mu\": 1, \"sigma\": 0.1},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"descriptive\",\n            \"description\": \"Coefficient of variation (relative std dev)\",\n            \"usage\": \"Compare variability across scales\",\n            \"formula_latex\": r\"CV = \\frac{\\sigma}{\\mu}\",\n        },\n    )\n\n    stats[\"pooled_variance\"] = Expression(\n        name=\"pooled_variance\",\n        symbolic_expr=\"((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2)\",\n        params={\"n1\": 30, \"s1\": 1, \"n2\": 30, \"s2\": 1},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"descriptive\",\n            \"description\": \"Pooled variance for two-sample t-test\",\n            \"usage\": \"Comparing two groups, ANOVA\",\n            \"formula_latex\": r\"s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}\",\n        },\n    )\n\n    stats[\"standard_error\"] = Expression(\n        name=\"standard_error\",\n        symbolic_expr=\"sigma / sqrt(n)\",\n        params={\"sigma\": 1, \"n\": 30},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"descriptive\",\n            \"description\": \"Standard error of the mean\",\n            \"usage\": \"Confidence intervals, significance testing\",\n            \"formula_latex\": r\"SE = \\frac{\\sigma}{\\sqrt{n}}\",\n        },\n    )\n\n    # ================================================================\n    # Bayesian Statistics\n    # ================================================================\n\n    stats[\"bayes_posterior_kernel\"] = Expression(\n        name=\"bayes_posterior_kernel\",\n        symbolic_expr=\"exp(-((x - mu_prior)**2) / (2 * sigma_prior**2)) * exp(-((y - x)**2) / (2 * sigma_lik**2))\",\n        params={\"mu_prior\": 0, \"sigma_prior\": 1, \"sigma_lik\": 1},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"bayesian\",\n            \"description\": \"Gaussian prior \u00d7 Gaussian likelihood (posterior kernel)\",\n            \"usage\": \"Bayesian inference, conjugate priors\",\n            \"formula_latex\": r\"\\mathcal{N}(x|\\mu_0,\\sigma_0^2) \\cdot \\mathcal{N}(y|x,\\sigma_l^2)\",\n        },\n    )\n\n    stats[\"log_prior_normal\"] = Expression(\n        name=\"log_prior_normal\",\n        symbolic_expr=\"-(x - mu)**2 / (2 * sigma**2) - log(sigma) - log(2 * pi) / 2\",\n        params={\"mu\": 0, \"sigma\": 1},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"bayesian\",\n            \"description\": \"Log of normal prior density\",\n            \"usage\": \"Log-space Bayesian computation, weight priors in BNNs\",\n            \"formula_latex\": r\"-\\frac{(x-\\mu)^2}{2\\sigma^2} - \\ln\\sigma - \\frac{\\ln 2\\pi}{2}\",\n        },\n    )\n\n    stats[\"evidence_lower_bound\"] = Expression(\n        name=\"evidence_lower_bound\",\n        symbolic_expr=\"log_likelihood - kl_term\",\n        params={\"log_likelihood\": 0, \"kl_term\": 0},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"bayesian\",\n            \"description\": \"Evidence Lower Bound (ELBO) = E[log p(x|z)] - KL(q||p)\",\n            \"usage\": \"Variational autoencoders, variational inference\",\n            \"formula_latex\": r\"ELBO = \\mathbb{E}[\\log p(x|z)] - D_{KL}(q(z|x) \\| p(z))\",\n        },\n    )\n\n    # ================================================================\n    # Regression &amp; Correlation (scalar forms)\n    # ================================================================\n\n    stats[\"pearson_r_component\"] = Expression(\n        name=\"pearson_r_component\",\n        symbolic_expr=\"(x - mu_x) * (y - mu_y) / (sigma_x * sigma_y)\",\n        params={\"mu_x\": 0, \"mu_y\": 0, \"sigma_x\": 1, \"sigma_y\": 1},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"correlation\",\n            \"description\": \"Element-wise Pearson correlation component\",\n            \"usage\": \"Correlation analysis, feature selection\",\n            \"formula_latex\": r\"\\frac{(x-\\mu_x)(y-\\mu_y)}{\\sigma_x \\sigma_y}\",\n        },\n    )\n\n    stats[\"linear_regression_pred\"] = Expression(\n        name=\"linear_regression_pred\",\n        symbolic_expr=\"beta_0 + beta_1 * x\",\n        params={\"beta_0\": 0, \"beta_1\": 1},\n        trainable_params=[\"beta_0\", \"beta_1\"],\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"regression\",\n            \"description\": \"Simple linear regression prediction\",\n            \"usage\": \"Regression, trend estimation\",\n            \"formula_latex\": r\"\\hat{y} = \\beta_0 + \\beta_1 x\",\n        },\n    )\n\n    stats[\"logistic_regression_pred\"] = Expression(\n        name=\"logistic_regression_pred\",\n        symbolic_expr=\"1 / (1 + exp(-(beta_0 + beta_1 * x)))\",\n        params={\"beta_0\": 0, \"beta_1\": 1},\n        trainable_params=[\"beta_0\", \"beta_1\"],\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"regression\",\n            \"description\": \"Logistic regression prediction (binary)\",\n            \"usage\": \"Binary classification, probability estimation\",\n            \"formula_latex\": r\"\\sigma(\\beta_0 + \\beta_1 x) = \\frac{1}{1+e^{-(\\beta_0+\\beta_1 x)}}\",\n        },\n    )\n\n    # ================================================================\n    # Moment Generating &amp; Characteristic Functions\n    # ================================================================\n\n    stats[\"mgf_normal\"] = Expression(\n        name=\"mgf_normal\",\n        symbolic_expr=\"exp(mu * t + sigma**2 * t**2 / 2)\",\n        params={\"mu\": 0, \"sigma\": 1},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"generating_function\",\n            \"description\": \"Moment generating function of Normal distribution\",\n            \"usage\": \"Deriving moments, proving CLT\",\n            \"formula_latex\": r\"M(t) = e^{\\mu t + \\sigma^2 t^2/2}\",\n        },\n    )\n\n    stats[\"mgf_exponential\"] = Expression(\n        name=\"mgf_exponential\",\n        symbolic_expr=\"lambda_param / (lambda_param - t)\",\n        params={\"lambda_param\": 1},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"generating_function\",\n            \"description\": \"Moment generating function of Exponential distribution\",\n            \"usage\": \"Computing exponential moments\",\n            \"formula_latex\": r\"M(t) = \\frac{\\lambda}{\\lambda - t}\",\n        },\n    )\n\n    stats[\"characteristic_normal\"] = Expression(\n        name=\"characteristic_normal\",\n        symbolic_expr=\"exp(I * mu * t - sigma**2 * t**2 / 2)\",\n        params={\"mu\": 0, \"sigma\": 1},\n        metadata={\n            \"category\": \"statistics\",\n            \"subcategory\": \"generating_function\",\n            \"description\": \"Characteristic function of Normal distribution\",\n            \"usage\": \"Fourier analysis of distributions, CLT proofs\",\n            \"formula_latex\": r\"\\varphi(t) = e^{i\\mu t - \\sigma^2 t^2/2}\",\n        },\n    )\n\n    return stats\n</code></pre>"},{"location":"api/reference/#linear-algebra","title":"Linear Algebra","text":""},{"location":"api/reference/#neurogebra.repository.linalg.get_linalg_expressions","title":"<code>neurogebra.repository.linalg.get_linalg_expressions()</code>","text":"<p>Get dictionary of linear algebra expressions.</p> <p>Returns:</p> Type Description <code>Dict[str, Expression]</code> <p>Dictionary mapping names to Expression instances</p> Source code in <code>src/neurogebra/repository/linalg.py</code> <pre><code>def get_linalg_expressions() -&gt; Dict[str, Expression]:\n    \"\"\"\n    Get dictionary of linear algebra expressions.\n\n    Returns:\n        Dictionary mapping names to Expression instances\n    \"\"\"\n    la: Dict[str, Expression] = {}\n\n    # ================================================================\n    # Vector Norms (element-wise components)\n    # ================================================================\n\n    la[\"l1_norm_elem\"] = Expression(\n        name=\"l1_norm_elem\",\n        symbolic_expr=\"Abs(x)\",\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"norm\",\n            \"description\": \"L1 norm element |x\u1d62| (sum over elements for vector norm)\",\n            \"usage\": \"Sparsity, Manhattan distance, LASSO\",\n            \"formula_latex\": r\"|x_i|\",\n        },\n    )\n\n    la[\"l2_norm_elem\"] = Expression(\n        name=\"l2_norm_elem\",\n        symbolic_expr=\"x**2\",\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"norm\",\n            \"description\": \"L2 norm element x\u1d62\u00b2 (sum then sqrt for vector norm)\",\n            \"usage\": \"Euclidean distance, weight decay, Ridge\",\n            \"formula_latex\": r\"x_i^2\",\n        },\n    )\n\n    la[\"lp_norm_elem\"] = Expression(\n        name=\"lp_norm_elem\",\n        symbolic_expr=\"Abs(x)**p\",\n        params={\"p\": 2},\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"norm\",\n            \"description\": \"General Lp norm element |x\u1d62|^p\",\n            \"usage\": \"Generalized norms, robust statistics\",\n            \"formula_latex\": r\"|x_i|^p\",\n        },\n    )\n\n    la[\"huber_norm_elem\"] = Expression(\n        name=\"huber_norm_elem\",\n        symbolic_expr=\"Piecewise((x**2 / 2, Abs(x) &lt;= delta), (delta * (Abs(x) - delta / 2), True))\",\n        params={\"delta\": 1},\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"norm\",\n            \"description\": \"Huber norm element (smooth L1/L2 transition)\",\n            \"usage\": \"Robust regression, smooth optimization\",\n            \"formula_latex\": r\"\\begin{cases} x^2/2 &amp; |x|\\le\\delta \\\\ \\delta(|x|-\\delta/2) &amp; \\text{otherwise} \\end{cases}\",\n        },\n    )\n\n    # ================================================================\n    # Inner Products &amp; Similarities\n    # ================================================================\n\n    la[\"dot_product_elem\"] = Expression(\n        name=\"dot_product_elem\",\n        symbolic_expr=\"x * y\",\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"inner_product\",\n            \"description\": \"Dot product element x\u1d62\u00b7y\u1d62 (sum for full dot product)\",\n            \"usage\": \"Similarity, projections, attention scores\",\n            \"formula_latex\": r\"x_i \\cdot y_i\",\n        },\n    )\n\n    la[\"cosine_similarity\"] = Expression(\n        name=\"cosine_similarity\",\n        symbolic_expr=\"dot_xy / (norm_x * norm_y + 1e-8)\",\n        params={\"dot_xy\": 1, \"norm_x\": 1, \"norm_y\": 1},\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"similarity\",\n            \"description\": \"Cosine similarity cos(\u03b8) = (x\u00b7y) / (\u2016x\u2016\u2016y\u2016)\",\n            \"usage\": \"Text similarity, recommendation, embeddings\",\n            \"formula_latex\": r\"\\cos\\theta = \\frac{\\mathbf{x}\\cdot\\mathbf{y}}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|}\",\n        },\n    )\n\n    la[\"scaled_dot_product\"] = Expression(\n        name=\"scaled_dot_product\",\n        symbolic_expr=\"dot_qk / sqrt(d_k)\",\n        params={\"dot_qk\": 1, \"d_k\": 64},\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"inner_product\",\n            \"description\": \"Scaled dot-product (Transformer attention core)\",\n            \"usage\": \"Self-attention, cross-attention in Transformers\",\n            \"formula_latex\": r\"\\frac{Q \\cdot K^T}{\\sqrt{d_k}}\",\n        },\n    )\n\n    # ================================================================\n    # Distance Metrics\n    # ================================================================\n\n    la[\"euclidean_dist_elem\"] = Expression(\n        name=\"euclidean_dist_elem\",\n        symbolic_expr=\"(x - y)**2\",\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"distance\",\n            \"description\": \"Squared Euclidean distance element (sum then sqrt)\",\n            \"usage\": \"K-means, KNN, distance-based clustering\",\n            \"formula_latex\": r\"(x_i - y_i)^2\",\n        },\n    )\n\n    la[\"manhattan_dist_elem\"] = Expression(\n        name=\"manhattan_dist_elem\",\n        symbolic_expr=\"Abs(x - y)\",\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"distance\",\n            \"description\": \"Manhattan distance element |x\u1d62 - y\u1d62|\",\n            \"usage\": \"High-dimensional data, grid movement\",\n            \"formula_latex\": r\"|x_i - y_i|\",\n        },\n    )\n\n    la[\"minkowski_dist_elem\"] = Expression(\n        name=\"minkowski_dist_elem\",\n        symbolic_expr=\"Abs(x - y)**p\",\n        params={\"p\": 2},\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"distance\",\n            \"description\": \"Minkowski distance element |x\u1d62 - y\u1d62|^p\",\n            \"usage\": \"Generalized distance (p=1 Manhattan, p=2 Euclidean)\",\n            \"formula_latex\": r\"|x_i - y_i|^p\",\n        },\n    )\n\n    la[\"chebyshev_dist\"] = Expression(\n        name=\"chebyshev_dist\",\n        symbolic_expr=\"Max(Abs(x - y), Abs(a - b))\",\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"distance\",\n            \"description\": \"Chebyshev distance max|x\u1d62 - y\u1d62| (2D demo)\",\n            \"usage\": \"Chess-board distance, L\u221e norm\",\n            \"formula_latex\": r\"\\max_i |x_i - y_i|\",\n        },\n    )\n\n    la[\"mahalanobis_1d\"] = Expression(\n        name=\"mahalanobis_1d\",\n        symbolic_expr=\"Abs(x - mu) / sigma\",\n        params={\"mu\": 0, \"sigma\": 1},\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"distance\",\n            \"description\": \"1D Mahalanobis distance (generalized z-score)\",\n            \"usage\": \"Outlier detection, multivariate anomaly detection\",\n            \"formula_latex\": r\"\\frac{|x - \\mu|}{\\sigma}\",\n        },\n    )\n\n    la[\"canberra_dist_elem\"] = Expression(\n        name=\"canberra_dist_elem\",\n        symbolic_expr=\"Abs(x - y) / (Abs(x) + Abs(y) + 1e-8)\",\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"distance\",\n            \"description\": \"Canberra distance element\",\n            \"usage\": \"Sensitive to small values, ecology data\",\n            \"formula_latex\": r\"\\frac{|x_i - y_i|}{|x_i| + |y_i|}\",\n        },\n    )\n\n    # ================================================================\n    # Projections &amp; Decompositions\n    # ================================================================\n\n    la[\"vector_projection_scalar\"] = Expression(\n        name=\"vector_projection_scalar\",\n        symbolic_expr=\"dot_ab / (norm_b**2 + 1e-8)\",\n        params={\"dot_ab\": 1, \"norm_b\": 1},\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"projection\",\n            \"description\": \"Scalar projection coefficient (a\u00b7b / \u2016b\u2016\u00b2)\",\n            \"usage\": \"Gram-Schmidt, PCA components, projections\",\n            \"formula_latex\": r\"\\frac{\\mathbf{a}\\cdot\\mathbf{b}}{\\|\\mathbf{b}\\|^2}\",\n        },\n    )\n\n    la[\"orthogonal_residual_elem\"] = Expression(\n        name=\"orthogonal_residual_elem\",\n        symbolic_expr=\"a - proj_coeff * b\",\n        params={\"proj_coeff\": 0},\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"projection\",\n            \"description\": \"Orthogonal residual element a\u1d62 - (proj coeff)\u00b7b\u1d62\",\n            \"usage\": \"Gram-Schmidt orthogonalization, residual computation\",\n            \"formula_latex\": r\"a_i - \\frac{\\mathbf{a}\\cdot\\mathbf{b}}{\\|\\mathbf{b}\\|^2} b_i\",\n        },\n    )\n\n    # ================================================================\n    # Matrix Operations (2\u00d72 scalar forms)\n    # ================================================================\n\n    la[\"determinant_2x2\"] = Expression(\n        name=\"determinant_2x2\",\n        symbolic_expr=\"a * d - b * c\",\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"matrix\",\n            \"description\": \"Determinant of 2\u00d72 matrix [[a,b],[c,d]]\",\n            \"usage\": \"Invertibility, area scaling, eigenvalue computation\",\n            \"formula_latex\": r\"\\det = ad - bc\",\n        },\n    )\n\n    la[\"trace_2x2\"] = Expression(\n        name=\"trace_2x2\",\n        symbolic_expr=\"a + d\",\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"matrix\",\n            \"description\": \"Trace of 2\u00d72 matrix [[a,b],[c,d]]\",\n            \"usage\": \"Sum of eigenvalues, matrix regularization\",\n            \"formula_latex\": r\"\\text{tr}(A) = a + d\",\n        },\n    )\n\n    la[\"eigenvalue_2x2\"] = Expression(\n        name=\"eigenvalue_2x2\",\n        symbolic_expr=\"(a + d) / 2 + sqrt(((a - d) / 2)**2 + b * c)\",\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"matrix\",\n            \"description\": \"Larger eigenvalue of 2\u00d72 matrix [[a,b],[c,d]]\",\n            \"usage\": \"PCA, spectral analysis, stability analysis\",\n            \"formula_latex\": r\"\\lambda_1 = \\frac{a+d}{2} + \\sqrt{(\\frac{a-d}{2})^2 + bc}\",\n        },\n    )\n\n    la[\"frobenius_elem\"] = Expression(\n        name=\"frobenius_elem\",\n        symbolic_expr=\"x**2\",\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"matrix\",\n            \"description\": \"Frobenius norm element (sum all a\u1d62\u2c7c\u00b2, then sqrt)\",\n            \"usage\": \"Matrix norm, regularization of weight matrices\",\n            \"formula_latex\": r\"a_{ij}^2\",\n        },\n    )\n\n    # ================================================================\n    # Kernel Functions (additional kernels; see also algebra.py)\n    # ================================================================\n\n    la[\"sigmoid_kernel\"] = Expression(\n        name=\"sigmoid_kernel\",\n        symbolic_expr=\"tanh(alpha * x * y + c_coeff)\",\n        params={\"alpha\": 1, \"c_coeff\": 0},\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"kernel\",\n            \"description\": \"Sigmoid (hyperbolic tangent) kernel\",\n            \"usage\": \"Neural network-inspired kernel, SVM\",\n            \"formula_latex\": r\"K(x,y) = \\tanh(\\alpha x y + c)\",\n        },\n    )\n\n    # ================================================================\n    # Attention &amp; Transformer Components\n    # ================================================================\n\n    la[\"softmax_score\"] = Expression(\n        name=\"softmax_score\",\n        symbolic_expr=\"exp(x) / (exp(x) + exp(y) + 1e-8)\",\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"attention\",\n            \"description\": \"2-class softmax probability\",\n            \"usage\": \"Attention weights, classification head\",\n            \"formula_latex\": r\"\\frac{e^{x_i}}{\\sum_j e^{x_j}}\",\n        },\n    )\n\n    la[\"log_softmax_score\"] = Expression(\n        name=\"log_softmax_score\",\n        symbolic_expr=\"x - log(exp(x) + exp(y) + 1e-8)\",\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"attention\",\n            \"description\": \"Log-softmax (numerically stable, 2-class)\",\n            \"usage\": \"NLLLoss input, cross-entropy computation\",\n            \"formula_latex\": r\"x_i - \\ln\\sum_j e^{x_j}\",\n        },\n    )\n\n    la[\"layer_norm_elem\"] = Expression(\n        name=\"layer_norm_elem\",\n        symbolic_expr=\"gamma_ln * (x - mu) / (sigma + 1e-5) + beta_ln\",\n        params={\"mu\": 0, \"sigma\": 1, \"gamma_ln\": 1, \"beta_ln\": 0},\n        trainable_params=[\"gamma_ln\", \"beta_ln\"],\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"normalization\",\n            \"description\": \"Layer normalization element\",\n            \"usage\": \"Transformers, stabilizing training\",\n            \"formula_latex\": r\"\\gamma \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta\",\n        },\n    )\n\n    la[\"batch_norm_elem\"] = Expression(\n        name=\"batch_norm_elem\",\n        symbolic_expr=\"gamma_bn * (x - mu_batch) / (sqrt(var_batch + 1e-5)) + beta_bn\",\n        params={\"mu_batch\": 0, \"var_batch\": 1, \"gamma_bn\": 1, \"beta_bn\": 0},\n        trainable_params=[\"gamma_bn\", \"beta_bn\"],\n        metadata={\n            \"category\": \"linalg\",\n            \"subcategory\": \"normalization\",\n            \"description\": \"Batch normalization element\",\n            \"usage\": \"CNNs, stabilizing deep network training\",\n            \"formula_latex\": r\"\\gamma \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\beta\",\n        },\n    )\n\n    return la\n</code></pre>"},{"location":"api/reference/#metrics","title":"Metrics","text":""},{"location":"api/reference/#neurogebra.repository.metrics.get_metrics_expressions","title":"<code>neurogebra.repository.metrics.get_metrics_expressions()</code>","text":"<p>Get dictionary of evaluation metric expressions.</p> <p>Returns:</p> Type Description <code>Dict[str, Expression]</code> <p>Dictionary mapping names to Expression instances</p> Source code in <code>src/neurogebra/repository/metrics.py</code> <pre><code>def get_metrics_expressions() -&gt; Dict[str, Expression]:\n    \"\"\"\n    Get dictionary of evaluation metric expressions.\n\n    Returns:\n        Dictionary mapping names to Expression instances\n    \"\"\"\n    met: Dict[str, Expression] = {}\n\n    # ================================================================\n    # Classification Metrics (element-wise / scalar forms)\n    # ================================================================\n\n    met[\"accuracy_elem\"] = Expression(\n        name=\"accuracy_elem\",\n        symbolic_expr=\"Piecewise((1, Eq(y_pred, y_true)), (0, True))\",\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"classification\",\n            \"description\": \"Element-wise accuracy indicator (1 if correct)\",\n            \"usage\": \"Classification evaluation, per-sample correctness\",\n            \"formula_latex\": r\"\\mathbb{1}[\\hat{y}_i = y_i]\",\n        },\n    )\n\n    met[\"precision_formula\"] = Expression(\n        name=\"precision_formula\",\n        symbolic_expr=\"tp / (tp + fp + 1e-8)\",\n        params={\"tp\": 1, \"fp\": 0},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"classification\",\n            \"description\": \"Precision = TP / (TP + FP)\",\n            \"usage\": \"When false positives are costly (spam detection)\",\n            \"formula_latex\": r\"\\text{Precision} = \\frac{TP}{TP + FP}\",\n        },\n    )\n\n    met[\"recall_formula\"] = Expression(\n        name=\"recall_formula\",\n        symbolic_expr=\"tp / (tp + fn + 1e-8)\",\n        params={\"tp\": 1, \"fn\": 0},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"classification\",\n            \"description\": \"Recall (Sensitivity) = TP / (TP + FN)\",\n            \"usage\": \"When false negatives are costly (medical diagnosis)\",\n            \"formula_latex\": r\"\\text{Recall} = \\frac{TP}{TP + FN}\",\n        },\n    )\n\n    met[\"f1_score_formula\"] = Expression(\n        name=\"f1_score_formula\",\n        symbolic_expr=\"2 * prec * rec / (prec + rec + 1e-8)\",\n        params={\"prec\": 1, \"rec\": 1},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"classification\",\n            \"description\": \"F1 score: harmonic mean of precision and recall\",\n            \"usage\": \"Balanced classification evaluation\",\n            \"formula_latex\": r\"F_1 = \\frac{2 \\cdot P \\cdot R}{P + R}\",\n        },\n    )\n\n    met[\"fbeta_score_formula\"] = Expression(\n        name=\"fbeta_score_formula\",\n        symbolic_expr=\"(1 + beta_f**2) * prec * rec / (beta_f**2 * prec + rec + 1e-8)\",\n        params={\"prec\": 1, \"rec\": 1, \"beta_f\": 1},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"classification\",\n            \"description\": \"F\u03b2 score: weighted harmonic mean (\u03b2&gt;1 favors recall)\",\n            \"usage\": \"Custom precision-recall tradeoff\",\n            \"formula_latex\": r\"F_\\beta = \\frac{(1+\\beta^2) P R}{\\beta^2 P + R}\",\n        },\n    )\n\n    met[\"specificity_formula\"] = Expression(\n        name=\"specificity_formula\",\n        symbolic_expr=\"tn / (tn + fp + 1e-8)\",\n        params={\"tn\": 1, \"fp\": 0},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"classification\",\n            \"description\": \"Specificity (True Negative Rate) = TN / (TN + FP)\",\n            \"usage\": \"Complementary to recall for binary classification\",\n            \"formula_latex\": r\"\\text{Specificity} = \\frac{TN}{TN + FP}\",\n        },\n    )\n\n    met[\"balanced_accuracy\"] = Expression(\n        name=\"balanced_accuracy\",\n        symbolic_expr=\"(sensitivity + specificity) / 2\",\n        params={\"sensitivity\": 1, \"specificity\": 1},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"classification\",\n            \"description\": \"Balanced accuracy (average of TPR and TNR)\",\n            \"usage\": \"Imbalanced datasets\",\n            \"formula_latex\": r\"\\frac{TPR + TNR}{2}\",\n        },\n    )\n\n    met[\"matthews_corr\"] = Expression(\n        name=\"matthews_corr\",\n        symbolic_expr=\"(tp * tn - fp * fn) / (sqrt((tp + fp)*(tp + fn)*(tn + fp)*(tn + fn)) + 1e-8)\",\n        params={\"tp\": 1, \"tn\": 1, \"fp\": 0, \"fn\": 0},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"classification\",\n            \"description\": \"Matthews Correlation Coefficient (MCC)\",\n            \"usage\": \"Balanced metric even with imbalanced classes\",\n            \"formula_latex\": r\"MCC = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\",\n        },\n    )\n\n    met[\"log_loss_elem\"] = Expression(\n        name=\"log_loss_elem\",\n        symbolic_expr=\"-(y * log(p + 1e-15) + (1 - y) * log(1 - p + 1e-15))\",\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"classification\",\n            \"description\": \"Element-wise log loss (binary cross-entropy)\",\n            \"usage\": \"Probabilistic classification evaluation\",\n            \"formula_latex\": r\"-[y\\ln p + (1-y)\\ln(1-p)]\",\n        },\n    )\n\n    met[\"brier_score_elem\"] = Expression(\n        name=\"brier_score_elem\",\n        symbolic_expr=\"(p - y)**2\",\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"classification\",\n            \"description\": \"Brier score element (probability calibration)\",\n            \"usage\": \"Assessing calibration of probabilistic predictions\",\n            \"formula_latex\": r\"(p_i - y_i)^2\",\n        },\n    )\n\n    # ================================================================\n    # Regression Metrics\n    # ================================================================\n\n    met[\"mse_elem\"] = Expression(\n        name=\"mse_elem\",\n        symbolic_expr=\"(y_pred - y_true)**2\",\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"regression\",\n            \"description\": \"Mean Squared Error element\",\n            \"usage\": \"Standard regression evaluation\",\n            \"formula_latex\": r\"(\\hat{y}_i - y_i)^2\",\n        },\n    )\n\n    met[\"mae_elem\"] = Expression(\n        name=\"mae_elem\",\n        symbolic_expr=\"Abs(y_pred - y_true)\",\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"regression\",\n            \"description\": \"Mean Absolute Error element\",\n            \"usage\": \"Robust regression evaluation\",\n            \"formula_latex\": r\"|\\hat{y}_i - y_i|\",\n        },\n    )\n\n    met[\"rmse_formula\"] = Expression(\n        name=\"rmse_formula\",\n        symbolic_expr=\"sqrt(mse_val)\",\n        params={\"mse_val\": 1},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"regression\",\n            \"description\": \"Root Mean Squared Error from MSE value\",\n            \"usage\": \"Regression metric in original units\",\n            \"formula_latex\": r\"RMSE = \\sqrt{MSE}\",\n        },\n    )\n\n    met[\"r_squared\"] = Expression(\n        name=\"r_squared\",\n        symbolic_expr=\"1 - ss_res / (ss_tot + 1e-8)\",\n        params={\"ss_res\": 0, \"ss_tot\": 1},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"regression\",\n            \"description\": \"R\u00b2 coefficient of determination\",\n            \"usage\": \"Proportion of variance explained by model\",\n            \"formula_latex\": r\"R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\",\n        },\n    )\n\n    met[\"adjusted_r_squared\"] = Expression(\n        name=\"adjusted_r_squared\",\n        symbolic_expr=\"1 - (1 - r2) * (n_samples - 1) / (n_samples - n_features - 1)\",\n        params={\"r2\": 0.9, \"n_samples\": 100, \"n_features\": 5},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"regression\",\n            \"description\": \"Adjusted R\u00b2 (penalizes extra features)\",\n            \"usage\": \"Feature selection, model comparison\",\n            \"formula_latex\": r\"R^2_{adj} = 1 - (1-R^2)\\frac{n-1}{n-p-1}\",\n        },\n    )\n\n    met[\"mape_elem\"] = Expression(\n        name=\"mape_elem\",\n        symbolic_expr=\"Abs((y_true - y_pred) / (y_true + 1e-8))\",\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"regression\",\n            \"description\": \"Mean Absolute Percentage Error element\",\n            \"usage\": \"Relative error metric, forecasting\",\n            \"formula_latex\": r\"\\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|\",\n        },\n    )\n\n    met[\"smape_elem\"] = Expression(\n        name=\"smape_elem\",\n        symbolic_expr=\"Abs(y_pred - y_true) / ((Abs(y_pred) + Abs(y_true)) / 2 + 1e-8)\",\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"regression\",\n            \"description\": \"Symmetric MAPE element\",\n            \"usage\": \"Balanced percentage error, avoids MAPE asymmetry\",\n            \"formula_latex\": r\"\\frac{|\\hat{y}-y|}{(|\\hat{y}|+|y|)/2}\",\n        },\n    )\n\n    met[\"huber_loss_elem\"] = Expression(\n        name=\"huber_loss_elem\",\n        symbolic_expr=\"Piecewise(((y_pred - y_true)**2 / 2, Abs(y_pred - y_true) &lt;= delta), (delta * (Abs(y_pred - y_true) - delta / 2), True))\",\n        params={\"delta\": 1.0},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"regression\",\n            \"description\": \"Huber loss element (robust to outliers)\",\n            \"usage\": \"When data has outliers, DQN training\",\n            \"formula_latex\": r\"\\begin{cases} \\frac{(y-\\hat{y})^2}{2} &amp; |y-\\hat{y}|\\le\\delta \\\\ \\delta(|y-\\hat{y}|-\\delta/2) &amp; \\text{otherwise} \\end{cases}\",\n        },\n    )\n\n    met[\"explained_variance_ratio\"] = Expression(\n        name=\"explained_variance_ratio\",\n        symbolic_expr=\"lambda_i / (total_variance + 1e-8)\",\n        params={\"lambda_i\": 1, \"total_variance\": 10},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"regression\",\n            \"description\": \"Explained variance ratio (PCA eigenvalue fraction)\",\n            \"usage\": \"PCA component importance, dimensionality reduction\",\n            \"formula_latex\": r\"\\frac{\\lambda_i}{\\sum_j \\lambda_j}\",\n        },\n    )\n\n    # ================================================================\n    # Similarity &amp; Set Metrics\n    # ================================================================\n\n    met[\"jaccard_index\"] = Expression(\n        name=\"jaccard_index\",\n        symbolic_expr=\"n_intersect / (n_union + 1e-8)\",\n        params={\"n_intersect\": 1, \"n_union\": 1},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"similarity\",\n            \"description\": \"Jaccard index (Intersection over Union)\",\n            \"usage\": \"Object detection (IoU), set similarity\",\n            \"formula_latex\": r\"J = \\frac{|A \\cap B|}{|A \\cup B|}\",\n        },\n    )\n\n    met[\"dice_coefficient\"] = Expression(\n        name=\"dice_coefficient\",\n        symbolic_expr=\"2 * n_intersect / (size_a + size_b + 1e-8)\",\n        params={\"n_intersect\": 1, \"size_a\": 1, \"size_b\": 1},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"similarity\",\n            \"description\": \"Dice coefficient (F1 for sets)\",\n            \"usage\": \"Image segmentation, medical imaging\",\n            \"formula_latex\": r\"DSC = \\frac{2|A \\cap B|}{|A| + |B|}\",\n        },\n    )\n\n    met[\"cosine_distance_metric\"] = Expression(\n        name=\"cosine_distance_metric\",\n        symbolic_expr=\"1 - dot_ab / (norm_a * norm_b + 1e-8)\",\n        params={\"dot_ab\": 1, \"norm_a\": 1, \"norm_b\": 1},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"similarity\",\n            \"description\": \"Cosine distance = 1 - cosine similarity\",\n            \"usage\": \"NLP embedding distance, recommendation\",\n            \"formula_latex\": r\"d = 1 - \\frac{\\mathbf{a}\\cdot\\mathbf{b}}{\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|}\",\n        },\n    )\n\n    # ================================================================\n    # Ranking Metrics\n    # ================================================================\n\n    met[\"reciprocal_rank\"] = Expression(\n        name=\"reciprocal_rank\",\n        symbolic_expr=\"1 / rank\",\n        params={\"rank\": 1},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"ranking\",\n            \"description\": \"Reciprocal Rank: 1/rank of first relevant result\",\n            \"usage\": \"Information retrieval, search engine evaluation\",\n            \"formula_latex\": r\"RR = \\frac{1}{\\text{rank}}\",\n        },\n    )\n\n    met[\"dcg_elem\"] = Expression(\n        name=\"dcg_elem\",\n        symbolic_expr=\"rel / log(pos + 1, 2)\",\n        params={\"rel\": 1, \"pos\": 1},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"ranking\",\n            \"description\": \"DCG element: relevance / log\u2082(position + 1)\",\n            \"usage\": \"Discounted Cumulative Gain for ranking\",\n            \"formula_latex\": r\"\\frac{rel_i}{\\log_2(i+1)}\",\n        },\n    )\n\n    met[\"ndcg_formula\"] = Expression(\n        name=\"ndcg_formula\",\n        symbolic_expr=\"dcg / (idcg + 1e-8)\",\n        params={\"dcg\": 1, \"idcg\": 1},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"ranking\",\n            \"description\": \"Normalized DCG = DCG / ideal DCG\",\n            \"usage\": \"Comparing ranked lists, search quality\",\n            \"formula_latex\": r\"nDCG = \\frac{DCG}{IDCG}\",\n        },\n    )\n\n    # ================================================================\n    # Information Criteria (Model Selection)\n    # ================================================================\n\n    met[\"aic\"] = Expression(\n        name=\"aic\",\n        symbolic_expr=\"2 * k - 2 * log_likelihood\",\n        params={\"k\": 1, \"log_likelihood\": 0},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"model_selection\",\n            \"description\": \"Akaike Information Criterion\",\n            \"usage\": \"Model selection, complexity-accuracy tradeoff\",\n            \"formula_latex\": r\"AIC = 2k - 2\\ln(\\hat{L})\",\n        },\n    )\n\n    met[\"bic\"] = Expression(\n        name=\"bic\",\n        symbolic_expr=\"k * log(n_obs) - 2 * log_likelihood\",\n        params={\"k\": 1, \"n_obs\": 100, \"log_likelihood\": 0},\n        metadata={\n            \"category\": \"metric\",\n            \"subcategory\": \"model_selection\",\n            \"description\": \"Bayesian Information Criterion\",\n            \"usage\": \"Model selection (stronger penalty for complexity)\",\n            \"formula_latex\": r\"BIC = k\\ln(n) - 2\\ln(\\hat{L})\",\n        },\n    )\n\n    return met\n</code></pre>"},{"location":"api/reference/#transforms","title":"Transforms","text":""},{"location":"api/reference/#neurogebra.repository.transforms.get_transforms_expressions","title":"<code>neurogebra.repository.transforms.get_transforms_expressions()</code>","text":"<p>Get dictionary of transform expressions.</p> <p>Returns:</p> Type Description <code>Dict[str, Expression]</code> <p>Dictionary mapping names to Expression instances</p> Source code in <code>src/neurogebra/repository/transforms.py</code> <pre><code>def get_transforms_expressions() -&gt; Dict[str, Expression]:\n    \"\"\"\n    Get dictionary of transform expressions.\n\n    Returns:\n        Dictionary mapping names to Expression instances\n    \"\"\"\n    tf: Dict[str, Expression] = {}\n\n    # ================================================================\n    # Normalization Transforms\n    # ================================================================\n\n    tf[\"min_max_normalize\"] = Expression(\n        name=\"min_max_normalize\",\n        symbolic_expr=\"(x - x_min) / (x_max - x_min + 1e-8)\",\n        params={\"x_min\": 0, \"x_max\": 1},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"normalization\",\n            \"description\": \"Min-Max normalization to [0, 1]\",\n            \"usage\": \"Feature scaling, image pixel normalization\",\n            \"formula_latex\": r\"\\frac{x - x_{min}}{x_{max} - x_{min}}\",\n        },\n    )\n\n    tf[\"min_max_to_range\"] = Expression(\n        name=\"min_max_to_range\",\n        symbolic_expr=\"a_range + (x - x_min) * (b_range - a_range) / (x_max - x_min + 1e-8)\",\n        params={\"x_min\": 0, \"x_max\": 1, \"a_range\": -1, \"b_range\": 1},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"normalization\",\n            \"description\": \"Min-Max normalization to [a, b]\",\n            \"usage\": \"Scaling to arbitrary range (e.g., [-1, 1])\",\n            \"formula_latex\": r\"a + \\frac{(x - x_{min})(b - a)}{x_{max} - x_{min}}\",\n        },\n    )\n\n    tf[\"z_score_normalize\"] = Expression(\n        name=\"z_score_normalize\",\n        symbolic_expr=\"(x - mu) / (sigma + 1e-8)\",\n        params={\"mu\": 0, \"sigma\": 1},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"normalization\",\n            \"description\": \"Z-score standardization (mean=0, std=1)\",\n            \"usage\": \"Most common preprocessing, SVM, logistic regression\",\n            \"formula_latex\": r\"\\frac{x - \\mu}{\\sigma}\",\n        },\n    )\n\n    tf[\"robust_scale\"] = Expression(\n        name=\"robust_scale\",\n        symbolic_expr=\"(x - median_val) / (iqr + 1e-8)\",\n        params={\"median_val\": 0, \"iqr\": 1},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"normalization\",\n            \"description\": \"Robust scaling (median and IQR)\",\n            \"usage\": \"Data with outliers, robust preprocessing\",\n            \"formula_latex\": r\"\\frac{x - \\text{median}}{IQR}\",\n        },\n    )\n\n    tf[\"l2_normalize\"] = Expression(\n        name=\"l2_normalize\",\n        symbolic_expr=\"x / (norm_l2 + 1e-8)\",\n        params={\"norm_l2\": 1},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"normalization\",\n            \"description\": \"L2 (unit vector) normalization\",\n            \"usage\": \"Embedding normalization, cosine similarity prep\",\n            \"formula_latex\": r\"\\frac{x}{\\|x\\|_2}\",\n        },\n    )\n\n    tf[\"max_abs_scale\"] = Expression(\n        name=\"max_abs_scale\",\n        symbolic_expr=\"x / (max_abs + 1e-8)\",\n        params={\"max_abs\": 1},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"normalization\",\n            \"description\": \"MaxAbs scaling (preserves sparsity)\",\n            \"usage\": \"Sparse data, data already centered at zero\",\n            \"formula_latex\": r\"\\frac{x}{\\max|x|}\",\n        },\n    )\n\n    # ================================================================\n    # Logarithmic &amp; Power Transforms\n    # ================================================================\n\n    tf[\"log_transform\"] = Expression(\n        name=\"log_transform\",\n        symbolic_expr=\"log(x + 1)\",\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"power\",\n            \"description\": \"Log(1+x) transform for right-skewed data\",\n            \"usage\": \"Count data, income, prices\",\n            \"formula_latex\": r\"\\ln(x + 1)\",\n        },\n    )\n\n    tf[\"log10_transform\"] = Expression(\n        name=\"log10_transform\",\n        symbolic_expr=\"log(x + 1, 10)\",\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"power\",\n            \"description\": \"Log base-10 transform\",\n            \"usage\": \"Orders of magnitude, scientific data\",\n            \"formula_latex\": r\"\\log_{10}(x + 1)\",\n        },\n    )\n\n    tf[\"sqrt_transform\"] = Expression(\n        name=\"sqrt_transform\",\n        symbolic_expr=\"sqrt(x)\",\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"power\",\n            \"description\": \"Square root transform (mild right-skew fix)\",\n            \"usage\": \"Count data, variance stabilization\",\n            \"formula_latex\": r\"\\sqrt{x}\",\n        },\n    )\n\n    tf[\"box_cox_approx\"] = Expression(\n        name=\"box_cox_approx\",\n        symbolic_expr=\"Piecewise(((x**lambda_bc - 1) / lambda_bc, lambda_bc != 0), (log(x), True))\",\n        params={\"lambda_bc\": 0.5},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"power\",\n            \"description\": \"Box-Cox transform (power family for normality)\",\n            \"usage\": \"Making data more Gaussian, regression assumption\",\n            \"formula_latex\": r\"\\frac{x^\\lambda - 1}{\\lambda}\",\n        },\n    )\n\n    tf[\"yeo_johnson_positive\"] = Expression(\n        name=\"yeo_johnson_positive\",\n        symbolic_expr=\"((x + 1)**lambda_yj - 1) / lambda_yj\",\n        params={\"lambda_yj\": 0.5},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"power\",\n            \"description\": \"Yeo-Johnson transform (positive x, \u03bb\u22600)\",\n            \"usage\": \"Handles zero and negative values (unlike Box-Cox)\",\n            \"formula_latex\": r\"\\frac{(x+1)^\\lambda - 1}{\\lambda}\",\n        },\n    )\n\n    tf[\"power_transform\"] = Expression(\n        name=\"power_transform\",\n        symbolic_expr=\"x**p_exp\",\n        params={\"p_exp\": 2.0},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"power\",\n            \"description\": \"General power transform x^p\",\n            \"usage\": \"Feature engineering, polynomial features\",\n            \"formula_latex\": r\"x^p\",\n        },\n    )\n\n    # ================================================================\n    # Activation-Style Transforms (used in preprocessing)\n    # ================================================================\n\n    tf[\"sigmoid_transform\"] = Expression(\n        name=\"sigmoid_transform\",\n        symbolic_expr=\"1 / (1 + exp(-k_sig * (x - x0_sig)))\",\n        params={\"k_sig\": 1, \"x0_sig\": 0},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"nonlinear\",\n            \"description\": \"Sigmoid squashing to (0, 1)\",\n            \"usage\": \"Probability calibration, soft thresholding\",\n            \"formula_latex\": r\"\\frac{1}{1 + e^{-k(x-x_0)}}\",\n        },\n    )\n\n    tf[\"tanh_transform\"] = Expression(\n        name=\"tanh_transform\",\n        symbolic_expr=\"tanh(x)\",\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"nonlinear\",\n            \"description\": \"Tanh squashing to (-1, 1)\",\n            \"usage\": \"Centering with bounded output\",\n            \"formula_latex\": r\"\\tanh(x)\",\n        },\n    )\n\n    tf[\"softplus_transform\"] = Expression(\n        name=\"softplus_transform\",\n        symbolic_expr=\"log(1 + exp(x))\",\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"nonlinear\",\n            \"description\": \"Softplus: smooth approximation to ReLU\",\n            \"usage\": \"Ensuring positive outputs, smooth activation\",\n            \"formula_latex\": r\"\\ln(1 + e^x)\",\n        },\n    )\n\n    tf[\"softsign_transform\"] = Expression(\n        name=\"softsign_transform\",\n        symbolic_expr=\"x / (1 + Abs(x))\",\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"nonlinear\",\n            \"description\": \"Softsign: slower saturation than tanh\",\n            \"usage\": \"Alternative to tanh, polynomial decay\",\n            \"formula_latex\": r\"\\frac{x}{1 + |x|}\",\n        },\n    )\n\n    # ================================================================\n    # Clipping &amp; Quantization\n    # ================================================================\n\n    tf[\"clip_transform\"] = Expression(\n        name=\"clip_transform\",\n        symbolic_expr=\"Max(low, Min(x, high))\",\n        params={\"low\": 0, \"high\": 1},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"clipping\",\n            \"description\": \"Clip (clamp) values to [low, high]\",\n            \"usage\": \"Gradient clipping, pixel range enforcement\",\n            \"formula_latex\": r\"\\text{clip}(x, a, b)\",\n        },\n    )\n\n    tf[\"winsorize\"] = Expression(\n        name=\"winsorize\",\n        symbolic_expr=\"Max(lower_pct, Min(x, upper_pct))\",\n        params={\"lower_pct\": -3, \"upper_pct\": 3},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"clipping\",\n            \"description\": \"Winsorize (clip to percentile bounds)\",\n            \"usage\": \"Outlier handling, robust statistics\",\n            \"formula_latex\": r\"\\text{clip}(x, q_{lo}, q_{hi})\",\n        },\n    )\n\n    # ================================================================\n    # Encoding Transforms\n    # ================================================================\n\n    tf[\"thermometer_bit\"] = Expression(\n        name=\"thermometer_bit\",\n        symbolic_expr=\"Piecewise((1, x &gt;= threshold), (0, True))\",\n        params={\"threshold\": 0.5},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"encoding\",\n            \"description\": \"Thermometer encoding bit (1 if x \u2265 threshold)\",\n            \"usage\": \"Thermometer encoding, binary discretization\",\n            \"formula_latex\": r\"\\mathbb{1}[x \\ge \\theta]\",\n        },\n    )\n\n    tf[\"gaussian_basis\"] = Expression(\n        name=\"gaussian_basis\",\n        symbolic_expr=\"exp(-((x - center)**2) / (2 * width**2))\",\n        params={\"center\": 0, \"width\": 1},\n        trainable_params=[\"center\", \"width\"],\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"encoding\",\n            \"description\": \"Gaussian radial basis function\",\n            \"usage\": \"RBF feature encoding, kernel approximation\",\n            \"formula_latex\": r\"e^{-\\frac{(x-c)^2}{2w^2}}\",\n        },\n    )\n\n    tf[\"fourier_feature_sin\"] = Expression(\n        name=\"fourier_feature_sin\",\n        symbolic_expr=\"sin(2 * pi * freq * x)\",\n        params={\"freq\": 1},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"encoding\",\n            \"description\": \"Sine component of random Fourier feature\",\n            \"usage\": \"Random Fourier features, positional encoding\",\n            \"formula_latex\": r\"\\sin(2\\pi f x)\",\n        },\n    )\n\n    tf[\"fourier_feature_cos\"] = Expression(\n        name=\"fourier_feature_cos\",\n        symbolic_expr=\"cos(2 * pi * freq * x)\",\n        params={\"freq\": 1},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"encoding\",\n            \"description\": \"Cosine component of random Fourier feature\",\n            \"usage\": \"Random Fourier features, positional encoding\",\n            \"formula_latex\": r\"\\cos(2\\pi f x)\",\n        },\n    )\n\n    tf[\"positional_enc_sin\"] = Expression(\n        name=\"positional_enc_sin\",\n        symbolic_expr=\"sin(pos / (10000**(2*i / d_model)))\",\n        params={\"pos\": 1, \"i\": 0, \"d_model\": 512},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"encoding\",\n            \"description\": \"Transformer positional encoding (sine component)\",\n            \"usage\": \"Transformer input embeddings\",\n            \"formula_latex\": r\"PE(pos,2i) = \\sin\\frac{pos}{10000^{2i/d}}\",\n        },\n    )\n\n    tf[\"positional_enc_cos\"] = Expression(\n        name=\"positional_enc_cos\",\n        symbolic_expr=\"cos(pos / (10000**(2*i / d_model)))\",\n        params={\"pos\": 1, \"i\": 0, \"d_model\": 512},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"encoding\",\n            \"description\": \"Transformer positional encoding (cosine component)\",\n            \"usage\": \"Transformer input embeddings\",\n            \"formula_latex\": r\"PE(pos,2i+1) = \\cos\\frac{pos}{10000^{2i/d}}\",\n        },\n    )\n\n    # ================================================================\n    # Signal Processing Transforms\n    # ================================================================\n\n    tf[\"moving_average_weight\"] = Expression(\n        name=\"moving_average_weight\",\n        symbolic_expr=\"1 / window_size\",\n        params={\"window_size\": 5},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"signal\",\n            \"description\": \"Uniform moving average weight\",\n            \"usage\": \"Smoothing time series, noise reduction\",\n            \"formula_latex\": r\"\\frac{1}{k}\",\n        },\n    )\n\n    tf[\"exponential_smoothing\"] = Expression(\n        name=\"exponential_smoothing\",\n        symbolic_expr=\"alpha_es * x + (1 - alpha_es) * s_prev\",\n        params={\"alpha_es\": 0.3, \"s_prev\": 0},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"signal\",\n            \"description\": \"Exponential smoothing update\",\n            \"usage\": \"Time series forecasting, EMA indicators\",\n            \"formula_latex\": r\"s_t = \\alpha x_t + (1-\\alpha) s_{t-1}\",\n        },\n    )\n\n    tf[\"difference_transform\"] = Expression(\n        name=\"difference_transform\",\n        symbolic_expr=\"x - x_prev\",\n        params={\"x_prev\": 0},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"signal\",\n            \"description\": \"First-order differencing\",\n            \"usage\": \"Making time series stationary, detrending\",\n            \"formula_latex\": r\"\\Delta x_t = x_t - x_{t-1}\",\n        },\n    )\n\n    tf[\"log_return\"] = Expression(\n        name=\"log_return\",\n        symbolic_expr=\"log(x / (x_prev + 1e-8))\",\n        params={\"x_prev\": 1},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"signal\",\n            \"description\": \"Log return (financial time series)\",\n            \"usage\": \"Stock prices, financial modeling\",\n            \"formula_latex\": r\"r_t = \\ln\\frac{P_t}{P_{t-1}}\",\n        },\n    )\n\n    # ================================================================\n    # Weight Initialization Formulas\n    # ================================================================\n\n    tf[\"xavier_uniform_bound\"] = Expression(\n        name=\"xavier_uniform_bound\",\n        symbolic_expr=\"sqrt(6 / (fan_in + fan_out))\",\n        params={\"fan_in\": 256, \"fan_out\": 256},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"initialization\",\n            \"description\": \"Xavier/Glorot uniform init bound\",\n            \"usage\": \"Linear/sigmoid layer initialization\",\n            \"formula_latex\": r\"\\text{bound} = \\sqrt{\\frac{6}{n_{in}+n_{out}}}\",\n        },\n    )\n\n    tf[\"xavier_normal_std\"] = Expression(\n        name=\"xavier_normal_std\",\n        symbolic_expr=\"sqrt(2 / (fan_in + fan_out))\",\n        params={\"fan_in\": 256, \"fan_out\": 256},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"initialization\",\n            \"description\": \"Xavier/Glorot normal init standard deviation\",\n            \"usage\": \"Linear/sigmoid layer initialization\",\n            \"formula_latex\": r\"\\sigma = \\sqrt{\\frac{2}{n_{in}+n_{out}}}\",\n        },\n    )\n\n    tf[\"he_uniform_bound\"] = Expression(\n        name=\"he_uniform_bound\",\n        symbolic_expr=\"sqrt(6 / fan_in)\",\n        params={\"fan_in\": 256},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"initialization\",\n            \"description\": \"He/Kaiming uniform init bound\",\n            \"usage\": \"ReLU layer initialization\",\n            \"formula_latex\": r\"\\text{bound} = \\sqrt{\\frac{6}{n_{in}}}\",\n        },\n    )\n\n    tf[\"he_normal_std\"] = Expression(\n        name=\"he_normal_std\",\n        symbolic_expr=\"sqrt(2 / fan_in)\",\n        params={\"fan_in\": 256},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"initialization\",\n            \"description\": \"He/Kaiming normal init standard deviation\",\n            \"usage\": \"ReLU layer initialization\",\n            \"formula_latex\": r\"\\sigma = \\sqrt{\\frac{2}{n_{in}}}\",\n        },\n    )\n\n    tf[\"lecun_normal_std\"] = Expression(\n        name=\"lecun_normal_std\",\n        symbolic_expr=\"sqrt(1 / fan_in)\",\n        params={\"fan_in\": 256},\n        metadata={\n            \"category\": \"transform\",\n            \"subcategory\": \"initialization\",\n            \"description\": \"LeCun normal init standard deviation\",\n            \"usage\": \"SELU activation, self-normalizing networks\",\n            \"formula_latex\": r\"\\sigma = \\sqrt{\\frac{1}{n_{in}}}\",\n        },\n    )\n\n    return tf\n</code></pre>"},{"location":"api/reference/#optimization","title":"Optimization","text":""},{"location":"api/reference/#neurogebra.repository.optimization.get_optimization_expressions","title":"<code>neurogebra.repository.optimization.get_optimization_expressions()</code>","text":"<p>Get dictionary of optimization expressions.</p> <p>Returns:</p> Type Description <code>Dict[str, Expression]</code> <p>Dictionary mapping names to Expression instances</p> Source code in <code>src/neurogebra/repository/optimization.py</code> <pre><code>def get_optimization_expressions() -&gt; Dict[str, Expression]:\n    \"\"\"\n    Get dictionary of optimization expressions.\n\n    Returns:\n        Dictionary mapping names to Expression instances\n    \"\"\"\n    opt: Dict[str, Expression] = {}\n\n    # ================================================================\n    # Gradient-Based Optimizer Update Rules\n    # ================================================================\n\n    opt[\"sgd_step\"] = Expression(\n        name=\"sgd_step\",\n        symbolic_expr=\"w - lr * grad\",\n        params={\"lr\": 0.01},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"optimizer\",\n            \"description\": \"Stochastic Gradient Descent: w \u2190 w - \u03b7\u2207L\",\n            \"usage\": \"Basic optimization, baseline training\",\n            \"pros\": [\"Simple\", \"Low memory\", \"Well-understood\"],\n            \"cons\": [\"Slow convergence\", \"Sensitive to learning rate\"],\n            \"formula_latex\": r\"w_{t+1} = w_t - \\eta \\nabla L\",\n        },\n    )\n\n    opt[\"momentum_step\"] = Expression(\n        name=\"momentum_step\",\n        symbolic_expr=\"w - (mu_momentum * v + lr * grad)\",\n        params={\"lr\": 0.01, \"mu_momentum\": 0.9, \"v\": 0},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"optimizer\",\n            \"description\": \"SGD with Momentum: v \u2190 \u03bcv + \u03b7\u2207L, w \u2190 w - v\",\n            \"usage\": \"Faster convergence, escaping local minima\",\n            \"pros\": [\"Accelerates in consistent gradient direction\"],\n            \"cons\": [\"Extra hyperparameter \u03bc\"],\n            \"formula_latex\": r\"v_{t+1} = \\mu v_t + \\eta \\nabla L;\\; w_{t+1} = w_t - v_{t+1}\",\n        },\n    )\n\n    opt[\"nesterov_step\"] = Expression(\n        name=\"nesterov_step\",\n        symbolic_expr=\"w - (mu_momentum * v + lr * grad_lookahead)\",\n        params={\"lr\": 0.01, \"mu_momentum\": 0.9, \"v\": 0, \"grad_lookahead\": 0},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"optimizer\",\n            \"description\": \"Nesterov Accelerated Gradient\",\n            \"usage\": \"Improved momentum, better convergence\",\n            \"pros\": [\"Look-ahead gradient correction\", \"Faster convergence\"],\n            \"formula_latex\": r\"v_{t+1} = \\mu v_t + \\eta \\nabla L(w_t - \\mu v_t);\\; w_{t+1} = w_t - v_{t+1}\",\n        },\n    )\n\n    opt[\"adagrad_step\"] = Expression(\n        name=\"adagrad_step\",\n        symbolic_expr=\"w - lr * grad / (sqrt(G + 1e-8))\",\n        params={\"lr\": 0.01, \"G\": 0},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"optimizer\",\n            \"description\": \"AdaGrad: adaptive per-parameter learning rate\",\n            \"usage\": \"Sparse data, NLP embeddings\",\n            \"pros\": [\"Adapts to parameter frequency\", \"Good for sparse gradients\"],\n            \"cons\": [\"Learning rate monotonically decreases\"],\n            \"formula_latex\": r\"w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\nabla L\",\n        },\n    )\n\n    opt[\"rmsprop_step\"] = Expression(\n        name=\"rmsprop_step\",\n        symbolic_expr=\"w - lr * grad / (sqrt(v_rms + 1e-8))\",\n        params={\"lr\": 0.001, \"v_rms\": 0, \"rho\": 0.9},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"optimizer\",\n            \"description\": \"RMSProp: running average of squared gradients\",\n            \"usage\": \"RNNs, non-stationary objectives\",\n            \"pros\": [\"Fixes AdaGrad decay\", \"Adapts to recent gradients\"],\n            \"formula_latex\": r\"v_t = \\rho v_{t-1} + (1-\\rho)(\\nabla L)^2;\\; w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{v_t+\\epsilon}} \\nabla L\",\n        },\n    )\n\n    opt[\"adam_step\"] = Expression(\n        name=\"adam_step\",\n        symbolic_expr=\"w - lr * m_hat / (sqrt(v_hat) + 1e-8)\",\n        params={\"lr\": 0.001, \"m_hat\": 0, \"v_hat\": 0},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"optimizer\",\n            \"description\": \"Adam: adaptive moments (most popular optimizer)\",\n            \"usage\": \"Default choice for deep learning\",\n            \"pros\": [\"Adaptive LR\", \"Momentum\", \"Bias correction\"],\n            \"formula_latex\": r\"w_{t+1} = w_t - \\frac{\\eta \\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\",\n        },\n    )\n\n    opt[\"adamw_step\"] = Expression(\n        name=\"adamw_step\",\n        symbolic_expr=\"w * (1 - lr * lambda_wd) - lr * m_hat / (sqrt(v_hat) + 1e-8)\",\n        params={\"lr\": 0.001, \"m_hat\": 0, \"v_hat\": 0, \"lambda_wd\": 0.01},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"optimizer\",\n            \"description\": \"AdamW: Adam with decoupled weight decay\",\n            \"usage\": \"Transformers, modern deep learning\",\n            \"pros\": [\"Better generalization than Adam\", \"Correct weight decay\"],\n            \"formula_latex\": r\"w_{t+1} = w_t(1-\\eta\\lambda) - \\frac{\\eta\\hat{m}_t}{\\sqrt{\\hat{v}_t}+\\epsilon}\",\n        },\n    )\n\n    opt[\"adam_m_update\"] = Expression(\n        name=\"adam_m_update\",\n        symbolic_expr=\"beta1 * m + (1 - beta1) * grad\",\n        params={\"beta1\": 0.9, \"m\": 0},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"optimizer\",\n            \"description\": \"Adam first moment (mean) update\",\n            \"usage\": \"Component of Adam optimizer\",\n            \"formula_latex\": r\"m_t = \\beta_1 m_{t-1} + (1-\\beta_1)\\nabla L\",\n        },\n    )\n\n    opt[\"adam_v_update\"] = Expression(\n        name=\"adam_v_update\",\n        symbolic_expr=\"beta2 * v_adam + (1 - beta2) * grad**2\",\n        params={\"beta2\": 0.999, \"v_adam\": 0},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"optimizer\",\n            \"description\": \"Adam second moment (variance) update\",\n            \"usage\": \"Component of Adam optimizer\",\n            \"formula_latex\": r\"v_t = \\beta_2 v_{t-1} + (1-\\beta_2)(\\nabla L)^2\",\n        },\n    )\n\n    opt[\"adam_bias_correction_m\"] = Expression(\n        name=\"adam_bias_correction_m\",\n        symbolic_expr=\"m / (1 - beta1**t)\",\n        params={\"beta1\": 0.9, \"m\": 0, \"t\": 1},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"optimizer\",\n            \"description\": \"Adam first moment bias correction\",\n            \"usage\": \"Component of Adam optimizer\",\n            \"formula_latex\": r\"\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}\",\n        },\n    )\n\n    opt[\"adam_bias_correction_v\"] = Expression(\n        name=\"adam_bias_correction_v\",\n        symbolic_expr=\"v_adam / (1 - beta2**t)\",\n        params={\"beta2\": 0.999, \"v_adam\": 0, \"t\": 1},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"optimizer\",\n            \"description\": \"Adam second moment bias correction\",\n            \"usage\": \"Component of Adam optimizer\",\n            \"formula_latex\": r\"\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\",\n        },\n    )\n\n    # ================================================================\n    # Learning Rate Schedules\n    # ================================================================\n\n    opt[\"constant_lr\"] = Expression(\n        name=\"constant_lr\",\n        symbolic_expr=\"lr_init\",\n        params={\"lr_init\": 0.001},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"lr_schedule\",\n            \"description\": \"Constant learning rate (baseline)\",\n            \"usage\": \"Simple training, debugging\",\n            \"formula_latex\": r\"\\eta_t = \\eta_0\",\n        },\n    )\n\n    opt[\"step_decay_lr\"] = Expression(\n        name=\"step_decay_lr\",\n        symbolic_expr=\"lr_init * gamma_decay**floor(epoch / step_size)\",\n        params={\"lr_init\": 0.01, \"gamma_decay\": 0.1, \"step_size\": 30},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"lr_schedule\",\n            \"description\": \"Step decay: reduce LR by factor every N epochs\",\n            \"usage\": \"Image classification (ResNet schedule)\",\n            \"formula_latex\": r\"\\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor t/s \\rfloor}\",\n        },\n    )\n\n    opt[\"exponential_decay_lr\"] = Expression(\n        name=\"exponential_decay_lr\",\n        symbolic_expr=\"lr_init * exp(-k_decay * epoch)\",\n        params={\"lr_init\": 0.01, \"k_decay\": 0.01},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"lr_schedule\",\n            \"description\": \"Exponential decay learning rate\",\n            \"usage\": \"Smooth LR reduction over training\",\n            \"formula_latex\": r\"\\eta_t = \\eta_0 e^{-kt}\",\n        },\n    )\n\n    opt[\"cosine_annealing_lr\"] = Expression(\n        name=\"cosine_annealing_lr\",\n        symbolic_expr=\"lr_min + (lr_max - lr_min) * (1 + cos(pi * epoch / T_max)) / 2\",\n        params={\"lr_min\": 1e-6, \"lr_max\": 0.01, \"T_max\": 100},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"lr_schedule\",\n            \"description\": \"Cosine annealing schedule (warm restarts capable)\",\n            \"usage\": \"State-of-the-art training, SGDR\",\n            \"formula_latex\": r\"\\eta_t = \\eta_{min} + \\frac{\\eta_{max}-\\eta_{min}}{2}(1+\\cos\\frac{\\pi t}{T_{max}})\",\n        },\n    )\n\n    opt[\"warmup_linear_lr\"] = Expression(\n        name=\"warmup_linear_lr\",\n        symbolic_expr=\"lr_target * Min(1, epoch / warmup_steps)\",\n        params={\"lr_target\": 0.001, \"warmup_steps\": 1000},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"lr_schedule\",\n            \"description\": \"Linear warmup (ramp up, then constant)\",\n            \"usage\": \"Transformers, large batch training\",\n            \"formula_latex\": r\"\\eta_t = \\eta_{target} \\cdot \\min(1, t/t_{warmup})\",\n        },\n    )\n\n    opt[\"polynomial_decay_lr\"] = Expression(\n        name=\"polynomial_decay_lr\",\n        symbolic_expr=\"(lr_init - lr_end) * (1 - epoch / max_epochs)**power_lr + lr_end\",\n        params={\"lr_init\": 0.01, \"lr_end\": 1e-6, \"max_epochs\": 100, \"power_lr\": 1},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"lr_schedule\",\n            \"description\": \"Polynomial decay learning rate\",\n            \"usage\": \"Object detection, semantic segmentation\",\n            \"formula_latex\": r\"\\eta_t = (\\eta_0 - \\eta_{end})(1 - t/T)^p + \\eta_{end}\",\n        },\n    )\n\n    opt[\"inverse_sqrt_lr\"] = Expression(\n        name=\"inverse_sqrt_lr\",\n        symbolic_expr=\"lr_init / sqrt(Max(epoch, warmup_steps))\",\n        params={\"lr_init\": 0.01, \"warmup_steps\": 4000},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"lr_schedule\",\n            \"description\": \"Inverse square root schedule (Transformer original)\",\n            \"usage\": \"Attention Is All You Need schedule\",\n            \"formula_latex\": r\"\\eta_t = \\frac{\\eta_0}{\\sqrt{\\max(t, t_w)}}\",\n        },\n    )\n\n    opt[\"cyclical_lr\"] = Expression(\n        name=\"cyclical_lr\",\n        symbolic_expr=\"lr_min + (lr_max - lr_min) * Abs(2 * (epoch / (2 * step_size) - floor(epoch / (2 * step_size) + Rational(1,2))))\",\n        params={\"lr_min\": 1e-4, \"lr_max\": 0.01, \"step_size\": 2000},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"lr_schedule\",\n            \"description\": \"Cyclical learning rate (triangular policy)\",\n            \"usage\": \"Super-convergence, finding optimal LR range\",\n            \"formula_latex\": r\"\\eta_t = \\eta_{min} + (\\eta_{max}-\\eta_{min}) \\cdot \\text{tri}(t)\",\n        },\n    )\n\n    opt[\"one_cycle_approx_lr\"] = Expression(\n        name=\"one_cycle_approx_lr\",\n        symbolic_expr=\"lr_max * (1 + cos(pi * epoch / T_max)) / 2\",\n        params={\"lr_max\": 0.01, \"T_max\": 100},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"lr_schedule\",\n            \"description\": \"1-cycle policy approximation (cosine phase)\",\n            \"usage\": \"Fast training with super-convergence\",\n            \"formula_latex\": r\"\\eta_t \\approx \\frac{\\eta_{max}}{2}(1 + \\cos\\frac{\\pi t}{T})\",\n        },\n    )\n\n    # ================================================================\n    # Gradient Clipping &amp; Processing\n    # ================================================================\n\n    opt[\"gradient_clip_value\"] = Expression(\n        name=\"gradient_clip_value\",\n        symbolic_expr=\"Max(Min(grad, clip_val), -clip_val)\",\n        params={\"clip_val\": 1.0},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"gradient_processing\",\n            \"description\": \"Gradient clipping by value\",\n            \"usage\": \"Prevent exploding gradients in RNNs\",\n            \"formula_latex\": r\"\\text{clip}(g, -c, c)\",\n        },\n    )\n\n    opt[\"gradient_clip_norm\"] = Expression(\n        name=\"gradient_clip_norm\",\n        symbolic_expr=\"grad * Min(1, max_norm / (grad_norm + 1e-8))\",\n        params={\"max_norm\": 1.0, \"grad_norm\": 1.0},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"gradient_processing\",\n            \"description\": \"Gradient clipping by global norm\",\n            \"usage\": \"Standard gradient clipping for deep networks\",\n            \"formula_latex\": r\"g \\cdot \\min(1, \\frac{c}{\\|g\\|})\",\n        },\n    )\n\n    opt[\"ema_update\"] = Expression(\n        name=\"ema_update\",\n        symbolic_expr=\"alpha_ema * param_new + (1 - alpha_ema) * param_ema\",\n        params={\"alpha_ema\": 0.001},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"gradient_processing\",\n            \"description\": \"Exponential Moving Average parameter update\",\n            \"usage\": \"Model averaging, Polyak averaging, EMA models\",\n            \"formula_latex\": r\"\\bar{\\theta}_t = \\alpha\\theta_t + (1-\\alpha)\\bar{\\theta}_{t-1}\",\n        },\n    )\n\n    # ================================================================\n    # Loss Landscape &amp; Convergence\n    # ================================================================\n\n    opt[\"quadratic_bowl\"] = Expression(\n        name=\"quadratic_bowl\",\n        symbolic_expr=\"a_qb * (x - x_opt)**2 + b_qb * (y - y_opt)**2\",\n        params={\"a_qb\": 1, \"b_qb\": 1, \"x_opt\": 0, \"y_opt\": 0},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"landscape\",\n            \"description\": \"Quadratic bowl (simple convex surface)\",\n            \"usage\": \"Visualizing optimizer trajectories\",\n            \"formula_latex\": r\"f(x,y) = a(x-x^*)^2 + b(y-y^*)^2\",\n        },\n    )\n\n    opt[\"rosenbrock\"] = Expression(\n        name=\"rosenbrock\",\n        symbolic_expr=\"(a_rb - x)**2 + b_rb * (y - x**2)**2\",\n        params={\"a_rb\": 1, \"b_rb\": 100},\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"landscape\",\n            \"description\": \"Rosenbrock function (banana function)\",\n            \"usage\": \"Optimizer benchmarking, non-convex optimization testing\",\n            \"formula_latex\": r\"f(x,y) = (a-x)^2 + b(y-x^2)^2\",\n        },\n    )\n\n    opt[\"rastrigin_2d\"] = Expression(\n        name=\"rastrigin_2d\",\n        symbolic_expr=\"20 + (x**2 - 10*cos(2*pi*x)) + (y**2 - 10*cos(2*pi*y))\",\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"landscape\",\n            \"description\": \"Rastrigin function (many local minima)\",\n            \"usage\": \"Global optimization algorithms, swarm intelligence\",\n            \"formula_latex\": r\"f(\\mathbf{x}) = An + \\sum[x_i^2 - A\\cos(2\\pi x_i)]\",\n        },\n    )\n\n    opt[\"ackley_2d\"] = Expression(\n        name=\"ackley_2d\",\n        symbolic_expr=\"-20 * exp(-0.2 * sqrt(0.5 * (x**2 + y**2))) - exp(0.5 * (cos(2*pi*x) + cos(2*pi*y))) + exp(1) + 20\",\n        metadata={\n            \"category\": \"optimization\",\n            \"subcategory\": \"landscape\",\n            \"description\": \"Ackley function (nearly flat outer region)\",\n            \"usage\": \"Testing optimizer escape from flat regions\",\n            \"formula_latex\": r\"-20e^{-0.2\\sqrt{0.5(x^2+y^2)}} - e^{0.5(\\cos 2\\pi x + \\cos 2\\pi y)} + e + 20\",\n        },\n    )\n\n    return opt\n</code></pre>"},{"location":"api/reference/#visualization","title":"Visualization","text":""},{"location":"api/reference/#plotting","title":"Plotting","text":""},{"location":"api/reference/#neurogebra.viz.plotting","title":"<code>neurogebra.viz.plotting</code>","text":"<p>Plotting utilities for Neurogebra.</p> <p>Provides static plotting functions for expressions using matplotlib.</p>"},{"location":"api/reference/#neurogebra.viz.plotting-classes","title":"Classes","text":""},{"location":"api/reference/#neurogebra.viz.plotting-functions","title":"Functions","text":""},{"location":"api/reference/#neurogebra.viz.plotting.plot_expression","title":"<code>plot_expression(expression, x_range=(-5, 5), n_points=500, title=None, xlabel='x', ylabel='f(x)', figsize=(8, 5), show_grid=True, show_formula=True, ax=None, **eval_kwargs)</code>","text":"<p>Plot a single expression.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>Expression</code> <p>Expression to plot</p> required <code>x_range</code> <code>Tuple[float, float]</code> <p>(min, max) range for x-axis</p> <code>(-5, 5)</code> <code>n_points</code> <code>int</code> <p>Number of points to evaluate</p> <code>500</code> <code>title</code> <code>Optional[str]</code> <p>Plot title (defaults to expression name)</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>X-axis label</p> <code>'x'</code> <code>ylabel</code> <code>str</code> <p>Y-axis label</p> <code>'f(x)'</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size</p> <code>(8, 5)</code> <code>show_grid</code> <code>bool</code> <p>Whether to show grid</p> <code>True</code> <code>show_formula</code> <code>bool</code> <p>Whether to show formula in legend</p> <code>True</code> <code>ax</code> <code>Optional[Axes]</code> <p>Optional matplotlib Axes to plot on</p> <code>None</code> <code>**eval_kwargs</code> <code>Any</code> <p>Additional keyword arguments for eval</p> <code>{}</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib Figure</p> Source code in <code>src/neurogebra/viz/plotting.py</code> <pre><code>def plot_expression(\n    expression: Expression,\n    x_range: Tuple[float, float] = (-5, 5),\n    n_points: int = 500,\n    title: Optional[str] = None,\n    xlabel: str = \"x\",\n    ylabel: str = \"f(x)\",\n    figsize: Tuple[int, int] = (8, 5),\n    show_grid: bool = True,\n    show_formula: bool = True,\n    ax: Optional[plt.Axes] = None,\n    **eval_kwargs: Any,\n) -&gt; plt.Figure:\n    \"\"\"\n    Plot a single expression.\n\n    Args:\n        expression: Expression to plot\n        x_range: (min, max) range for x-axis\n        n_points: Number of points to evaluate\n        title: Plot title (defaults to expression name)\n        xlabel: X-axis label\n        ylabel: Y-axis label\n        figsize: Figure size\n        show_grid: Whether to show grid\n        show_formula: Whether to show formula in legend\n        ax: Optional matplotlib Axes to plot on\n        **eval_kwargs: Additional keyword arguments for eval\n\n    Returns:\n        matplotlib Figure\n    \"\"\"\n    x = np.linspace(x_range[0], x_range[1], n_points)\n\n    # Evaluate expression\n    try:\n        y = np.array([expression.eval(x=xi, **eval_kwargs) for xi in x])\n    except Exception:\n        y = np.vectorize(lambda xi: expression.eval(x=xi, **eval_kwargs))(x)\n\n    # Create figure if no axes provided\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.figure\n\n    label = f\"${expression.formula}$\" if show_formula else expression.name\n    ax.plot(x, y, label=label, linewidth=2)\n\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_title(title or f\"{expression.name}\")\n    ax.legend(fontsize=9)\n\n    if show_grid:\n        ax.grid(True, alpha=0.3)\n        ax.axhline(y=0, color=\"k\", linewidth=0.5)\n        ax.axvline(x=0, color=\"k\", linewidth=0.5)\n\n    fig.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/reference/#neurogebra.viz.plotting.plot_comparison","title":"<code>plot_comparison(expressions, x_range=(-5, 5), n_points=500, title='Expression Comparison', figsize=(10, 6), show_grid=True)</code>","text":"<p>Plot multiple expressions on the same axes for comparison.</p> <p>Parameters:</p> Name Type Description Default <code>expressions</code> <code>List[Expression]</code> <p>List of Expressions to compare</p> required <code>x_range</code> <code>Tuple[float, float]</code> <p>(min, max) range for x-axis</p> <code>(-5, 5)</code> <code>n_points</code> <code>int</code> <p>Number of points</p> <code>500</code> <code>title</code> <code>str</code> <p>Plot title</p> <code>'Expression Comparison'</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size</p> <code>(10, 6)</code> <code>show_grid</code> <code>bool</code> <p>Whether to show grid</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib Figure</p> Source code in <code>src/neurogebra/viz/plotting.py</code> <pre><code>def plot_comparison(\n    expressions: List[Expression],\n    x_range: Tuple[float, float] = (-5, 5),\n    n_points: int = 500,\n    title: str = \"Expression Comparison\",\n    figsize: Tuple[int, int] = (10, 6),\n    show_grid: bool = True,\n) -&gt; plt.Figure:\n    \"\"\"\n    Plot multiple expressions on the same axes for comparison.\n\n    Args:\n        expressions: List of Expressions to compare\n        x_range: (min, max) range for x-axis\n        n_points: Number of points\n        title: Plot title\n        figsize: Figure size\n        show_grid: Whether to show grid\n\n    Returns:\n        matplotlib Figure\n    \"\"\"\n    fig, ax = plt.subplots(figsize=figsize)\n    x = np.linspace(x_range[0], x_range[1], n_points)\n\n    for expr in expressions:\n        try:\n            y = np.array([expr.eval(x=xi) for xi in x])\n            ax.plot(x, y, label=expr.name, linewidth=2)\n        except Exception as e:\n            print(f\"Warning: Could not plot {expr.name}: {e}\")\n\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"f(x)\")\n    ax.set_title(title)\n    ax.legend(fontsize=9)\n\n    if show_grid:\n        ax.grid(True, alpha=0.3)\n        ax.axhline(y=0, color=\"k\", linewidth=0.5)\n        ax.axvline(x=0, color=\"k\", linewidth=0.5)\n\n    fig.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/reference/#neurogebra.viz.plotting.plot_gradient","title":"<code>plot_gradient(expression, var='x', x_range=(-5, 5), n_points=500, figsize=(10, 5))</code>","text":"<p>Plot expression alongside its gradient.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>Expression</code> <p>Expression to plot</p> required <code>var</code> <code>str</code> <p>Variable to differentiate with respect to</p> <code>'x'</code> <code>x_range</code> <code>Tuple[float, float]</code> <p>(min, max) range for x-axis</p> <code>(-5, 5)</code> <code>n_points</code> <code>int</code> <p>Number of points</p> <code>500</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size</p> <code>(10, 5)</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib Figure</p> Source code in <code>src/neurogebra/viz/plotting.py</code> <pre><code>def plot_gradient(\n    expression: Expression,\n    var: str = \"x\",\n    x_range: Tuple[float, float] = (-5, 5),\n    n_points: int = 500,\n    figsize: Tuple[int, int] = (10, 5),\n) -&gt; plt.Figure:\n    \"\"\"\n    Plot expression alongside its gradient.\n\n    Args:\n        expression: Expression to plot\n        var: Variable to differentiate with respect to\n        x_range: (min, max) range for x-axis\n        n_points: Number of points\n        figsize: Figure size\n\n    Returns:\n        matplotlib Figure\n    \"\"\"\n    grad_expr = expression.gradient(var)\n\n    fig, axes = plt.subplots(1, 2, figsize=figsize)\n\n    x = np.linspace(x_range[0], x_range[1], n_points)\n\n    # Plot original\n    y = np.array([expression.eval(x=xi) for xi in x])\n    axes[0].plot(x, y, \"b-\", linewidth=2)\n    axes[0].set_title(f\"{expression.name}\")\n    axes[0].set_xlabel(\"x\")\n    axes[0].set_ylabel(\"f(x)\")\n    axes[0].grid(True, alpha=0.3)\n\n    # Plot gradient\n    dy = np.array([grad_expr.eval(x=xi) for xi in x])\n    axes[1].plot(x, dy, \"r-\", linewidth=2)\n    axes[1].set_title(f\"d({expression.name})/d{var}\")\n    axes[1].set_xlabel(\"x\")\n    axes[1].set_ylabel(\"f'(x)\")\n    axes[1].grid(True, alpha=0.3)\n\n    fig.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/reference/#neurogebra.viz.plotting.plot_training_history","title":"<code>plot_training_history(history, figsize=(12, 4))</code>","text":"<p>Plot training history (loss and accuracy).</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>Dict[str, List]</code> <p>Training history dict with 'loss', 'val_loss',      'accuracy', 'val_accuracy' keys</p> required <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size</p> <code>(12, 4)</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib Figure</p> Source code in <code>src/neurogebra/viz/plotting.py</code> <pre><code>def plot_training_history(\n    history: Dict[str, List],\n    figsize: Tuple[int, int] = (12, 4),\n) -&gt; plt.Figure:\n    \"\"\"\n    Plot training history (loss and accuracy).\n\n    Args:\n        history: Training history dict with 'loss', 'val_loss',\n                 'accuracy', 'val_accuracy' keys\n        figsize: Figure size\n\n    Returns:\n        matplotlib Figure\n    \"\"\"\n    has_val = bool(history.get(\"val_loss\"))\n    has_acc = bool(history.get(\"accuracy\"))\n    n_plots = 1 + int(has_acc)\n\n    fig, axes = plt.subplots(1, n_plots, figsize=figsize)\n    if n_plots == 1:\n        axes = [axes]\n\n    epochs = range(1, len(history[\"loss\"]) + 1)\n\n    # Loss plot\n    axes[0].plot(\n        epochs, history[\"loss\"], \"b-\", linewidth=2, label=\"Training Loss\"\n    )\n    if has_val:\n        axes[0].plot(\n            epochs,\n            history[\"val_loss\"],\n            \"r--\",\n            linewidth=2,\n            label=\"Validation Loss\",\n        )\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[0].set_title(\"Training Progress\")\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    # Accuracy plot\n    if has_acc:\n        axes[1].plot(\n            epochs,\n            history[\"accuracy\"],\n            \"b-\",\n            linewidth=2,\n            label=\"Training Acc\",\n        )\n        if history.get(\"val_accuracy\"):\n            axes[1].plot(\n                epochs,\n                history[\"val_accuracy\"],\n                \"r--\",\n                linewidth=2,\n                label=\"Validation Acc\",\n            )\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Accuracy\")\n        axes[1].set_title(\"Model Accuracy\")\n        axes[1].legend()\n        axes[1].grid(True, alpha=0.3)\n\n    fig.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/reference/#interactive","title":"Interactive","text":""},{"location":"api/reference/#neurogebra.viz.interactive","title":"<code>neurogebra.viz.interactive</code>","text":"<p>Interactive visualization tools for Neurogebra.</p> <p>Provides interactive plotting using plotly (optional dependency).</p>"},{"location":"api/reference/#neurogebra.viz.interactive-classes","title":"Classes","text":""},{"location":"api/reference/#neurogebra.viz.interactive-functions","title":"Functions","text":""},{"location":"api/reference/#neurogebra.viz.interactive.check_plotly","title":"<code>check_plotly()</code>","text":"<p>Raise if plotly is not installed.</p> Source code in <code>src/neurogebra/viz/interactive.py</code> <pre><code>def check_plotly():\n    \"\"\"Raise if plotly is not installed.\"\"\"\n    if not PLOTLY_AVAILABLE:\n        raise ImportError(\n            \"Plotly is required for interactive visualizations. \"\n            \"Install it with: pip install neurogebra[viz] \"\n            \"or pip install plotly\"\n        )\n</code></pre>"},{"location":"api/reference/#neurogebra.viz.interactive.interactive_plot","title":"<code>interactive_plot(expression, x_range=(-5, 5), n_points=500, title=None)</code>","text":"<p>Create an interactive plot of an expression using Plotly.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>Expression</code> <p>Expression to plot</p> required <code>x_range</code> <code>Tuple[float, float]</code> <p>(min, max) range for x values</p> <code>(-5, 5)</code> <code>n_points</code> <code>int</code> <p>Number of evaluation points</p> <code>500</code> <code>title</code> <code>Optional[str]</code> <p>Plot title</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly Figure object</p> Source code in <code>src/neurogebra/viz/interactive.py</code> <pre><code>def interactive_plot(\n    expression: Expression,\n    x_range: Tuple[float, float] = (-5, 5),\n    n_points: int = 500,\n    title: Optional[str] = None,\n) -&gt; \"go.Figure\":\n    \"\"\"\n    Create an interactive plot of an expression using Plotly.\n\n    Args:\n        expression: Expression to plot\n        x_range: (min, max) range for x values\n        n_points: Number of evaluation points\n        title: Plot title\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    check_plotly()\n\n    x = np.linspace(x_range[0], x_range[1], n_points)\n    y = np.array([expression.eval(x=xi) for xi in x])\n\n    fig = go.Figure()\n    fig.add_trace(\n        go.Scatter(\n            x=x,\n            y=y,\n            mode=\"lines\",\n            name=expression.name,\n            line=dict(width=2),\n            hovertemplate=\"x: %{x:.3f}&lt;br&gt;f(x): %{y:.3f}&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n\n    fig.update_layout(\n        title=title or f\"{expression.name}: {expression.formula}\",\n        xaxis_title=\"x\",\n        yaxis_title=\"f(x)\",\n        template=\"plotly_white\",\n        hovermode=\"x unified\",\n    )\n\n    return fig\n</code></pre>"},{"location":"api/reference/#neurogebra.viz.interactive.interactive_comparison","title":"<code>interactive_comparison(expressions, x_range=(-5, 5), n_points=500, title='Expression Comparison')</code>","text":"<p>Interactive comparison of multiple expressions.</p> <p>Parameters:</p> Name Type Description Default <code>expressions</code> <code>List[Expression]</code> <p>List of Expressions to compare</p> required <code>x_range</code> <code>Tuple[float, float]</code> <p>(min, max) range</p> <code>(-5, 5)</code> <code>n_points</code> <code>int</code> <p>Number of points</p> <code>500</code> <code>title</code> <code>str</code> <p>Plot title</p> <code>'Expression Comparison'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly Figure object</p> Source code in <code>src/neurogebra/viz/interactive.py</code> <pre><code>def interactive_comparison(\n    expressions: List[Expression],\n    x_range: Tuple[float, float] = (-5, 5),\n    n_points: int = 500,\n    title: str = \"Expression Comparison\",\n) -&gt; \"go.Figure\":\n    \"\"\"\n    Interactive comparison of multiple expressions.\n\n    Args:\n        expressions: List of Expressions to compare\n        x_range: (min, max) range\n        n_points: Number of points\n        title: Plot title\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    check_plotly()\n\n    fig = go.Figure()\n    x = np.linspace(x_range[0], x_range[1], n_points)\n\n    for expr in expressions:\n        try:\n            y = np.array([expr.eval(x=xi) for xi in x])\n            fig.add_trace(\n                go.Scatter(\n                    x=x,\n                    y=y,\n                    mode=\"lines\",\n                    name=expr.name,\n                    line=dict(width=2),\n                )\n            )\n        except Exception as e:\n            print(f\"Warning: Could not plot {expr.name}: {e}\")\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"x\",\n        yaxis_title=\"f(x)\",\n        template=\"plotly_white\",\n        hovermode=\"x unified\",\n    )\n\n    return fig\n</code></pre>"},{"location":"api/reference/#utilities","title":"Utilities","text":""},{"location":"api/reference/#helpers","title":"Helpers","text":""},{"location":"api/reference/#neurogebra.utils.helpers","title":"<code>neurogebra.utils.helpers</code>","text":"<p>Helper utilities for Neurogebra.</p> <p>Provides common utility functions used across the library.</p>"},{"location":"api/reference/#neurogebra.utils.helpers-functions","title":"Functions","text":""},{"location":"api/reference/#neurogebra.utils.helpers.validate_array","title":"<code>validate_array(data, name='input')</code>","text":"<p>Validate and convert input to numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Input data (list, tuple, numpy array, or scalar)</p> required <code>name</code> <code>str</code> <p>Name for error messages</p> <code>'input'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>NumPy array</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If data cannot be converted</p> Source code in <code>src/neurogebra/utils/helpers.py</code> <pre><code>def validate_array(data: Any, name: str = \"input\") -&gt; np.ndarray:\n    \"\"\"\n    Validate and convert input to numpy array.\n\n    Args:\n        data: Input data (list, tuple, numpy array, or scalar)\n        name: Name for error messages\n\n    Returns:\n        NumPy array\n\n    Raises:\n        TypeError: If data cannot be converted\n    \"\"\"\n    try:\n        arr = np.asarray(data, dtype=np.float64)\n        return arr\n    except (ValueError, TypeError) as e:\n        raise TypeError(f\"Cannot convert {name} to numpy array: {e}\")\n</code></pre>"},{"location":"api/reference/#neurogebra.utils.helpers.clip_gradients","title":"<code>clip_gradients(gradients, max_norm=1.0)</code>","text":"<p>Clip gradients by global norm.</p> <p>Parameters:</p> Name Type Description Default <code>gradients</code> <code>Dict[str, float]</code> <p>Dictionary of parameter name -&gt; gradient value</p> required <code>max_norm</code> <code>float</code> <p>Maximum allowed gradient norm</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Clipped gradients</p> Source code in <code>src/neurogebra/utils/helpers.py</code> <pre><code>def clip_gradients(\n    gradients: Dict[str, float],\n    max_norm: float = 1.0,\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Clip gradients by global norm.\n\n    Args:\n        gradients: Dictionary of parameter name -&gt; gradient value\n        max_norm: Maximum allowed gradient norm\n\n    Returns:\n        Clipped gradients\n    \"\"\"\n    total_norm = np.sqrt(sum(g**2 for g in gradients.values()))\n\n    if total_norm &gt; max_norm:\n        scale = max_norm / total_norm\n        return {k: v * scale for k, v in gradients.items()}\n\n    return gradients\n</code></pre>"},{"location":"api/reference/#neurogebra.utils.helpers.numerical_gradient","title":"<code>numerical_gradient(func, x, epsilon=1e-05)</code>","text":"<p>Compute numerical gradient using central differences.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Function to differentiate</p> required <code>x</code> <code>ndarray</code> <p>Point at which to compute gradient</p> required <code>epsilon</code> <code>float</code> <p>Step size for finite differences</p> <code>1e-05</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Gradient array</p> Source code in <code>src/neurogebra/utils/helpers.py</code> <pre><code>def numerical_gradient(\n    func: Callable, x: np.ndarray, epsilon: float = 1e-5\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute numerical gradient using central differences.\n\n    Args:\n        func: Function to differentiate\n        x: Point at which to compute gradient\n        epsilon: Step size for finite differences\n\n    Returns:\n        Gradient array\n    \"\"\"\n    x = np.asarray(x, dtype=np.float64)\n    grad = np.zeros_like(x)\n\n    for i in range(len(x)):\n        x_plus = x.copy()\n        x_minus = x.copy()\n        x_plus[i] += epsilon\n        x_minus[i] -= epsilon\n\n        grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n\n    return grad\n</code></pre>"},{"location":"api/reference/#neurogebra.utils.helpers.normalize","title":"<code>normalize(data, method='minmax')</code>","text":"<p>Normalize data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input array</p> required <code>method</code> <code>str</code> <p>Normalization method ('minmax', 'standard', 'l2')</p> <code>'minmax'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Normalized array</p> Source code in <code>src/neurogebra/utils/helpers.py</code> <pre><code>def normalize(data: np.ndarray, method: str = \"minmax\") -&gt; np.ndarray:\n    \"\"\"\n    Normalize data.\n\n    Args:\n        data: Input array\n        method: Normalization method ('minmax', 'standard', 'l2')\n\n    Returns:\n        Normalized array\n    \"\"\"\n    data = np.asarray(data, dtype=np.float64)\n\n    if method == \"minmax\":\n        data_min = data.min()\n        data_max = data.max()\n        if data_max - data_min == 0:\n            return np.zeros_like(data)\n        return (data - data_min) / (data_max - data_min)\n\n    elif method == \"standard\":\n        mean = data.mean()\n        std = data.std()\n        if std == 0:\n            return np.zeros_like(data)\n        return (data - mean) / std\n\n    elif method == \"l2\":\n        norm = np.linalg.norm(data)\n        if norm == 0:\n            return np.zeros_like(data)\n        return data / norm\n\n    else:\n        raise ValueError(f\"Unknown normalization method: {method}\")\n</code></pre>"},{"location":"api/reference/#neurogebra.utils.helpers.generate_data","title":"<code>generate_data(func, x_range=(-5, 5), n_points=100, noise_std=0.0, seed=None)</code>","text":"<p>Generate synthetic data from a function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Function to generate data from</p> required <code>x_range</code> <code>tuple</code> <p>(min, max) range for x values</p> <code>(-5, 5)</code> <code>n_points</code> <code>int</code> <p>Number of data points</p> <code>100</code> <code>noise_std</code> <code>float</code> <p>Standard deviation of Gaussian noise</p> <code>0.0</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of (X, y) arrays</p> Source code in <code>src/neurogebra/utils/helpers.py</code> <pre><code>def generate_data(\n    func: Callable,\n    x_range: tuple = (-5, 5),\n    n_points: int = 100,\n    noise_std: float = 0.0,\n    seed: Optional[int] = None,\n) -&gt; tuple:\n    \"\"\"\n    Generate synthetic data from a function.\n\n    Args:\n        func: Function to generate data from\n        x_range: (min, max) range for x values\n        n_points: Number of data points\n        noise_std: Standard deviation of Gaussian noise\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (X, y) arrays\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    X = np.linspace(x_range[0], x_range[1], n_points)\n    y = np.array([func(x) for x in X], dtype=np.float64)\n\n    if noise_std &gt; 0:\n        y += np.random.normal(0, noise_std, size=n_points)\n\n    return X, y\n</code></pre>"},{"location":"api/reference/#explain","title":"Explain","text":""},{"location":"api/reference/#neurogebra.utils.explain.ExpressionExplainer","title":"<code>neurogebra.utils.explain.ExpressionExplainer</code>","text":"<p>Generates detailed explanations of mathematical expressions.</p> <p>Supports multiple difficulty levels and output formats.</p> Source code in <code>src/neurogebra/utils/explain.py</code> <pre><code>class ExpressionExplainer:\n    \"\"\"\n    Generates detailed explanations of mathematical expressions.\n\n    Supports multiple difficulty levels and output formats.\n    \"\"\"\n\n    @staticmethod\n    def explain(\n        expression: Expression,\n        level: str = \"intermediate\",\n        format: str = \"text\",\n    ) -&gt; str:\n        \"\"\"\n        Generate explanation for an expression.\n\n        Args:\n            expression: Expression to explain\n            level: Detail level ('beginner', 'intermediate', 'advanced')\n            format: Output format ('text', 'markdown', 'latex')\n\n        Returns:\n            Explanation string\n        \"\"\"\n        if format == \"markdown\":\n            return ExpressionExplainer._explain_markdown(expression, level)\n        elif format == \"latex\":\n            return ExpressionExplainer._explain_latex(expression, level)\n        else:\n            return expression.explain(level=level)\n\n    @staticmethod\n    def _explain_markdown(expression: Expression, level: str) -&gt; str:\n        \"\"\"Generate Markdown-formatted explanation.\"\"\"\n        lines = []\n        lines.append(f\"## {expression.name}\")\n        lines.append(\"\")\n        lines.append(f\"**Formula:** ${expression.formula}$\")\n        lines.append(\"\")\n\n        if \"description\" in expression.metadata:\n            lines.append(f\"**Description:** {expression.metadata['description']}\")\n            lines.append(\"\")\n\n        if \"usage\" in expression.metadata:\n            lines.append(f\"**Usage:** {expression.metadata['usage']}\")\n            lines.append(\"\")\n\n        if expression.params:\n            lines.append(\"**Parameters:**\")\n            for k, v in expression.params.items():\n                lines.append(f\"- `{k}` = {v}\")\n            lines.append(\"\")\n\n        if level in (\"intermediate\", \"advanced\"):\n            if \"pros\" in expression.metadata:\n                lines.append(\"**Advantages:**\")\n                for pro in expression.metadata[\"pros\"]:\n                    lines.append(f\"- \u2705 {pro}\")\n                lines.append(\"\")\n\n            if \"cons\" in expression.metadata:\n                lines.append(\"**Disadvantages:**\")\n                for con in expression.metadata[\"cons\"]:\n                    lines.append(f\"- \u26a0\ufe0f {con}\")\n                lines.append(\"\")\n\n        if level == \"advanced\":\n            lines.append(\"**Derivatives:**\")\n            for var in expression.variables:\n                grad = sp.diff(expression.symbolic_expr, var)\n                lines.append(f\"- $\\\\frac{{\\\\partial}}{{\\\\partial {var}}} = {sp.latex(grad)}$\")\n            lines.append(\"\")\n\n        return \"\\n\".join(lines)\n\n    @staticmethod\n    def _explain_latex(expression: Expression, level: str) -&gt; str:\n        \"\"\"Generate LaTeX-formatted explanation.\"\"\"\n        lines = []\n        lines.append(f\"\\\\section{{{expression.name}}}\")\n        lines.append(f\"\\\\[{expression.formula}\\\\]\")\n        lines.append(\"\")\n\n        if \"description\" in expression.metadata:\n            lines.append(f\"{expression.metadata['description']}\")\n            lines.append(\"\")\n\n        if level == \"advanced\":\n            lines.append(\"\\\\subsection{Derivatives}\")\n            for var in expression.variables:\n                grad = sp.diff(expression.symbolic_expr, var)\n                lines.append(\n                    f\"\\\\[\\\\frac{{\\\\partial}}{{\\\\partial {var}}} = {sp.latex(grad)}\\\\]\"\n                )\n\n        return \"\\n\".join(lines)\n\n    @staticmethod\n    def compare_expressions(\n        expressions: list,\n        format: str = \"text\",\n    ) -&gt; str:\n        \"\"\"\n        Compare multiple expressions.\n\n        Args:\n            expressions: List of Expression instances\n            format: Output format\n\n        Returns:\n            Comparison string\n        \"\"\"\n        if format == \"markdown\":\n            lines = [\"| Name | Formula | Category |\", \"|------|---------|----------|\"]\n            for expr in expressions:\n                cat = expr.metadata.get(\"category\", \"N/A\")\n                formula = str(expr.symbolic_expr)[:30]\n                lines.append(f\"| {expr.name} | ${formula}$ | {cat} |\")\n            return \"\\n\".join(lines)\n        else:\n            lines = [f\"{'Name':&lt;20} {'Formula':&lt;40} {'Category':&lt;15}\"]\n            lines.append(\"-\" * 75)\n            for expr in expressions:\n                cat = expr.metadata.get(\"category\", \"N/A\")\n                formula = str(expr.symbolic_expr)[:38]\n                lines.append(f\"{expr.name:&lt;20} {formula:&lt;40} {cat:&lt;15}\")\n            return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/reference/#neurogebra.utils.explain.ExpressionExplainer-functions","title":"Functions","text":""},{"location":"api/reference/#neurogebra.utils.explain.ExpressionExplainer.explain","title":"<code>explain(expression, level='intermediate', format='text')</code>  <code>staticmethod</code>","text":"<p>Generate explanation for an expression.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>Expression</code> <p>Expression to explain</p> required <code>level</code> <code>str</code> <p>Detail level ('beginner', 'intermediate', 'advanced')</p> <code>'intermediate'</code> <code>format</code> <code>str</code> <p>Output format ('text', 'markdown', 'latex')</p> <code>'text'</code> <p>Returns:</p> Type Description <code>str</code> <p>Explanation string</p> Source code in <code>src/neurogebra/utils/explain.py</code> <pre><code>@staticmethod\ndef explain(\n    expression: Expression,\n    level: str = \"intermediate\",\n    format: str = \"text\",\n) -&gt; str:\n    \"\"\"\n    Generate explanation for an expression.\n\n    Args:\n        expression: Expression to explain\n        level: Detail level ('beginner', 'intermediate', 'advanced')\n        format: Output format ('text', 'markdown', 'latex')\n\n    Returns:\n        Explanation string\n    \"\"\"\n    if format == \"markdown\":\n        return ExpressionExplainer._explain_markdown(expression, level)\n    elif format == \"latex\":\n        return ExpressionExplainer._explain_latex(expression, level)\n    else:\n        return expression.explain(level=level)\n</code></pre>"},{"location":"api/reference/#neurogebra.utils.explain.ExpressionExplainer.compare_expressions","title":"<code>compare_expressions(expressions, format='text')</code>  <code>staticmethod</code>","text":"<p>Compare multiple expressions.</p> <p>Parameters:</p> Name Type Description Default <code>expressions</code> <code>list</code> <p>List of Expression instances</p> required <code>format</code> <code>str</code> <p>Output format</p> <code>'text'</code> <p>Returns:</p> Type Description <code>str</code> <p>Comparison string</p> Source code in <code>src/neurogebra/utils/explain.py</code> <pre><code>@staticmethod\ndef compare_expressions(\n    expressions: list,\n    format: str = \"text\",\n) -&gt; str:\n    \"\"\"\n    Compare multiple expressions.\n\n    Args:\n        expressions: List of Expression instances\n        format: Output format\n\n    Returns:\n        Comparison string\n    \"\"\"\n    if format == \"markdown\":\n        lines = [\"| Name | Formula | Category |\", \"|------|---------|----------|\"]\n        for expr in expressions:\n            cat = expr.metadata.get(\"category\", \"N/A\")\n            formula = str(expr.symbolic_expr)[:30]\n            lines.append(f\"| {expr.name} | ${formula}$ | {cat} |\")\n        return \"\\n\".join(lines)\n    else:\n        lines = [f\"{'Name':&lt;20} {'Formula':&lt;40} {'Category':&lt;15}\"]\n        lines.append(\"-\" * 75)\n        for expr in expressions:\n            cat = expr.metadata.get(\"category\", \"N/A\")\n            formula = str(expr.symbolic_expr)[:38]\n            lines.append(f\"{expr.name:&lt;20} {formula:&lt;40} {cat:&lt;15}\")\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/reference/#framework-bridges","title":"Framework Bridges","text":"<p>Convert Neurogebra expressions to production frameworks.</p>"},{"location":"api/reference/#pytorch-bridge","title":"PyTorch Bridge","text":""},{"location":"api/reference/#neurogebra.bridges.pytorch_bridge","title":"<code>neurogebra.bridges.pytorch_bridge</code>","text":"<p>PyTorch bridge for Neurogebra expressions.</p> <p>Converts Neurogebra expressions to PyTorch-compatible functions.</p>"},{"location":"api/reference/#neurogebra.bridges.pytorch_bridge-classes","title":"Classes","text":""},{"location":"api/reference/#neurogebra.bridges.pytorch_bridge-functions","title":"Functions","text":""},{"location":"api/reference/#neurogebra.bridges.pytorch_bridge.check_torch","title":"<code>check_torch()</code>","text":"<p>Raise if PyTorch is not installed.</p> Source code in <code>src/neurogebra/bridges/pytorch_bridge.py</code> <pre><code>def check_torch():\n    \"\"\"Raise if PyTorch is not installed.\"\"\"\n    if not TORCH_AVAILABLE:\n        raise ImportError(\n            \"PyTorch is required for this feature. \"\n            \"Install it with: pip install neurogebra[frameworks] \"\n            \"or pip install torch\"\n        )\n</code></pre>"},{"location":"api/reference/#neurogebra.bridges.pytorch_bridge.to_pytorch","title":"<code>to_pytorch(expression)</code>","text":"<p>Convert a Neurogebra Expression to a PyTorch nn.Module.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>Expression</code> <p>Neurogebra Expression instance</p> required <p>Returns:</p> Type Description <code>Module</code> <p>PyTorch nn.Module that implements the expression</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If PyTorch is not installed</p> Source code in <code>src/neurogebra/bridges/pytorch_bridge.py</code> <pre><code>def to_pytorch(expression: Expression) -&gt; \"nn.Module\":\n    \"\"\"\n    Convert a Neurogebra Expression to a PyTorch nn.Module.\n\n    Args:\n        expression: Neurogebra Expression instance\n\n    Returns:\n        PyTorch nn.Module that implements the expression\n\n    Raises:\n        ImportError: If PyTorch is not installed\n    \"\"\"\n    check_torch()\n\n    class ExpressionModule(nn.Module):\n        def __init__(self, expr: Expression):\n            super().__init__()\n            self.expr_name = expr.name\n            self._sympy_expr = expr.symbolic_expr\n\n            # Create trainable parameters\n            for param_name in expr.trainable_params:\n                if param_name in expr.params:\n                    value = float(expr.params[param_name])\n                    setattr(\n                        self,\n                        param_name,\n                        nn.Parameter(torch.tensor(value)),\n                    )\n\n        def forward(self, x: \"torch.Tensor\") -&gt; \"torch.Tensor\":\n            # Use numpy for evaluation, convert back\n            x_np = x.detach().cpu().numpy()\n            result = np.vectorize(\n                lambda val: float(\n                    self._sympy_expr.subs(\"x\", val)\n                )\n            )(x_np)\n            return torch.tensor(result, dtype=x.dtype, device=x.device)\n\n    return ExpressionModule(expression)\n</code></pre>"},{"location":"api/reference/#neurogebra.bridges.pytorch_bridge.from_pytorch","title":"<code>from_pytorch(module, name='pytorch_expr')</code>","text":"<p>Create a Neurogebra Expression from a simple PyTorch activation.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>PyTorch module (e.g., nn.ReLU())</p> required <code>name</code> <code>str</code> <p>Name for the expression</p> <code>'pytorch_expr'</code> <p>Returns:</p> Type Description <code>Expression</code> <p>Neurogebra Expression</p> Note <p>This creates a numerical-only expression (no symbolic form).</p> Source code in <code>src/neurogebra/bridges/pytorch_bridge.py</code> <pre><code>def from_pytorch(module: \"nn.Module\", name: str = \"pytorch_expr\") -&gt; Expression:\n    \"\"\"\n    Create a Neurogebra Expression from a simple PyTorch activation.\n\n    Args:\n        module: PyTorch module (e.g., nn.ReLU())\n        name: Name for the expression\n\n    Returns:\n        Neurogebra Expression\n\n    Note:\n        This creates a numerical-only expression (no symbolic form).\n    \"\"\"\n    check_torch()\n\n    # Map known PyTorch modules to symbolic expressions\n    module_map = {\n        \"ReLU\": \"Max(0, x)\",\n        \"Sigmoid\": \"1 / (1 + exp(-x))\",\n        \"Tanh\": \"tanh(x)\",\n        \"Softplus\": \"log(1 + exp(x))\",\n        \"ELU\": \"Piecewise((x, x &gt; 0), (exp(x) - 1, True))\",\n    }\n\n    class_name = module.__class__.__name__\n\n    if class_name in module_map:\n        return Expression(\n            name=name,\n            symbolic_expr=module_map[class_name],\n            metadata={\"source\": \"pytorch\", \"original_class\": class_name},\n        )\n\n    # Fallback: create expression from numerical evaluation\n    return Expression(\n        name=name,\n        symbolic_expr=\"x\",  # Placeholder\n        metadata={\n            \"source\": \"pytorch\",\n            \"original_class\": class_name,\n            \"warning\": \"Symbolic form not available, using placeholder\",\n        },\n    )\n</code></pre>"},{"location":"api/reference/#tensorflow-bridge","title":"TensorFlow Bridge","text":""},{"location":"api/reference/#neurogebra.bridges.tensorflow_bridge","title":"<code>neurogebra.bridges.tensorflow_bridge</code>","text":"<p>TensorFlow bridge for Neurogebra expressions.</p> <p>Converts Neurogebra expressions to TensorFlow-compatible functions.</p>"},{"location":"api/reference/#neurogebra.bridges.tensorflow_bridge-classes","title":"Classes","text":""},{"location":"api/reference/#neurogebra.bridges.tensorflow_bridge-functions","title":"Functions","text":""},{"location":"api/reference/#neurogebra.bridges.tensorflow_bridge.check_tensorflow","title":"<code>check_tensorflow()</code>","text":"<p>Raise if TensorFlow is not installed.</p> Source code in <code>src/neurogebra/bridges/tensorflow_bridge.py</code> <pre><code>def check_tensorflow():\n    \"\"\"Raise if TensorFlow is not installed.\"\"\"\n    if not TF_AVAILABLE:\n        raise ImportError(\n            \"TensorFlow is required for this feature. \"\n            \"Install it with: pip install neurogebra[frameworks] \"\n            \"or pip install tensorflow\"\n        )\n</code></pre>"},{"location":"api/reference/#neurogebra.bridges.tensorflow_bridge.to_tensorflow","title":"<code>to_tensorflow(expression)</code>","text":"<p>Convert a Neurogebra Expression to a TensorFlow function.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>Expression</code> <p>Neurogebra Expression instance</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>TensorFlow-compatible function</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If TensorFlow is not installed</p> Source code in <code>src/neurogebra/bridges/tensorflow_bridge.py</code> <pre><code>def to_tensorflow(expression: Expression) -&gt; Callable:\n    \"\"\"\n    Convert a Neurogebra Expression to a TensorFlow function.\n\n    Args:\n        expression: Neurogebra Expression instance\n\n    Returns:\n        TensorFlow-compatible function\n\n    Raises:\n        ImportError: If TensorFlow is not installed\n    \"\"\"\n    check_tensorflow()\n\n    @tf.function\n    def tf_expression(x):\n        x_np = x.numpy()\n        result = np.vectorize(\n            lambda val: float(expression.symbolic_expr.subs(\"x\", val))\n        )(x_np)\n        return tf.constant(result, dtype=x.dtype)\n\n    return tf_expression\n</code></pre>"},{"location":"api/reference/#neurogebra.bridges.tensorflow_bridge.to_keras_layer","title":"<code>to_keras_layer(expression, name=None)</code>","text":"<p>Convert a Neurogebra Expression to a Keras Layer.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>Expression</code> <p>Neurogebra Expression instance</p> required <code>name</code> <code>Optional[str]</code> <p>Optional layer name</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Keras Lambda layer</p> Source code in <code>src/neurogebra/bridges/tensorflow_bridge.py</code> <pre><code>def to_keras_layer(expression: Expression, name: Optional[str] = None) -&gt; Any:\n    \"\"\"\n    Convert a Neurogebra Expression to a Keras Layer.\n\n    Args:\n        expression: Neurogebra Expression instance\n        name: Optional layer name\n\n    Returns:\n        Keras Lambda layer\n    \"\"\"\n    check_tensorflow()\n\n    layer_name = name or f\"neurogebra_{expression.name}\"\n\n    def layer_fn(x):\n        # Use numpy callback for evaluation\n        return tf.numpy_function(\n            lambda x_np: np.vectorize(\n                lambda val: float(expression.symbolic_expr.subs(\"x\", val))\n            )(x_np).astype(np.float32),\n            [x],\n            tf.float32,\n        )\n\n    return tf.keras.layers.Lambda(layer_fn, name=layer_name)\n</code></pre>"},{"location":"api/reference/#jax-bridge","title":"JAX Bridge","text":""},{"location":"api/reference/#neurogebra.bridges.jax_bridge","title":"<code>neurogebra.bridges.jax_bridge</code>","text":"<p>JAX bridge for Neurogebra expressions.</p> <p>Converts Neurogebra expressions to JAX-compatible functions.</p>"},{"location":"api/reference/#neurogebra.bridges.jax_bridge-classes","title":"Classes","text":""},{"location":"api/reference/#neurogebra.bridges.jax_bridge-functions","title":"Functions","text":""},{"location":"api/reference/#neurogebra.bridges.jax_bridge.check_jax","title":"<code>check_jax()</code>","text":"<p>Raise if JAX is not installed.</p> Source code in <code>src/neurogebra/bridges/jax_bridge.py</code> <pre><code>def check_jax():\n    \"\"\"Raise if JAX is not installed.\"\"\"\n    if not JAX_AVAILABLE:\n        raise ImportError(\n            \"JAX is required for this feature. \"\n            \"Install it with: pip install jax jaxlib\"\n        )\n</code></pre>"},{"location":"api/reference/#neurogebra.bridges.jax_bridge.to_jax","title":"<code>to_jax(expression)</code>","text":"<p>Convert a Neurogebra Expression to a JAX-compatible function.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>Expression</code> <p>Neurogebra Expression instance</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>JAX-compatible function that can be used with jit, grad, etc.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If JAX is not installed</p> Source code in <code>src/neurogebra/bridges/jax_bridge.py</code> <pre><code>def to_jax(expression: Expression) -&gt; Callable:\n    \"\"\"\n    Convert a Neurogebra Expression to a JAX-compatible function.\n\n    Args:\n        expression: Neurogebra Expression instance\n\n    Returns:\n        JAX-compatible function that can be used with jit, grad, etc.\n\n    Raises:\n        ImportError: If JAX is not installed\n    \"\"\"\n    check_jax()\n\n    def jax_fn(x):\n        # Convert to numpy, evaluate, convert back\n        x_np = np.asarray(x)\n        result = np.vectorize(\n            lambda val: float(expression.symbolic_expr.subs(\"x\", val))\n        )(x_np)\n        return jnp.array(result)\n\n    return jax_fn\n</code></pre>"},{"location":"api/reference/#neurogebra.bridges.jax_bridge.to_jax_grad","title":"<code>to_jax_grad(expression, var='x')</code>","text":"<p>Convert expression and return its gradient as a JAX function.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>Expression</code> <p>Neurogebra Expression instance</p> required <code>var</code> <code>str</code> <p>Variable to differentiate with respect to</p> <code>'x'</code> <p>Returns:</p> Type Description <code>Callable</code> <p>JAX function computing the gradient</p> Source code in <code>src/neurogebra/bridges/jax_bridge.py</code> <pre><code>def to_jax_grad(expression: Expression, var: str = \"x\") -&gt; Callable:\n    \"\"\"\n    Convert expression and return its gradient as a JAX function.\n\n    Args:\n        expression: Neurogebra Expression instance\n        var: Variable to differentiate with respect to\n\n    Returns:\n        JAX function computing the gradient\n    \"\"\"\n    check_jax()\n\n    grad_expr = expression.gradient(var)\n    return to_jax(grad_expr)\n</code></pre>"},{"location":"examples/custom_activation/","title":"Custom Activation Functions","text":"<p>Learn how to create and use custom activation functions with Neurogebra.</p>"},{"location":"examples/custom_activation/#creating-a-custom-activation","title":"Creating a Custom Activation","text":"<pre><code>from neurogebra import Expression, MathForge\n\n# Define a custom activation\ncustom_act = Expression(\n    name=\"my_activation\",\n    symbolic_expr=\"x * tanh(x)\",\n    metadata={\n        \"category\": \"activation\",\n        \"description\": \"Custom x*tanh(x) activation function\",\n        \"usage\": \"Alternative non-monotonic activation\",\n        \"pros\": [\"Smooth\", \"Non-monotonic\"],\n        \"cons\": [\"More expensive than ReLU\"],\n    }\n)\n\n# Use it directly\nresult = custom_act.eval(x=2.0)\nprint(f\"my_activation(2.0) = {result}\")\n\n# Get its gradient\ngrad = custom_act.gradient(\"x\")\nprint(f\"Gradient formula: {grad.symbolic_expr}\")\n</code></pre>"},{"location":"examples/custom_activation/#registering-with-mathforge","title":"Registering with MathForge","text":"<pre><code>forge = MathForge()\nforge.register(\"my_activation\", custom_act)\n\n# Now it's searchable and accessible\nretrieved = forge.get(\"my_activation\")\nprint(retrieved.explain())\n</code></pre>"},{"location":"examples/custom_activation/#parametric-activations","title":"Parametric Activations","text":"<pre><code># Activation with learnable parameter\nparametric = Expression(\n    name=\"parametric_relu\",\n    symbolic_expr=\"Max(alpha * x, x)\",\n    params={\"alpha\": 0.25},\n    trainable_params=[\"alpha\"],\n    metadata={\n        \"category\": \"activation\",\n        \"description\": \"Parametric ReLU with learnable slope\",\n    }\n)\n\n# Use with custom alpha\nresult = parametric.eval(x=-4)  # alpha * (-4) = 0.25 * -4 = -1.0\n</code></pre>"},{"location":"examples/custom_activation/#comparing-activations","title":"Comparing Activations","text":"<pre><code>from neurogebra.viz.plotting import plot_comparison\n\nforge = MathForge()\n\nactivations = [\n    forge.get(\"relu\"),\n    forge.get(\"sigmoid\"),\n    forge.get(\"swish\"),\n    custom_act,\n]\n\nfig = plot_comparison(activations, x_range=(-3, 3))\n</code></pre>"},{"location":"examples/custom_loss/","title":"Custom Loss Functions","text":"<p>Learn how to create and use custom loss functions with Neurogebra.</p>"},{"location":"examples/custom_loss/#creating-a-custom-loss","title":"Creating a Custom Loss","text":"<pre><code>from neurogebra import Expression, MathForge\n\n# Focal Loss for class imbalance\nfocal_loss = Expression(\n    name=\"focal_loss\",\n    symbolic_expr=\"-alpha * (1 - y_pred)**gamma * y_true * log(y_pred)\",\n    params={\"alpha\": 0.25, \"gamma\": 2.0},\n    metadata={\n        \"category\": \"loss\",\n        \"description\": \"Focal Loss for addressing class imbalance\",\n        \"usage\": \"Object detection, imbalanced classification\",\n    }\n)\n\n# Evaluate\nresult = focal_loss.eval(y_pred=0.9, y_true=1.0)\nprint(f\"Focal loss: {result}\")\n</code></pre>"},{"location":"examples/custom_loss/#composing-loss-functions","title":"Composing Loss Functions","text":"<pre><code>forge = MathForge()\n\n# Weighted combination\nmse = forge.get(\"mse\")\nmae = forge.get(\"mae\")\n\n# Hybrid loss\nhybrid = 0.8 * mse + 0.2 * mae\n\n# String-based composition\ncomposed = forge.compose(\"mse + 0.1*mae\")\n</code></pre>"},{"location":"examples/custom_loss/#loss-with-regularization","title":"Loss with Regularization","text":"<pre><code>from neurogebra.repository.regularizers import get_regularizers\n\nregs = get_regularizers()\nl2 = regs[\"l2\"]\n\n# Regularized loss\nmse = forge.get(\"mse\")\nregularized_loss = mse + 0.01 * l2\n</code></pre>"},{"location":"examples/custom_loss/#understanding-losses","title":"Understanding Losses","text":"<pre><code>forge = MathForge()\n\n# Compare MSE vs MAE\nprint(forge.explain(\"mse\", level=\"advanced\"))\nprint(forge.explain(\"mae\", level=\"advanced\"))\nprint(forge.explain(\"huber\", level=\"advanced\"))\n\n# Visual comparison\nfrom neurogebra.viz.plotting import plot_comparison\n\nlosses = [forge.get(\"mse\"), forge.get(\"mae\")]\nfig = plot_comparison(losses, x_range=(-3, 3))\n</code></pre>"},{"location":"examples/training_expressions/","title":"Training Expressions","text":"<p>Learn how to train mathematical expressions to fit data.</p>"},{"location":"examples/training_expressions/#basic-training","title":"Basic Training","text":"<pre><code>import numpy as np\nfrom neurogebra import Expression\nfrom neurogebra.core.trainer import Trainer\n\n# Create trainable linear expression\nexpr = Expression(\n    \"line\",\n    \"m*x + b\",\n    params={\"m\": 0.0, \"b\": 0.0},\n    trainable_params=[\"m\", \"b\"]\n)\n\n# Generate data\nX = np.linspace(0, 10, 100)\ny = 2.5 * X + 3.0 + np.random.normal(0, 0.5, 100)\n\n# Train\ntrainer = Trainer(expr, learning_rate=0.001)\nhistory = trainer.fit(X, y, epochs=500)\n\nprint(f\"Learned: m={expr.params['m']:.3f} (true: 2.5)\")\nprint(f\"Learned: b={expr.params['b']:.3f} (true: 3.0)\")\n</code></pre>"},{"location":"examples/training_expressions/#training-a-quadratic","title":"Training a Quadratic","text":"<pre><code>expr = Expression(\n    \"quad\",\n    \"a*x**2 + b*x + c\",\n    params={\"a\": 0.0, \"b\": 0.0, \"c\": 0.0},\n    trainable_params=[\"a\", \"b\", \"c\"]\n)\n\nX = np.linspace(-5, 5, 200)\ny = 0.5 * X**2 - 2 * X + 1\n\ntrainer = Trainer(expr, learning_rate=0.0001)\nhistory = trainer.fit(X, y, epochs=1000)\n</code></pre>"},{"location":"examples/training_expressions/#using-adam-optimizer","title":"Using Adam Optimizer","text":"<pre><code>trainer = Trainer(expr, learning_rate=0.01, optimizer=\"adam\")\nhistory = trainer.fit(X, y, epochs=200)\n</code></pre>"},{"location":"examples/training_expressions/#visualizing-training","title":"Visualizing Training","text":"<pre><code>from neurogebra.viz.plotting import plot_training_history, plot_expression\n\n# Plot loss curve\nfig = plot_training_history(history)\n\n# Plot fitted expression\nfig = plot_expression(expr, x_range=(-5, 5))\n</code></pre>"},{"location":"examples/training_expressions/#using-callbacks","title":"Using Callbacks","text":"<pre><code>def my_callback(epoch, loss, params):\n    if epoch % 50 == 0:\n        print(f\"Epoch {epoch}: loss={loss:.4f}, params={params}\")\n\ntrainer = Trainer(expr, learning_rate=0.01)\nhistory = trainer.fit(X, y, epochs=300, callback=my_callback)\n</code></pre>"},{"location":"examples/training_expressions/#tips-for-training","title":"Tips for Training","text":"<ol> <li>Start with small learning rates (0.001 or less)</li> <li>Scale your data - normalize X and y for better convergence</li> <li>Use Adam for faster convergence on complex expressions</li> <li>Monitor the loss - if it oscillates, reduce learning rate</li> <li>Try different initializations - parameters start at their initial values</li> </ol>"},{"location":"getting-started/first-program/","title":"Your First Program","text":"<p>Let's write your very first Neurogebra program. By the end of this page, you'll understand the core concept of the library.</p>"},{"location":"getting-started/first-program/#the-big-idea","title":"The Big Idea","text":"<p>In Machine Learning, everything is math. Neural networks use:</p> <ul> <li>Activation functions (like ReLU, Sigmoid) to add non-linearity</li> <li>Loss functions (like MSE, Cross-Entropy) to measure errors</li> <li>Gradients (derivatives) to learn from mistakes</li> </ul> <p>Neurogebra gives you all of these as readable, explainable mathematical expressions.</p>"},{"location":"getting-started/first-program/#step-1-import-neurogebra","title":"Step 1: Import Neurogebra","text":"<pre><code>from neurogebra import MathForge\n</code></pre> <p><code>MathForge</code> is your main tool. Think of it as a toolbox that contains every mathematical function used in ML.</p>"},{"location":"getting-started/first-program/#step-2-create-a-mathforge-instance","title":"Step 2: Create a MathForge Instance","text":"<pre><code>forge = MathForge()\n</code></pre> <p>This loads 50+ pre-built mathematical expressions into memory.</p>"},{"location":"getting-started/first-program/#step-3-get-an-expression","title":"Step 3: Get an Expression","text":"<pre><code>relu = forge.get(\"relu\")\n</code></pre> <p>Now <code>relu</code> is a mathematical expression object. It's not just a function \u2014 it's an object that knows:</p> <ul> <li>Its formula</li> <li>How to explain itself</li> <li>How to compute its gradient</li> <li>How to evaluate numerically</li> </ul>"},{"location":"getting-started/first-program/#step-4-explore-the-expression","title":"Step 4: Explore the Expression","text":"<pre><code># See the name\nprint(relu.name)\n# Output: relu\n\n# See the symbolic formula\nprint(relu.symbolic_expr)\n# Output: Max(0, x)\n\n# Get a plain-English explanation\nprint(relu.explain())\n</code></pre>"},{"location":"getting-started/first-program/#step-5-evaluate-it","title":"Step 5: Evaluate It","text":"<pre><code># Positive input \u2192 returns the input\nprint(relu.eval(x=5))     # Output: 5\nprint(relu.eval(x=2.7))   # Output: 2.7\n\n# Negative input \u2192 returns 0\nprint(relu.eval(x=-3))    # Output: 0\nprint(relu.eval(x=-100))  # Output: 0\n\n# Zero \u2192 returns 0\nprint(relu.eval(x=0))     # Output: 0\n</code></pre> <p>What is ReLU?</p> <p>ReLU stands for Rectified Linear Unit. It's the most popular activation function in deep learning. The rule is simple: if the input is positive, output it as-is; if negative, output 0.</p> <p>Formula: \\(f(x) = \\max(0, x)\\)</p>"},{"location":"getting-started/first-program/#step-6-compute-the-gradient","title":"Step 6: Compute the Gradient","text":"<pre><code>relu_grad = relu.gradient(\"x\")\nprint(relu_grad)\n</code></pre> <p>The gradient (derivative) tells you how the output changes when the input changes. This is the foundation of how neural networks learn.</p>"},{"location":"getting-started/first-program/#step-7-try-more-expressions","title":"Step 7: Try More Expressions","text":"<pre><code># Sigmoid \u2014 maps any number to (0, 1)\nsigmoid = forge.get(\"sigmoid\")\nprint(sigmoid.eval(x=0))     # 0.5\nprint(sigmoid.eval(x=10))    # \u2248 1.0\nprint(sigmoid.eval(x=-10))   # \u2248 0.0\n\n# Mean Squared Error \u2014 measures prediction error\nmse = forge.get(\"mse\")\nprint(mse.eval(y_pred=3.0, y_true=5.0))  # 4.0 (error = (3-5)\u00b2 = 4)\n</code></pre>"},{"location":"getting-started/first-program/#complete-first-program","title":"Complete First Program","text":"<p>Here's everything together:</p> <pre><code>from neurogebra import MathForge\n\n# Create the forge\nforge = MathForge()\n\n# === ACTIVATION FUNCTIONS ===\nrelu = forge.get(\"relu\")\nsigmoid = forge.get(\"sigmoid\")\n\n# Evaluate\nprint(\"ReLU(5)  =\", relu.eval(x=5))        # 5\nprint(\"ReLU(-3) =\", relu.eval(x=-3))       # 0\nprint(\"Sigmoid(0) =\", sigmoid.eval(x=0))   # 0.5\n\n# Explain\nprint(\"\\n--- What is ReLU? ---\")\nprint(relu.explain())\n\n# Gradient\nrelu_grad = relu.gradient(\"x\")\nprint(\"\\nReLU gradient:\", relu_grad)\n\n# === LOSS FUNCTIONS ===\nmse = forge.get(\"mse\")\nerror = mse.eval(y_pred=2.5, y_true=3.0)\nprint(f\"\\nMSE(pred=2.5, true=3.0) = {error}\")\n\n# === EXPLORE ===\nprint(\"\\nAll available expressions:\")\nprint(forge.list_all())\n\nprint(\"\\nActivation functions:\")\nprint(forge.list_all(category=\"activation\"))\n</code></pre>"},{"location":"getting-started/first-program/#try-it-yourself","title":"Try It Yourself!","text":"<p>Exercise</p> <ol> <li>Get the <code>tanh</code> activation and evaluate it at <code>x=0</code>, <code>x=1</code>, <code>x=-1</code></li> <li>Get the <code>mae</code> (Mean Absolute Error) loss and evaluate it</li> <li>Use <code>forge.search(\"smooth\")</code> to find smooth activation functions</li> <li>Call <code>.explain()</code> on any expression that interests you</li> </ol>"},{"location":"getting-started/first-program/#key-takeaways","title":"Key Takeaways","text":"Concept Code What It Does Create forge <code>forge = MathForge()</code> Loads all math expressions Get expression <code>forge.get(\"relu\")</code> Gets a specific expression Evaluate <code>expr.eval(x=5)</code> Computes the result Explain <code>expr.explain()</code> Plain-English explanation Gradient <code>expr.gradient(\"x\")</code> Computes the derivative List all <code>forge.list_all()</code> Shows everything available Search <code>forge.search(\"query\")</code> Finds matching expressions <p>Next: How Neurogebra Works \u2192</p>"},{"location":"getting-started/how-it-works/","title":"How Neurogebra Works","text":"<p>This page explains the architecture of Neurogebra \u2014 how all the pieces fit together.</p>"},{"location":"getting-started/how-it-works/#the-architecture","title":"The Architecture","text":"<p>Neurogebra is built in layers, from low-level math to high-level model building:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          ModelBuilder / NeuroCraft       \u2502  \u2190 High-level: Build models\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              Trainer                     \u2502  \u2190 Train expressions on data\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              MathForge                   \u2502  \u2190 Access expressions\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502         Expression + Autograd            \u2502  \u2190 Core: Math + Gradients\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    Repository (Activations, Losses...)   \u2502  \u2190 Pre-built formulas\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502          SymPy + NumPy                   \u2502  \u2190 Foundation: Math engines\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/how-it-works/#core-components","title":"Core Components","text":""},{"location":"getting-started/how-it-works/#1-expression-the-building-block","title":"1. Expression \u2014 The Building Block","text":"<p>Everything in Neurogebra is an Expression. An Expression is a mathematical formula that can:</p> <ul> <li>Evaluate \u2192 give you a number</li> <li>Differentiate \u2192 compute its gradient</li> <li>Explain \u2192 describe itself in plain English</li> <li>Compose \u2192 combine with other expressions</li> <li>Train \u2192 learn parameter values from data</li> </ul> <pre><code>from neurogebra import Expression\n\n# A simple expression: y = mx + b\nline = Expression(\n    name=\"line\",\n    symbolic_expr=\"m*x + b\",\n    params={\"m\": 2.0, \"b\": 1.0}\n)\n\nprint(line.eval(x=3))  # 2*3 + 1 = 7.0\n</code></pre>"},{"location":"getting-started/how-it-works/#2-mathforge-the-toolbox","title":"2. MathForge \u2014 The Toolbox","text":"<p>MathForge loads all pre-built expressions from the Repository and gives you a clean interface:</p> <pre><code>from neurogebra import MathForge\n\nforge = MathForge()\n\n# Access 50+ expressions by name\nrelu = forge.get(\"relu\")\nmse = forge.get(\"mse\")\n</code></pre>"},{"location":"getting-started/how-it-works/#3-repository-the-expression-library","title":"3. Repository \u2014 The Expression Library","text":"<p>The repository contains pre-built expressions organized by category:</p> Category Examples Used For Activations ReLU, Sigmoid, Tanh, GELU, Swish Adding non-linearity to neural networks Losses MSE, MAE, Cross-Entropy, Huber Measuring how wrong predictions are Regularizers L1, L2, Elastic Net Preventing overfitting Algebra Polynomial, Quadratic General mathematical operations Calculus Derivatives, Integrals Mathematical analysis Statistics Mean, Variance, Standard Deviation Data analysis Linear Algebra Dot product, Norms Vector/matrix operations Metrics Accuracy, Precision, Recall Evaluating model performance Optimization Gradient Descent step Training algorithms Transforms Normalization, Standardization Data preprocessing"},{"location":"getting-started/how-it-works/#4-autograd-engine-the-gradient-machine","title":"4. Autograd Engine \u2014 The Gradient Machine","text":"<p>The autograd engine tracks computations and computes gradients automatically:</p> <pre><code>from neurogebra.core.autograd import Value\n\nx = Value(2.0)\ny = Value(3.0)\n\n# Forward: compute result\nz = x * y + x ** 2  # z = 2*3 + 2\u00b2 = 10\n\n# Backward: compute gradients\nz.backward()\n\nprint(x.grad)  # dz/dx = y + 2x = 3 + 4 = 7.0\nprint(y.grad)  # dz/dy = x = 2.0\n</code></pre>"},{"location":"getting-started/how-it-works/#5-trainer-the-learning-engine","title":"5. Trainer \u2014 The Learning Engine","text":"<p>The Trainer fits expression parameters to data:</p> <pre><code>from neurogebra import Expression\nfrom neurogebra.core.trainer import Trainer\nimport numpy as np\n\n# Expression with unknown parameters\nexpr = Expression(\"line\", \"m*x + b\",\n                  params={\"m\": 0.0, \"b\": 0.0},\n                  trainable_params=[\"m\", \"b\"])\n\n# Data: y = 2x + 1\nX = np.array([1, 2, 3, 4, 5], dtype=float)\ny = np.array([3, 5, 7, 9, 11], dtype=float)\n\n# Train to find m and b\ntrainer = Trainer(expr, learning_rate=0.01)\ntrainer.fit(X, y, epochs=200)\n\nprint(f\"Learned: m={expr.params['m']:.2f}, b={expr.params['b']:.2f}\")\n# Should be close to m=2.0, b=1.0\n</code></pre>"},{"location":"getting-started/how-it-works/#6-modelbuilder-the-neural-network-constructor","title":"6. ModelBuilder \u2014 The Neural Network Constructor","text":"<p>ModelBuilder lets you build neural networks with an educational interface:</p> <pre><code>from neurogebra import ModelBuilder\n\nbuilder = ModelBuilder()\n\nmodel = builder.Sequential([\n    builder.Dense(128, activation=\"relu\"),\n    builder.Dropout(0.2),\n    builder.Dense(64, activation=\"relu\"),\n    builder.Dense(10, activation=\"softmax\")\n])\n\nmodel.summary()            # See architecture\nmodel.explain_architecture()  # Get explanations\n</code></pre>"},{"location":"getting-started/how-it-works/#how-it-all-connects","title":"How It All Connects","text":"<p>Here's the typical workflow:</p> <pre><code>1. You create a MathForge           \u2192  forge = MathForge()\n2. You get expressions               \u2192  relu = forge.get(\"relu\")\n3. You explore them                   \u2192  relu.explain(), relu.eval(x=5)\n4. You compose complex expressions    \u2192  loss = forge.compose(\"mse + 0.1*mae\")\n5. You create trainable expressions   \u2192  expr = Expression(\"f\", \"a*x+b\", ...)\n6. You train on data                  \u2192  trainer.fit(X, y, epochs=100)\n7. You build full models              \u2192  model = builder.Sequential([...])\n</code></pre>"},{"location":"getting-started/how-it-works/#symbolic-vs-numerical","title":"Symbolic vs Numerical","text":"<p>Neurogebra uses SymPy for symbolic math and NumPy for numerical computation:</p> <pre><code>relu = forge.get(\"relu\")\n\n# SYMBOLIC \u2014 see the formula\nprint(relu.symbolic_expr)  # Max(0, x)\n\n# NUMERICAL \u2014 get a number\nprint(relu.eval(x=5))     # 5\n</code></pre> <p>Why both?</p> <ul> <li>Symbolic: You can see, manipulate, differentiate, and explain formulas</li> <li>Numerical: You can evaluate efficiently on real data with NumPy arrays</li> </ul>"},{"location":"getting-started/how-it-works/#summary","title":"Summary","text":"Component Class Purpose Expression <code>Expression</code> A math formula you can evaluate, differentiate, train MathForge <code>MathForge</code> Toolbox to access all pre-built expressions Value <code>Value</code> Scalar with automatic gradient tracking Tensor <code>Tensor</code> Array with automatic gradient tracking Trainer <code>Trainer</code> Fits expression parameters to data ModelBuilder <code>ModelBuilder</code> Builds neural network architectures NeuroCraft <code>NeuroCraft</code> Enhanced interface with educational features <p>Next: Python for ML \u2192</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing Neurogebra, make sure you have:</p> <ul> <li>Python 3.8 or higher installed on your computer</li> <li>pip (Python's package manager \u2014 comes with Python)</li> </ul> <p>Don't have Python?</p> <p>Download Python from python.org. During installation on Windows, check \"Add Python to PATH\".</p>"},{"location":"getting-started/installation/#step-1-install-neurogebra","title":"Step 1: Install Neurogebra","text":"<p>Open your terminal (Command Prompt on Windows, Terminal on Mac/Linux) and type:</p> <pre><code>pip install neurogebra\n</code></pre> <p>That's it! Neurogebra is now installed.</p>"},{"location":"getting-started/installation/#step-2-verify-installation","title":"Step 2: Verify Installation","text":"<p>Let's make sure it worked. Open a Python shell:</p> <pre><code>python\n</code></pre> <p>Then type:</p> <pre><code>import neurogebra\nprint(neurogebra.__version__)\n</code></pre> <p>You should see the version number printed (e.g., <code>0.1.1</code>). </p>"},{"location":"getting-started/installation/#optional-extras","title":"Optional Extras","text":"<p>Neurogebra has optional features you can install:</p>"},{"location":"getting-started/installation/#visualization-tools","title":"Visualization Tools","text":"<p><pre><code>pip install neurogebra[viz]\n</code></pre> Adds plotting and visualization (requires matplotlib and plotly).</p>"},{"location":"getting-started/installation/#performance-boost","title":"Performance Boost","text":"<p><pre><code>pip install neurogebra[fast]\n</code></pre> Adds Numba JIT compilation for faster numerical evaluation.</p>"},{"location":"getting-started/installation/#framework-bridges","title":"Framework Bridges","text":"<p><pre><code>pip install neurogebra[frameworks]\n</code></pre> Adds PyTorch, TensorFlow, and JAX integration.</p>"},{"location":"getting-started/installation/#everything","title":"Everything","text":"<p><pre><code>pip install neurogebra[all]\n</code></pre> Installs all optional dependencies at once.</p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>Common Issues</p> <p><code>pip</code> not found?</p> <p>Try <code>pip3 install neurogebra</code> or <code>python -m pip install neurogebra</code>.</p> <p>Permission denied?</p> <p>Try <code>pip install --user neurogebra</code> or use a virtual environment.</p> <p>Old Python version?</p> <p>Neurogebra requires Python 3.8+. Check with <code>python --version</code>.</p>"},{"location":"getting-started/installation/#using-a-virtual-environment-recommended","title":"Using a Virtual Environment (Recommended)","text":"<p>A virtual environment keeps your project dependencies isolated:</p> WindowsMac/Linux <pre><code>python -m venv myenv\nmyenv\\Scripts\\activate\npip install neurogebra\n</code></pre> <pre><code>python3 -m venv myenv\nsource myenv/bin/activate\npip install neurogebra\n</code></pre> <p>Next: Your First Program \u2192</p>"},{"location":"ml-fundamentals/math-behind-ml/","title":"Math Behind ML","text":"<p>Don't panic \u2014 the math behind ML is simpler than you think. Neurogebra was built specifically to make this math visible and understandable.</p>"},{"location":"ml-fundamentals/math-behind-ml/#the-three-pillars-of-ml-math","title":"The Three Pillars of ML Math","text":"<pre><code>1. Linear Algebra  \u2192 How data flows through the model\n2. Calculus         \u2192 How the model learns (gradients)\n3. Probability      \u2192 How we measure uncertainty\n</code></pre>"},{"location":"ml-fundamentals/math-behind-ml/#pillar-1-linear-algebra","title":"Pillar 1: Linear Algebra","text":""},{"location":"ml-fundamentals/math-behind-ml/#vectors","title":"Vectors","text":"<p>A vector is just a list of numbers. In ML, your data is vectors.</p> <pre><code>import numpy as np\n\n# A data point with 3 features\n# [height, weight, age]\nperson = np.array([170, 65, 25])\n\n# Weights of a neuron\nweights = np.array([0.5, -0.3, 0.1])\n</code></pre>"},{"location":"ml-fundamentals/math-behind-ml/#the-dot-product","title":"The Dot Product","text":"<p>The most important operation in neural networks:</p> \\[\\mathbf{w} \\cdot \\mathbf{x} = w_1 x_1 + w_2 x_2 + w_3 x_3\\] <pre><code># This is what EVERY neuron computes\noutput = np.dot(weights, person)\n# = 0.5*170 + (-0.3)*65 + 0.1*25\n# = 85 - 19.5 + 2.5\n# = 68.0\n</code></pre>"},{"location":"ml-fundamentals/math-behind-ml/#matrices","title":"Matrices","text":"<p>A matrix is a 2D array. A neural network layer is a matrix multiplication:</p> \\[\\mathbf{y} = \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}\\] <pre><code># Layer with 3 inputs, 2 outputs\nW = np.array([[0.5, -0.3, 0.1],\n              [0.2,  0.4, -0.1]])  # Shape: (2, 3)\nb = np.array([0.1, -0.2])          # Shape: (2,)\nx = np.array([170, 65, 25])        # Shape: (3,)\n\noutput = W @ x + b  # @ is matrix multiplication\nprint(output)  # Shape: (2,) \u2014 two output neurons\n</code></pre> <p>Neurogebra has linear algebra expressions:</p> <pre><code>from neurogebra import MathForge\n\nforge = MathForge()\nexpressions = forge.list_all(category=\"linalg\")\nprint(expressions)\n</code></pre>"},{"location":"ml-fundamentals/math-behind-ml/#pillar-2-calculus-derivatives-gradients","title":"Pillar 2: Calculus (Derivatives &amp; Gradients)","text":""},{"location":"ml-fundamentals/math-behind-ml/#what-is-a-derivative","title":"What is a Derivative?","text":"<p>A derivative tells you how much the output changes when you change the input a tiny bit.</p> \\[f(x) = x^2 \\quad \\Rightarrow \\quad f'(x) = 2x\\] <p>At \\(x = 3\\): the derivative is \\(2 \\times 3 = 6\\). This means: if you increase \\(x\\) by a tiny amount, \\(f(x)\\) increases by about 6 times that amount.</p> <pre><code>from neurogebra import Expression\n\nf = Expression(\"quadratic\", \"x**2\")\nf_prime = f.gradient(\"x\")\n\nprint(f\"f(3) = {f.eval(x=3)}\")           # 9\nprint(f\"f'(3) = {f_prime.eval(x=3)}\")    # 6\n</code></pre>"},{"location":"ml-fundamentals/math-behind-ml/#why-derivatives-matter-in-ml","title":"Why Derivatives Matter in ML","text":"<p>The loss function measures error. The derivative of the loss with respect to each weight tells us how to adjust that weight:</p> <pre><code>from neurogebra import MathForge\n\nforge = MathForge()\n\n# MSE loss: L = (prediction - target)\u00b2\nmse = forge.get(\"mse\")\nprint(mse.symbolic_expr)  # (y_pred - y_true)**2\n\n# Gradient: dL/d(prediction) = 2*(prediction - target)\ngrad = mse.gradient(\"y_pred\")\nprint(grad.symbolic_expr)  # 2*(y_pred - y_true)\n\n# If prediction=5, target=3:\n# gradient = 2*(5-3) = 4 (positive \u2192 decrease prediction)\nprint(grad.eval(y_pred=5, y_true=3))  # 4.0\n</code></pre>"},{"location":"ml-fundamentals/math-behind-ml/#the-chain-rule","title":"The Chain Rule","text":"<p>For composite functions, derivatives multiply:</p> \\[\\frac{d}{dx} f(g(x)) = f'(g(x)) \\cdot g'(x)\\] <p>This is exactly what backpropagation does \u2014 it applies the chain rule through every layer of a neural network.</p> <pre><code>from neurogebra.core.autograd import Value\n\n# Chain of operations\nx = Value(2.0)\nh = x ** 2        # h = x\u00b2 = 4\ny = h + 3         # y = h + 3 = 7\nz = y * 2         # z = y * 2 = 14\n\n# Backprop automatically applies chain rule\nz.backward()\n\nprint(f\"dz/dx = {x.grad}\")  # dz/dh * dh/dx = 2 * 2x = 2*4 = 8\n</code></pre>"},{"location":"ml-fundamentals/math-behind-ml/#gradient-descent","title":"Gradient Descent","text":"<p>The core learning algorithm: move in the opposite direction of the gradient:</p> \\[w_{new} = w_{old} - \\alpha \\cdot \\frac{\\partial L}{\\partial w}\\] <p>Where \\(\\alpha\\) is the learning rate.</p> <pre><code># Manual gradient descent\nw = 5.0          # Start with a guess\nlr = 0.1         # Learning rate\ntarget = 0.0     # Want to minimize w\u00b2\n\nfor step in range(20):\n    loss = w ** 2           # Loss function\n    gradient = 2 * w        # dL/dw = 2w\n    w = w - lr * gradient   # Update step\n\n    if step % 5 == 0:\n        print(f\"Step {step}: w = {w:.4f}, loss = {loss:.4f}\")\n\n# w gets closer and closer to 0 (the minimum of w\u00b2)\n</code></pre>"},{"location":"ml-fundamentals/math-behind-ml/#pillar-3-probability-statistics","title":"Pillar 3: Probability &amp; Statistics","text":""},{"location":"ml-fundamentals/math-behind-ml/#mean-average","title":"Mean (Average)","text":"\\[\\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\] <pre><code>import numpy as np\n\ndata = np.array([85, 92, 78, 95, 88])\nprint(f\"Mean: {np.mean(data)}\")  # 87.6\n</code></pre>"},{"location":"ml-fundamentals/math-behind-ml/#variance-and-standard-deviation","title":"Variance and Standard Deviation","text":"<p>How spread out the data is:</p> \\[\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2\\] <pre><code>print(f\"Variance: {np.var(data):.2f}\")  # 33.84\nprint(f\"Std Dev:  {np.std(data):.2f}\")  # 5.82\n</code></pre>"},{"location":"ml-fundamentals/math-behind-ml/#the-normal-gaussian-distribution","title":"The Normal (Gaussian) Distribution","text":"<p>Most data in ML follows a bell curve:</p> <pre><code># Generate normally distributed data\nsamples = np.random.normal(mean=0, scale=1, size=1000)\nprint(f\"Mean: {samples.mean():.2f}\")  # \u2248 0\nprint(f\"Std:  {samples.std():.2f}\")   # \u2248 1\n</code></pre>"},{"location":"ml-fundamentals/math-behind-ml/#softmax-probabilities-from-numbers","title":"Softmax \u2014 Probabilities from Numbers","text":"<p>Converts raw numbers into probabilities that sum to 1:</p> \\[\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\\] <pre><code>def softmax(x):\n    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n    return exp_x / exp_x.sum()\n\nscores = np.array([2.0, 1.0, 0.1])\nprobs = softmax(scores)\nprint(probs)  # [0.659, 0.242, 0.099]\nprint(probs.sum())  # 1.0 \u2014 they're probabilities!\n</code></pre>"},{"location":"ml-fundamentals/math-behind-ml/#putting-it-all-together-a-neuron","title":"Putting It All Together: A Neuron","text":"<p>A single neuron combines all three pillars:</p> <ol> <li>Linear Algebra: \\(z = \\mathbf{w} \\cdot \\mathbf{x} + b\\) (dot product)</li> <li>Calculus: Gradient descent learns \\(\\mathbf{w}\\) and \\(b\\)</li> <li>Probability: Activation (sigmoid) outputs a probability</li> </ol> <pre><code>from neurogebra.core.autograd import Value\n\n# Inputs\nx1, x2 = Value(1.0), Value(2.0)\n\n# Weights &amp; bias (the parameters to learn)\nw1, w2, b = Value(0.5), Value(-0.3), Value(0.1)\n\n# Step 1: Linear combination (Linear Algebra)\nz = w1 * x1 + w2 * x2 + b  # 0.5*1 + (-0.3)*2 + 0.1 = 0.0\n\n# Step 2: Activation (Probability - squash to 0-1)\noutput = z.sigmoid()  # \u03c3(0.0) = 0.5\n\n# Step 3: Compute gradients (Calculus)\noutput.backward()\n\nprint(f\"Output: {output.data:.4f}\")    # 0.5000\nprint(f\"dout/dw1 = {w1.grad:.4f}\")     # How to adjust w1\nprint(f\"dout/dw2 = {w2.grad:.4f}\")     # How to adjust w2\nprint(f\"dout/db  = {b.grad:.4f}\")      # How to adjust bias\n</code></pre>"},{"location":"ml-fundamentals/math-behind-ml/#neurogebra-makes-math-visible","title":"Neurogebra Makes Math Visible","text":"<p>Instead of hiding math inside C++ kernels like PyTorch:</p> <pre><code>from neurogebra import MathForge\n\nforge = MathForge()\n\n# See the formula\nsigmoid = forge.get(\"sigmoid\")\nprint(sigmoid.symbolic_expr)  # 1/(1 + exp(-x))\n\n# See the derivative\nprint(sigmoid.gradient(\"x\").symbolic_expr)\n# exp(-x)/(1 + exp(-x))\u00b2 \u2014 which equals \u03c3(x)\u00b7(1-\u03c3(x))\n\n# Explain it\nprint(sigmoid.explain())\n</code></pre> <p>Key Insight</p> <p>You don't need to memorize derivatives. Neurogebra computes them symbolically. But understanding what they mean is crucial for debugging and improving your models.</p> <p>Next: MathForge \u2014 The Core \u2192</p>"},{"location":"ml-fundamentals/ml-workflow/","title":"The ML Workflow","text":"<p>Every ML project follows the same steps. This page teaches you the standard workflow that professionals use.</p>"},{"location":"ml-fundamentals/ml-workflow/#the-7-steps","title":"The 7 Steps","text":"<pre><code>Step 1: Define the Problem\n    \u2193\nStep 2: Collect &amp; Prepare Data\n    \u2193\nStep 3: Choose a Model\n    \u2193\nStep 4: Train the Model\n    \u2193\nStep 5: Evaluate the Model\n    \u2193\nStep 6: Tune &amp; Improve\n    \u2193\nStep 7: Deploy (Use It)\n</code></pre>"},{"location":"ml-fundamentals/ml-workflow/#step-1-define-the-problem","title":"Step 1: Define the Problem","text":"<p>Before writing any code, ask:</p> <ul> <li>What am I predicting? (a number? a category?)</li> <li>What data do I have? (features and labels?)</li> <li>How will I measure success? (accuracy? error?)</li> </ul> <p>Example</p> <p>Problem: Predict house prices from size.</p> <ul> <li>Type: Regression (predicting a number)</li> <li>Input: House size in sqft</li> <li>Output: Price in dollars</li> <li>Success metric: Mean Squared Error (lower = better)</li> </ul>"},{"location":"ml-fundamentals/ml-workflow/#step-2-collect-prepare-data","title":"Step 2: Collect &amp; Prepare Data","text":"<pre><code>import numpy as np\n\n# Simulate a dataset\nnp.random.seed(42)\nn = 200\n\n# Features (inputs)\nhouse_size = np.random.uniform(500, 3000, n)\n\n# Labels (outputs) \u2014 the \"ground truth\"\nprice = 150 * house_size + 50000 + np.random.normal(0, 20000, n)\n\n# Split into train/test\nsplit = int(0.8 * n)\nX_train, X_test = house_size[:split], house_size[split:]\ny_train, y_test = price[:split], price[split:]\n\n# Normalize features\nX_mean, X_std = X_train.mean(), X_train.std()\nX_train_norm = (X_train - X_mean) / X_std\nX_test_norm = (X_test - X_mean) / X_std\n\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Testing samples:  {len(X_test)}\")\n</code></pre>"},{"location":"ml-fundamentals/ml-workflow/#step-3-choose-a-model","title":"Step 3: Choose a Model","text":"<p>Start simple. You can always make it more complex later.</p> <pre><code>from neurogebra import Expression\n\n# Simple linear model: price = w * size + b\nmodel = Expression(\n    \"house_price_model\",\n    \"w * x + b\",\n    params={\"w\": 0.0, \"b\": 0.0},\n    trainable_params=[\"w\", \"b\"]\n)\n</code></pre> <p>Model Selection Rule</p> <ul> <li>Linear data? \u2192 Start with linear model</li> <li>Non-linear data? \u2192 Use polynomial or neural network</li> <li>Image data? \u2192 Use CNN (convolutional neural network)</li> <li>Sequence data? \u2192 Use RNN or Transformer</li> </ul>"},{"location":"ml-fundamentals/ml-workflow/#step-4-train-the-model","title":"Step 4: Train the Model","text":"<pre><code>from neurogebra.core.trainer import Trainer\n\ntrainer = Trainer(model, learning_rate=0.01, optimizer=\"adam\")\n\nhistory = trainer.fit(\n    X_train_norm, y_train,\n    epochs=300,\n    verbose=True\n)\n\nprint(f\"\\nLearned parameters:\")\nprint(f\"  w = {model.params['w']:.2f}\")\nprint(f\"  b = {model.params['b']:.2f}\")\n</code></pre>"},{"location":"ml-fundamentals/ml-workflow/#what-happens-during-training","title":"What Happens During Training?","text":"Epoch Loss What's Happening 0 1,000,000 Random weights \u2014 terrible predictions 50 500,000 Getting better \u2014 weights adjusting 100 200,000 Much better \u2014 model is learning the pattern 200 100,000 Good \u2014 close to optimal 300 95,000 Converged \u2014 further training won't help much"},{"location":"ml-fundamentals/ml-workflow/#step-5-evaluate-the-model","title":"Step 5: Evaluate the Model","text":"<p>Never evaluate on training data. Use the test set:</p> <pre><code># Predict on test data\npredictions = np.array([model.eval(x=xi) for xi in X_test_norm])\n\n# Calculate metrics\nmse = np.mean((predictions - y_test) ** 2)\nrmse = np.sqrt(mse)\nmae = np.mean(np.abs(predictions - y_test))\n\nprint(f\"Test MSE:  {mse:.2f}\")\nprint(f\"Test RMSE: {rmse:.2f}\")\nprint(f\"Test MAE:  {mae:.2f}\")\n\n# R\u00b2 score (how much variance is explained)\nss_res = np.sum((y_test - predictions) ** 2)\nss_tot = np.sum((y_test - y_test.mean()) ** 2)\nr2 = 1 - ss_res / ss_tot\nprint(f\"R\u00b2 Score: {r2:.4f}\")  # 1.0 = perfect, 0.0 = terrible\n</code></pre>"},{"location":"ml-fundamentals/ml-workflow/#understanding-the-metrics","title":"Understanding the Metrics","text":"Metric What It Measures Good Value MSE Average squared error Lower = better RMSE Average error (same units as data) Lower = better MAE Average absolute error Lower = better R\u00b2 How much variance is explained 1.0 = perfect"},{"location":"ml-fundamentals/ml-workflow/#step-6-tune-improve","title":"Step 6: Tune &amp; Improve","text":"<p>If the model isn't good enough, try:</p>"},{"location":"ml-fundamentals/ml-workflow/#change-learning-rate","title":"Change Learning Rate","text":"<pre><code># Too high \u2192 oscillates, might diverge\ntrainer = Trainer(model, learning_rate=1.0)  # Bad!\n\n# Too low \u2192 takes forever to converge\ntrainer = Trainer(model, learning_rate=0.00001)  # Slow!\n\n# Just right \u2192 smooth convergence\ntrainer = Trainer(model, learning_rate=0.01)  # Good!\n</code></pre>"},{"location":"ml-fundamentals/ml-workflow/#use-a-better-optimizer","title":"Use a Better Optimizer","text":"<pre><code># Adam usually works better than SGD\ntrainer = Trainer(model, learning_rate=0.01, optimizer=\"adam\")\n</code></pre>"},{"location":"ml-fundamentals/ml-workflow/#use-a-more-complex-model","title":"Use a More Complex Model","text":"<pre><code># Polynomial model for non-linear data\nmodel = Expression(\n    \"polynomial\",\n    \"a*x**2 + b*x + c\",\n    params={\"a\": 0.0, \"b\": 0.0, \"c\": 0.0},\n    trainable_params=[\"a\", \"b\", \"c\"]\n)\n</code></pre>"},{"location":"ml-fundamentals/ml-workflow/#train-longer","title":"Train Longer","text":"<pre><code>trainer.fit(X_train_norm, y_train, epochs=1000)\n</code></pre>"},{"location":"ml-fundamentals/ml-workflow/#step-7-deploy-use-it","title":"Step 7: Deploy (Use It)","text":"<p>Once your model is good, use it to make predictions on new data:</p> <pre><code># New house: 1800 sqft\nnew_size = 1800\nnew_size_norm = (new_size - X_mean) / X_std\n\npredicted_price = model.eval(x=new_size_norm)\nprint(f\"Predicted price for {new_size} sqft: ${predicted_price:,.2f}\")\n</code></pre>"},{"location":"ml-fundamentals/ml-workflow/#common-pitfalls","title":"Common Pitfalls","text":"<p>Overfitting</p> <p>Problem: Model memorizes training data but fails on new data.</p> <p>Signs: Training loss is very low, test loss is high.</p> <p>Solutions: More data, regularization, simpler model, dropout.</p> <p>Underfitting</p> <p>Problem: Model is too simple to capture the pattern.</p> <p>Signs: Both training and test loss are high.</p> <p>Solutions: More complex model, more features, train longer.</p> <p>Data Leakage</p> <p>Problem: Test data accidentally influences training.</p> <p>Signs: Suspiciously good test results.</p> <p>Solutions: Always split data BEFORE any processing.</p>"},{"location":"ml-fundamentals/ml-workflow/#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code>import numpy as np\nfrom neurogebra import Expression\nfrom neurogebra.core.trainer import Trainer\n\n# === STEP 1: Define ===\n# Predicting y = 3x + 2 from noisy data\n\n# === STEP 2: Data ===\nnp.random.seed(42)\nX = np.linspace(0, 10, 200)\ny = 3 * X + 2 + np.random.normal(0, 1, 200)\n\nX_train, X_test = X[:160], X[160:]\ny_train, y_test = y[:160], y[160:]\n\n# === STEP 3: Model ===\nmodel = Expression(\"linear\", \"m*x + b\",\n                   params={\"m\": 0.0, \"b\": 0.0},\n                   trainable_params=[\"m\", \"b\"])\n\n# === STEP 4: Train ===\ntrainer = Trainer(model, learning_rate=0.001, optimizer=\"adam\")\nhistory = trainer.fit(X_train, y_train, epochs=500, verbose=True)\n\n# === STEP 5: Evaluate ===\npreds = np.array([model.eval(x=xi) for xi in X_test])\nmse = np.mean((preds - y_test) ** 2)\nprint(f\"\\nTest MSE: {mse:.4f}\")\nprint(f\"Learned: y = {model.params['m']:.2f}x + {model.params['b']:.2f}\")\n# Expected: y \u2248 3.00x + 2.00\n</code></pre> <p>Next: Math Behind ML \u2192</p>"},{"location":"ml-fundamentals/types-of-ml/","title":"Types of Machine Learning","text":"<p>Machine Learning has three main categories. Each solves a different kind of problem.</p>"},{"location":"ml-fundamentals/types-of-ml/#1-supervised-learning","title":"1. Supervised Learning","text":"<p>You have answers (labels) for your training data.</p> <p>The model learns the relationship between inputs and outputs.</p> <pre><code>Training Data:\n    Input: house_size = 1500 sqft \u2192 Output: price = $280,000 \u2713\n    Input: house_size = 2000 sqft \u2192 Output: price = $350,000 \u2713\n    Input: house_size = 800 sqft  \u2192 Output: price = $150,000 \u2713\n\nAfter Training:\n    Input: house_size = 1800 sqft \u2192 Output: price = ???\n    Model predicts: $315,000\n</code></pre>"},{"location":"ml-fundamentals/types-of-ml/#two-sub-types","title":"Two Sub-Types:","text":""},{"location":"ml-fundamentals/types-of-ml/#regression-predicting-numbers","title":"Regression \u2014 Predicting Numbers","text":"<p>Output is a continuous value (price, temperature, score).</p> <pre><code>from neurogebra import Expression\nfrom neurogebra.core.trainer import Trainer\nimport numpy as np\n\n# REGRESSION: Predict a number\nmodel = Expression(\"linear\", \"m*x + b\",\n                   params={\"m\": 0.0, \"b\": 0.0},\n                   trainable_params=[\"m\", \"b\"])\n\nX = np.array([1, 2, 3, 4, 5], dtype=float)\ny = np.array([2.1, 3.9, 6.1, 7.8, 10.2], dtype=float)\n\ntrainer = Trainer(model, learning_rate=0.01)\ntrainer.fit(X, y, epochs=200)\n</code></pre> Use Cases Loss Functions Output House prices MSE, MAE Continuous number Stock prediction Huber Continuous number Temperature forecast MSE Continuous number"},{"location":"ml-fundamentals/types-of-ml/#classification-predicting-categories","title":"Classification \u2014 Predicting Categories","text":"<p>Output is a class/category (spam/not-spam, cat/dog/bird).</p> <pre><code># The model outputs a probability using Sigmoid\nfrom neurogebra import MathForge\n\nforge = MathForge()\nsigmoid = forge.get(\"sigmoid\")\n\n# Output &gt; 0.5 \u2192 Class 1 (spam)\n# Output &lt; 0.5 \u2192 Class 0 (not spam)\nprint(sigmoid.eval(x=2.0))   # 0.88 \u2192 \"It's spam\"\nprint(sigmoid.eval(x=-1.5))  # 0.18 \u2192 \"Not spam\"\n</code></pre> Use Cases Loss Functions Output Spam detection Binary Cross-Entropy 0 or 1 Image recognition Cross-Entropy Class label Disease diagnosis Binary Cross-Entropy Yes or No"},{"location":"ml-fundamentals/types-of-ml/#2-unsupervised-learning","title":"2. Unsupervised Learning","text":"<p>You have NO answers (labels). The model finds patterns on its own.</p> <pre><code>Data (no labels):\n    [1.2, 1.3], [1.1, 1.4], [5.5, 5.6], [5.2, 5.8], [9.0, 9.1]\n\nModel finds:\n    Cluster A: [1.2, 1.3], [1.1, 1.4]         (small values)\n    Cluster B: [5.5, 5.6], [5.2, 5.8]         (medium values)\n    Cluster C: [9.0, 9.1]                      (large values)\n</code></pre> Use Cases Algorithms Customer segmentation K-Means clustering Anomaly detection Autoencoders Data compression PCA Recommendation Collaborative filtering"},{"location":"ml-fundamentals/types-of-ml/#3-reinforcement-learning","title":"3. Reinforcement Learning","text":"<p>The model learns by trial and error, receiving rewards or penalties.</p> <pre><code>Environment: Video game\nAgent: The AI player\nActions: Move left, right, jump\nRewards: +10 for collecting coin, -100 for falling\n\nThe agent tries random actions, learns which ones lead to rewards,\nand eventually masters the game.\n</code></pre> Use Cases Examples Game playing AlphaGo, Atari games Robotics Walking, grasping objects Self-driving Navigation decisions"},{"location":"ml-fundamentals/types-of-ml/#comparison-table","title":"Comparison Table","text":"Type Has Labels? Goal Neurogebra Focus Supervised Yes Predict from labeled data \u2705 Full support Unsupervised No Find patterns Partial Reinforcement Rewards Maximize reward Future feature <p>Neurogebra focuses on Supervised Learning</p> <p>This is where you'll spend most of your ML journey. Neurogebra excels at making supervised learning understandable \u2014 you can see every loss function, every gradient, every optimization step.</p>"},{"location":"ml-fundamentals/types-of-ml/#the-ml-problem-decision-tree","title":"The ML Problem Decision Tree","text":"<pre><code>What kind of output do you need?\n\u2502\n\u251c\u2500\u2500 A number (price, score, amount) \u2192 REGRESSION\n\u2502   \u2514\u2500\u2500 Neurogebra: use MSE/MAE loss, linear output\n\u2502\n\u251c\u2500\u2500 A category (yes/no) \u2192 BINARY CLASSIFICATION\n\u2502   \u2514\u2500\u2500 Neurogebra: use binary_crossentropy, sigmoid output\n\u2502\n\u251c\u2500\u2500 A category (cat/dog/bird) \u2192 MULTI-CLASS CLASSIFICATION\n\u2502   \u2514\u2500\u2500 Neurogebra: use cross_entropy, softmax output\n\u2502\n\u251c\u2500\u2500 Groups in data (no labels) \u2192 CLUSTERING\n\u2502   \u2514\u2500\u2500 Use: K-Means, DBSCAN\n\u2502\n\u2514\u2500\u2500 Sequential decisions \u2192 REINFORCEMENT LEARNING\n    \u2514\u2500\u2500 Use: Q-Learning, Policy Gradient\n</code></pre>"},{"location":"ml-fundamentals/types-of-ml/#neurogebra-makes-it-clear","title":"Neurogebra Makes It Clear","text":"<pre><code>from neurogebra import MathForge\n\nforge = MathForge()\n\n# See all loss functions \u2014 pick the right one for your task\nlosses = forge.list_all(category=\"loss\")\nprint(losses)\n\n# Each one explains when to use it\nfor name in [\"mse\", \"mae\", \"binary_crossentropy\", \"hinge\"]:\n    expr = forge.get(name)\n    print(f\"\\n{name}:\")\n    print(f\"  {expr.metadata.get('description', '')}\")\n    print(f\"  Best for: {expr.metadata.get('usage', '')}\")\n</code></pre> <p>Next: The ML Workflow \u2192</p>"},{"location":"ml-fundamentals/what-is-ml/","title":"What is Machine Learning?","text":"<p>This page explains Machine Learning in simple terms. No jargon, no PhD required.</p>"},{"location":"ml-fundamentals/what-is-ml/#the-simple-explanation","title":"The Simple Explanation","text":"<p>Machine Learning is teaching computers to learn from data instead of being explicitly programmed.</p>"},{"location":"ml-fundamentals/what-is-ml/#traditional-programming-vs-ml","title":"Traditional Programming vs ML","text":"<pre><code>Traditional Programming:\n    INPUT (data) + RULES (code) \u2192 OUTPUT (answers)\n\n    Example: if temperature &gt; 30: print(\"Hot\")\n\nMachine Learning:\n    INPUT (data) + OUTPUT (answers) \u2192 RULES (learned model)\n\n    Example: Give the model 1000 examples of temperatures \n             labeled \"Hot\" or \"Cold\", and it LEARNS the rule.\n</code></pre>"},{"location":"ml-fundamentals/what-is-ml/#a-concrete-example","title":"A Concrete Example","text":""},{"location":"ml-fundamentals/what-is-ml/#without-ml-you-write-the-rules","title":"Without ML: You Write the Rules","text":"<pre><code>def predict_house_price(size, bedrooms, location):\n    \"\"\"Hand-coded rules \u2014 fragile and incomplete.\"\"\"\n    base = size * 100\n    base += bedrooms * 5000\n    if location == \"city\":\n        base *= 1.5\n    return base\n</code></pre> <p>Problems: How did you choose <code>100</code>? What about pools? Garages? This doesn't scale.</p>"},{"location":"ml-fundamentals/what-is-ml/#with-ml-the-computer-learns-the-rules","title":"With ML: The Computer Learns the Rules","text":"<pre><code>from neurogebra import Expression\nfrom neurogebra.core.trainer import Trainer\nimport numpy as np\n\n# Define: price = w1*size + w2*bedrooms + bias\n# The model will LEARN w1, w2, and bias from data\n\nexpr = Expression(\n    \"house_price\",\n    \"w1*x + w2*x2 + b\",\n    params={\"w1\": 0.0, \"w2\": 0.0, \"b\": 0.0},\n    trainable_params=[\"w1\", \"w2\", \"b\"]\n)\n\n# Feed it data \u2192 it learns the rules automatically\n</code></pre>"},{"location":"ml-fundamentals/what-is-ml/#the-three-ingredients","title":"The Three Ingredients","text":"<p>Every ML system needs:</p>"},{"location":"ml-fundamentals/what-is-ml/#1-data","title":"1. Data","text":"<p>The examples the model learns from.</p> <pre><code># Example: House data\nsizes =    [800,  1200, 1500, 2000, 2500]\nprices =   [150k, 220k, 280k, 350k, 430k]\n</code></pre>"},{"location":"ml-fundamentals/what-is-ml/#2-model","title":"2. Model","text":"<p>The mathematical formula with learnable parameters.</p> <pre><code># price = weight \u00d7 size + bias\n# The model: f(x) = w*x + b\n# w and b are unknown \u2192 the model will learn them\n</code></pre>"},{"location":"ml-fundamentals/what-is-ml/#3-training","title":"3. Training","text":"<p>The process of finding the best parameters by minimizing error.</p> <pre><code># Step 1: Start with random weights\n# Step 2: Make predictions\n# Step 3: Measure error (loss)\n# Step 4: Adjust weights to reduce error\n# Step 5: Repeat steps 2-4 many times (epochs)\n</code></pre>"},{"location":"ml-fundamentals/what-is-ml/#the-training-loop-the-heart-of-ml","title":"The Training Loop \u2014 The Heart of ML","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          THE TRAINING LOOP               \u2502\n\u2502                                          \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                            \u2502\n\u2502   \u2502  START   \u2502 (random weights)          \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                            \u2502\n\u2502        \u25bc                                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n\u2502   \u2502 Forward Pass     \u2502 (make prediction) \u2502\n\u2502   \u2502 \u0177 = w*x + b     \u2502                    \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2502            \u25bc                             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n\u2502   \u2502 Compute Loss     \u2502 (measure error)   \u2502\n\u2502   \u2502 L = (\u0177 - y)\u00b2    \u2502                    \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2502            \u25bc                             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n\u2502   \u2502 Backward Pass    \u2502 (compute grads)   \u2502\n\u2502   \u2502 dL/dw, dL/db    \u2502                    \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2502            \u25bc                             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n\u2502   \u2502 Update Weights   \u2502 (learn!)          \u2502\n\u2502   \u2502 w = w - lr*dL/dw \u2502                    \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2502            \u2502                             \u2502\n\u2502            \u2514\u2500\u2500\u2500\u2500 Repeat N epochs \u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                          \u2502\n\u2502   Loss gets smaller each time = LEARNING \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ml-fundamentals/what-is-ml/#in-code-using-neurogebra","title":"In Code (Using Neurogebra):","text":"<pre><code>from neurogebra import Expression\nfrom neurogebra.core.trainer import Trainer\nimport numpy as np\n\n# The model: y = m*x + b\nmodel = Expression(\n    \"linear_model\",\n    \"m*x + b\",\n    params={\"m\": 0.0, \"b\": 0.0},\n    trainable_params=[\"m\", \"b\"]\n)\n\n# The data (y = 2x + 1)\nX = np.array([1, 2, 3, 4, 5], dtype=float)\ny = np.array([3, 5, 7, 9, 11], dtype=float)\n\n# The training\ntrainer = Trainer(model, learning_rate=0.01)\nhistory = trainer.fit(X, y, epochs=200)\n\n# After training: m \u2248 2.0, b \u2248 1.0\nprint(f\"Learned: y = {model.params['m']:.1f}x + {model.params['b']:.1f}\")\n</code></pre>"},{"location":"ml-fundamentals/what-is-ml/#key-terms-your-ml-vocabulary","title":"Key Terms \u2014 Your ML Vocabulary","text":"Term Simple Meaning Example Model A math formula with adjustable knobs <code>y = w*x + b</code> Parameters The knobs (weights and biases) <code>w</code> and <code>b</code> Training Turning the knobs to minimize error Running 200 epochs Loss How wrong the model is MSE: <code>(prediction - actual)\u00b2</code> Gradient Which direction to turn the knobs <code>dLoss/dw</code> (derivative) Learning Rate How much to turn each time <code>0.01</code> \u2014 small steps Epoch One complete pass through all data Epoch 1, 2, 3... 200 Prediction The model's answer <code>\u0177 = 2.1*3 + 0.9 = 7.2</code> Ground Truth The correct answer <code>y = 7</code>"},{"location":"ml-fundamentals/what-is-ml/#why-it-works","title":"Why It Works","text":"<p>Machine Learning works because of calculus \u2014 specifically, gradients (derivatives).</p> <p>The gradient tells the model: \"If you increase this weight slightly, the loss will go up/down by this much.\"</p> <ul> <li>If the gradient is positive \u2192 decreasing the weight reduces loss</li> <li>If the gradient is negative \u2192 increasing the weight reduces loss</li> <li>If the gradient is zero \u2192 you're at a minimum (optimal)</li> </ul> <p>Neurogebra makes this visible:</p> <pre><code>from neurogebra import MathForge\n\nforge = MathForge()\nmse = forge.get(\"mse\")\n\n# See the loss function\nprint(mse.symbolic_expr)  # (y_pred - y_true)**2\n\n# See its gradient\ngrad = mse.gradient(\"y_pred\")\nprint(grad.symbolic_expr)  # 2*(y_pred - y_true)\n\n# The gradient tells us: if prediction &gt; true, gradient is positive\n# \u2192 we should decrease the prediction\n# That's exactly what training does!\n</code></pre> <p>Next: Types of Machine Learning \u2192</p>"},{"location":"projects/project1-linear-regression/","title":"Project 1: Linear Regression \u2014 Neurogebra vs PyTorch","text":"<p>Build a complete linear regression model to predict house prices. We'll implement the exact same thing in both Neurogebra and PyTorch so you can see the differences.</p>"},{"location":"projects/project1-linear-regression/#goal","title":"\ud83c\udfaf Goal","text":"<p>Given house sizes (sq ft), predict the price ($).</p> <pre><code>Input: House size (1000, 1500, 2000, ...)\nOutput: Predicted price ($200k, $300k, $400k, ...)\n</code></pre>"},{"location":"projects/project1-linear-regression/#step-1-create-the-dataset","title":"Step 1: Create the Dataset","text":"NeurogebraPyTorch <pre><code>import numpy as np\n\n# Generate synthetic house data\nnp.random.seed(42)\n\n# Features: house size in sq ft (scaled to 0-1)\nX = np.random.uniform(500, 3500, 100)\nX_normalized = (X - X.mean()) / X.std()\n\n# Target: price in $1000s (true relationship: price = 200 * size + 50 + noise)\ny_true = 200 * X_normalized + 50 + np.random.normal(0, 10, 100)\n\n# Train/test split\nX_train, X_test = X_normalized[:80], X_normalized[80:]\ny_train, y_test = y_true[:80], y_true[80:]\n\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Test samples:     {len(X_test)}\")\nprint(f\"X range: [{X_normalized.min():.2f}, {X_normalized.max():.2f}]\")\nprint(f\"y range: [{y_true.min():.1f}, {y_true.max():.1f}]\")\n</code></pre> <pre><code>import numpy as np\nimport torch\n\n# Generate synthetic house data\nnp.random.seed(42)\n\nX = np.random.uniform(500, 3500, 100)\nX_normalized = (X - X.mean()) / X.std()\ny_true = 200 * X_normalized + 50 + np.random.normal(0, 10, 100)\n\nX_train, X_test = X_normalized[:80], X_normalized[80:]\ny_train, y_test = y_true[:80], y_true[80:]\n\n# PyTorch needs tensors\nX_train_t = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_test_t = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\ny_test_t = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Test samples:     {len(X_test)}\")\n</code></pre> <p>Key Difference #1</p> <p>Neurogebra works directly with NumPy arrays \u2014 no conversion needed. PyTorch requires converting to torch.Tensor objects first.</p>"},{"location":"projects/project1-linear-regression/#step-2-define-the-model","title":"Step 2: Define the Model","text":"NeurogebraPyTorch <pre><code>from neurogebra import Expression\n\n# Define model: y = w * x + b\nmodel = Expression(\n    \"house_price\",\n    \"w * x + b\",\n    params={\"w\": 0.0, \"b\": 0.0},\n    trainable_params=[\"w\", \"b\"]\n)\n\nprint(f\"Model: {model.symbolic_expr}\")\nprint(f\"Parameters: w={model.params['w']}, b={model.params['b']}\")\n</code></pre> <pre><code>import torch.nn as nn\n\n# Define model\nmodel = nn.Linear(1, 1)  # 1 input \u2192 1 output\n\n# Initialize weights to 0 (to match Neurogebra)\nnn.init.zeros_(model.weight)\nnn.init.zeros_(model.bias)\n\nprint(f\"Model: {model}\")\nprint(f\"Parameters: w={model.weight.item():.1f}, b={model.bias.item():.1f}\")\n</code></pre> <p>Key Difference #2</p> <p>Neurogebra: You write the actual math formula <code>w * x + b</code>. You can read and understand it. PyTorch: You specify <code>nn.Linear(1, 1)</code> \u2014 the math is hidden inside the module.</p>"},{"location":"projects/project1-linear-regression/#step-3-set-up-training","title":"Step 3: Set Up Training","text":"NeurogebraPyTorch <pre><code>from neurogebra.core.trainer import Trainer\n\n# Create trainer with Adam optimizer\ntrainer = Trainer(\n    model,\n    learning_rate=0.1,\n    optimizer=\"adam\"\n)\n\nprint(\"Optimizer: Adam\")\nprint(f\"Learning rate: {trainer.learning_rate}\")\n</code></pre> <pre><code>import torch.optim as optim\n\n# Loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.1)\n\nprint(\"Loss: MSE\")\nprint(\"Optimizer: Adam\")\nprint(f\"Learning rate: 0.1\")\n</code></pre> <p>Key Difference #3</p> <p>Neurogebra: One <code>Trainer</code> object handles everything. PyTorch: You need separate <code>criterion</code> (loss) and <code>optimizer</code> objects.</p>"},{"location":"projects/project1-linear-regression/#step-4-train-the-model","title":"Step 4: Train the Model","text":"NeurogebraPyTorch <pre><code># Train!\nhistory = trainer.fit(\n    X_train,\n    y_train,\n    epochs=200,\n    loss_fn=\"mse\",\n    verbose=True\n)\n\n# Check learned parameters\nprint(f\"\\nLearned: w = {model.params['w']:.4f} (true: 200)\")\nprint(f\"Learned: b = {model.params['b']:.4f} (true: 50)\")\nprint(f\"Final loss: {history['loss'][-1]:.4f}\")\n</code></pre> <pre><code># Training loop\nhistory = {\"loss\": []}\n\nfor epoch in range(200):\n    # Forward pass\n    predictions = model(X_train_t)\n    loss = criterion(predictions, y_train_t)\n\n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Record history\n    history[\"loss\"].append(loss.item())\n\n    # Print progress\n    if epoch % 20 == 0 or epoch == 199:\n        print(f\"Epoch {epoch:&gt;4d}/200: Loss = {loss.item():.6f}\")\n\nprint(f\"\\nLearned: w = {model.weight.item():.4f} (true: 200)\")\nprint(f\"Learned: b = {model.bias.item():.4f} (true: 50)\")\nprint(f\"Final loss: {history['loss'][-1]:.4f}\")\n</code></pre> <p>Key Difference #4</p> <p>Neurogebra: One line \u2014 <code>trainer.fit(X, y, epochs=200)</code>. Everything is handled for you.</p> <p>PyTorch: You write the full training loop manually:</p> <ol> <li>Forward pass</li> <li>Compute loss</li> <li>Zero gradients</li> <li>Backward pass</li> <li>Optimizer step</li> <li>Record metrics</li> </ol> <p>This gives PyTorch more flexibility, but Neurogebra is much simpler to learn.</p>"},{"location":"projects/project1-linear-regression/#step-5-evaluate-on-test-data","title":"Step 5: Evaluate on Test Data","text":"NeurogebraPyTorch <pre><code># Predict on test data\ny_pred = np.array([model.eval(x=float(xi)) for xi in X_test])\n\n# Calculate test MSE\ntest_mse = np.mean((y_pred - y_test) ** 2)\n\n# Calculate R\u00b2 score\nss_res = np.sum((y_test - y_pred) ** 2)\nss_tot = np.sum((y_test - np.mean(y_test)) ** 2)\nr2 = 1 - ss_res / ss_tot\n\nprint(f\"Test MSE:  {test_mse:.4f}\")\nprint(f\"R\u00b2 Score:  {r2:.4f}\")\nprint(f\"\\nSample predictions:\")\nfor i in range(5):\n    print(f\"  x={X_test[i]:.2f} \u2192 predicted={y_pred[i]:.1f}, actual={y_test[i]:.1f}\")\n</code></pre> <pre><code># Predict on test data\nwith torch.no_grad():\n    y_pred_t = model(X_test_t)\ny_pred = y_pred_t.numpy().flatten()\n\n# Calculate test MSE\ntest_mse = np.mean((y_pred - y_test) ** 2)\n\n# Calculate R\u00b2 score\nss_res = np.sum((y_test - y_pred) ** 2)\nss_tot = np.sum((y_test - np.mean(y_test)) ** 2)\nr2 = 1 - ss_res / ss_tot\n\nprint(f\"Test MSE:  {test_mse:.4f}\")\nprint(f\"R\u00b2 Score:  {r2:.4f}\")\nprint(f\"\\nSample predictions:\")\nfor i in range(5):\n    print(f\"  x={X_test[i]:.2f} \u2192 predicted={y_pred[i]:.1f}, actual={y_test[i]:.1f}\")\n</code></pre> <p>Key Difference #5</p> <p>PyTorch requires <code>torch.no_grad()</code> context and converting back to NumPy. Neurogebra evaluates directly \u2014 no special context needed.</p>"},{"location":"projects/project1-linear-regression/#step-6-visualize-results","title":"Step 6: Visualize Results","text":"NeurogebraPyTorch <pre><code>import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: Training Loss\naxes[0].plot(history[\"loss\"], linewidth=2)\naxes[0].set_xlabel(\"Epoch\")\naxes[0].set_ylabel(\"Loss (MSE)\")\naxes[0].set_title(\"Training Loss \u2014 Neurogebra\")\naxes[0].set_yscale(\"log\")\naxes[0].grid(True, alpha=0.3)\n\n# Plot 2: Predictions vs Actual\nx_line = np.linspace(X_test.min(), X_test.max(), 100)\ny_line = np.array([model.eval(x=float(xi)) for xi in x_line])\n\naxes[1].scatter(X_test, y_test, alpha=0.7, label=\"Actual\", color=\"blue\")\naxes[1].plot(x_line, y_line, color=\"red\", linewidth=2, label=\"Predicted\")\naxes[1].set_xlabel(\"House Size (normalized)\")\naxes[1].set_ylabel(\"Price ($1000s)\")\naxes[1].set_title(\"Predictions \u2014 Neurogebra\")\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: Training Loss\naxes[0].plot(history[\"loss\"], linewidth=2)\naxes[0].set_xlabel(\"Epoch\")\naxes[0].set_ylabel(\"Loss (MSE)\")\naxes[0].set_title(\"Training Loss \u2014 PyTorch\")\naxes[0].set_yscale(\"log\")\naxes[0].grid(True, alpha=0.3)\n\n# Plot 2: Predictions vs Actual\nx_line = np.linspace(X_test.min(), X_test.max(), 100)\nx_line_t = torch.tensor(x_line, dtype=torch.float32).unsqueeze(1)\nwith torch.no_grad():\n    y_line = model(x_line_t).numpy().flatten()\n\naxes[1].scatter(X_test, y_test, alpha=0.7, label=\"Actual\", color=\"blue\")\naxes[1].plot(x_line, y_line, color=\"red\", linewidth=2, label=\"Predicted\")\naxes[1].set_xlabel(\"House Size (normalized)\")\naxes[1].set_ylabel(\"Price ($1000s)\")\naxes[1].set_title(\"Predictions \u2014 PyTorch\")\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"projects/project1-linear-regression/#step-7-understand-what-you-built-neurogebra-bonus","title":"Step 7: Understand What You Built (Neurogebra Bonus)","text":"<p>This is where Neurogebra really shines \u2014 understanding and introspection:</p> <pre><code>from neurogebra import MathForge\n\nforge = MathForge()\n\n# Explain the model\nprint(\"=== Your Model ===\")\nprint(f\"Formula: y = {model.params['w']:.2f} * x + {model.params['b']:.2f}\")\nprint(f\"Symbolic: {model.symbolic_expr}\")\nprint()\n\n# Examine the gradient\ngrad = model.gradient(\"x\")\nprint(f\"Gradient dy/dx = {grad.symbolic_expr}\")\nprint(f\"This means: for every 1 unit increase in x, y increases by {model.params['w']:.2f}\")\nprint()\n\n# Examine the loss function\nmse = forge.get(\"mse\")\nprint(f\"Loss function: {mse.symbolic_expr}\")\nprint(f\"Loss gradient: {mse.gradient('y_pred').symbolic_expr}\")\nprint()\n\n# Understand activations used\nprint(\"=== Available Activations You Could Add ===\")\nfor name in [\"relu\", \"sigmoid\", \"tanh\"]:\n    act = forge.get(name)\n    print(f\"  {name}: {act.symbolic_expr}\")\n</code></pre> <p>Educational Value</p> <p>In PyTorch, you can't easily inspect formulas or see gradients symbolically. Neurogebra shows you exactly what's happening at every step.</p>"},{"location":"projects/project1-linear-regression/#full-side-by-side-comparison","title":"Full Side-by-Side Comparison","text":"Aspect Neurogebra PyTorch Data format NumPy arrays torch.Tensor Model definition Math formula: <code>\"w*x + b\"</code> <code>nn.Linear(1, 1)</code> Lines for training 1 (<code>trainer.fit(...)</code>) ~10 (manual loop) See the formula \u2705 <code>model.symbolic_expr</code> \u274c Hidden in module See gradients \u2705 <code>model.gradient(\"x\")</code> \u274c Only numerical values Total lines of code ~15 ~35 Learning curve Gentle Steep Production ready Educational Production GPU support Via bridges Native"},{"location":"projects/project1-linear-regression/#what-you-learned","title":"What You Learned","text":"<ol> <li>Linear regression fits <code>y = wx + b</code> to data</li> <li>Training = adjusting w and b to minimize loss</li> <li>MSE loss measures average squared error</li> <li>Adam optimizer efficiently updates parameters</li> <li>Neurogebra lets you see and understand every step</li> <li>PyTorch gives more control but requires more code</li> </ol> <p>Next Project: Image Classifier \u2192</p>"},{"location":"projects/project2-image-classifier/","title":"Project 2: Image Classifier \u2014 Neurogebra vs PyTorch","text":"<p>Build a digit classifier using the MNIST-style dataset. We'll train a model to recognize handwritten digits (0-9) comparing both frameworks side by side.</p>"},{"location":"projects/project2-image-classifier/#goal","title":"\ud83c\udfaf Goal","text":"<pre><code>Input:  8x8 pixel image of a handwritten digit\nOutput: Which digit (0-9) it represents\n</code></pre>"},{"location":"projects/project2-image-classifier/#step-1-load-the-dataset","title":"Step 1: Load the Dataset","text":"NeurogebraPyTorch <pre><code>import numpy as np\nfrom sklearn.datasets import load_digits\n\n# Load sklearn's built-in digits dataset (8x8 images)\ndigits = load_digits()\nX = digits.data          # (1797, 64) \u2014 flattened 8x8 images\ny = digits.target        # (1797,) \u2014 labels 0-9\n\n# Normalize pixel values to 0-1\nX = X / 16.0  # Max pixel value in this dataset is 16\n\n# Train/test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f\"Training: {X_train.shape[0]} images\")\nprint(f\"Testing:  {X_test.shape[0]} images\")\nprint(f\"Image shape: 8\u00d78 = {X_train.shape[1]} features\")\nprint(f\"Classes: {np.unique(y_train)}\")\n</code></pre> <pre><code>import numpy as np\nimport torch\nimport torch.nn as nn\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\n# Load sklearn's built-in digits dataset (8x8 images)\ndigits = load_digits()\nX = digits.data / 16.0\ny = digits.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Convert to PyTorch tensors\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.long)\nX_test_t  = torch.tensor(X_test, dtype=torch.float32)\ny_test_t  = torch.tensor(y_test, dtype=torch.long)\n\nprint(f\"Training: {X_train_t.shape[0]} images\")\nprint(f\"Testing:  {X_test_t.shape[0]} images\")\nprint(f\"Image shape: {X_train_t.shape[1]} features\")\n</code></pre>"},{"location":"projects/project2-image-classifier/#step-2-visualize-sample-images","title":"Step 2: Visualize Sample Images","text":"<pre><code>import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X_train[i].reshape(8, 8), cmap='gray')\n    ax.set_title(f\"Label: {y_train[i]}\", fontsize=12)\n    ax.axis('off')\nplt.suptitle(\"Sample Training Images\", fontsize=14)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"projects/project2-image-classifier/#step-3-build-the-neural-network","title":"Step 3: Build the Neural Network","text":"NeurogebraPyTorch <pre><code>from neurogebra.builders.model_builder import ModelBuilder\n\nbuilder = ModelBuilder()\n\n# Build a simple classifier\nmodel = builder.sequential([\n    {\"type\": \"dense\", \"units\": 64, \"activation\": \"relu\", \"input_shape\": (64,)},\n    {\"type\": \"dense\", \"units\": 32, \"activation\": \"relu\"},\n    {\"type\": \"dense\", \"units\": 10, \"activation\": \"softmax\"}\n])\n\nprint(model.summary())\n</code></pre> <p>What this means (in plain English):</p> <ul> <li>Layer 1: Takes 64 inputs (pixels), outputs 64 values, applies ReLU</li> <li>Layer 2: Takes 64 values, compresses to 32, applies ReLU</li> <li>Layer 3: Takes 32 values, outputs 10 (one per digit), applies Softmax (probabilities)</li> </ul> <pre><code>import torch.nn as nn\n\nclass DigitClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 10)\n        )\n\n    def forward(self, x):\n        return self.network(x)\n\nmodel = nn.Module()\nmodel = DigitClassifier()\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params:,}\")\nprint(model)\n</code></pre> <p>Key Difference</p> <p>Neurogebra: Declarative style \u2014 describe what you want with a dictionary. PyTorch: Class-based \u2014 define a class with <code>__init__</code> and <code>forward</code> methods.</p> <p>For beginners, Neurogebra's approach is much more intuitive.</p>"},{"location":"projects/project2-image-classifier/#step-4-implement-training-from-scratch-with-autograd","title":"Step 4: Implement Training from Scratch with Autograd","text":"<p>Since image classification requires a more hands-on approach, let's build a complete neural network from scratch using Neurogebra's autograd engine \u2014 and compare with PyTorch.</p> NeurogebraPyTorch <pre><code>from neurogebra.core.autograd import Value\nimport random\n\nclass Neuron:\n    \"\"\"Single neuron with weights, bias, and activation.\"\"\"\n    def __init__(self, n_inputs):\n        self.w = [Value(random.uniform(-1, 1) * (2/n_inputs)**0.5) for _ in range(n_inputs)]\n        self.b = Value(0.0)\n\n    def __call__(self, x):\n        # w \u00b7 x + b\n        activation = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n        return activation.relu()\n\n    def parameters(self):\n        return self.w + [self.b]\n\nclass Layer:\n    \"\"\"Layer of neurons.\"\"\"\n    def __init__(self, n_inputs, n_outputs):\n        self.neurons = [Neuron(n_inputs) for _ in range(n_outputs)]\n\n    def __call__(self, x):\n        return [n(x) for n in self.neurons]\n\n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\nclass MLP:\n    \"\"\"Multi-Layer Perceptron.\"\"\"\n    def __init__(self, sizes):\n        self.layers = [Layer(sizes[i], sizes[i+1]) for i in range(len(sizes)-1)]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]\n\n# Create network: 64 \u2192 32 \u2192 10\nmlp = MLP([64, 32, 10])\n\nprint(f\"Total parameters: {len(mlp.parameters()):,}\")\nprint(f\"Architecture: 64 \u2192 32 \u2192 10\")\n</code></pre> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nmodel = DigitClassifier()  # Defined in Step 3\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Architecture: 64 \u2192 64 \u2192 32 \u2192 10\")\n</code></pre>"},{"location":"projects/project2-image-classifier/#step-5-train-the-network","title":"Step 5: Train the Network","text":"NeurogebraPyTorch <pre><code># Training loop with Neurogebra autograd\nlearning_rate = 0.01\nbatch_size = 32\nepochs = 20\nhistory = {\"loss\": [], \"accuracy\": []}\n\nfor epoch in range(epochs):\n    # Shuffle data\n    indices = list(range(len(X_train)))\n    random.shuffle(indices)\n\n    epoch_loss = 0.0\n    correct = 0\n    total = 0\n\n    # Mini-batch training\n    for start in range(0, min(len(X_train), 320), batch_size):  # Use subset for speed\n        batch_idx = indices[start:start + batch_size]\n        batch_loss = Value(0.0)\n\n        for idx in batch_idx:\n            x = [Value(float(v)) for v in X_train[idx]]\n\n            # Forward pass\n            scores = mlp(x)\n\n            # Simple cross-entropy-like loss\n            # Softmax + negative log likelihood\n            target = int(y_train[idx])\n\n            # Compute max for numerical stability\n            max_score = max(s.data for s in scores)\n            exp_scores = [(s - Value(max_score)).exp() for s in scores]\n            sum_exp = sum(exp_scores)\n\n            # Loss = -log(probability of correct class)\n            prob_correct = exp_scores[target] / sum_exp\n            sample_loss = -(prob_correct.log())\n            batch_loss = batch_loss + sample_loss\n\n            # Track accuracy\n            predicted = max(range(10), key=lambda i: scores[i].data)\n            correct += (predicted == target)\n            total += 1\n\n        # Average loss\n        batch_loss = batch_loss / len(batch_idx)\n\n        # Backward pass\n        for p in mlp.parameters():\n            p.grad = 0.0\n        batch_loss.backward()\n\n        # Update weights (SGD)\n        for p in mlp.parameters():\n            p.data -= learning_rate * p.grad\n\n        epoch_loss += batch_loss.data\n\n    accuracy = correct / total if total &gt; 0 else 0\n    history[\"loss\"].append(epoch_loss)\n    history[\"accuracy\"].append(accuracy)\n\n    if epoch % 5 == 0 or epoch == epochs - 1:\n        print(f\"Epoch {epoch:&gt;3d}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {accuracy:.1%}\")\n</code></pre> <pre><code># Training loop with PyTorch\nepochs = 20\nbatch_size = 32\nhistory = {\"loss\": [], \"accuracy\": []}\n\nfor epoch in range(epochs):\n    # Shuffle data\n    perm = torch.randperm(len(X_train_t))\n    X_shuffled = X_train_t[perm]\n    y_shuffled = y_train_t[perm]\n\n    epoch_loss = 0.0\n    correct = 0\n    total = 0\n\n    # Mini-batch training\n    for start in range(0, len(X_train_t), batch_size):\n        X_batch = X_shuffled[start:start + batch_size]\n        y_batch = y_shuffled[start:start + batch_size]\n\n        # Forward pass\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n        # Track accuracy\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == y_batch).sum().item()\n        total += y_batch.size(0)\n\n    accuracy = correct / total\n    history[\"loss\"].append(epoch_loss)\n    history[\"accuracy\"].append(accuracy)\n\n    if epoch % 5 == 0 or epoch == epochs - 1:\n        print(f\"Epoch {epoch:&gt;3d}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {accuracy:.1%}\")\n</code></pre> <p>Key Difference</p> <p>Neurogebra's autograd: You see every computation step \u2014 the forward pass, softmax,  cross-entropy, and gradient updates are all explicit. This is extremely educational  because you understand exactly what backpropagation does.</p> <p>PyTorch: <code>loss.backward()</code> and <code>optimizer.step()</code> handle everything \u2014 faster but opaque.</p>"},{"location":"projects/project2-image-classifier/#step-6-evaluate-on-test-data","title":"Step 6: Evaluate on Test Data","text":"NeurogebraPyTorch <pre><code># Test evaluation\ncorrect = 0\ntotal = 0\npredictions = []\n\nfor i in range(len(X_test)):\n    x = [Value(float(v)) for v in X_test[i]]\n    scores = mlp(x)\n    predicted = max(range(10), key=lambda j: scores[j].data)\n    predictions.append(predicted)\n\n    if predicted == y_test[i]:\n        correct += 1\n    total += 1\n\ntest_accuracy = correct / total\nprint(f\"\\n=== Test Results (Neurogebra) ===\")\nprint(f\"Test Accuracy: {test_accuracy:.1%}\")\nprint(f\"Correct: {correct}/{total}\")\n</code></pre> <pre><code># Test evaluation\nwith torch.no_grad():\n    outputs = model(X_test_t)\n    _, predicted = torch.max(outputs, 1)\n    correct = (predicted == y_test_t).sum().item()\n    total = y_test_t.size(0)\n\ntest_accuracy = correct / total\nprint(f\"\\n=== Test Results (PyTorch) ===\")\nprint(f\"Test Accuracy: {test_accuracy:.1%}\")\nprint(f\"Correct: {correct}/{total}\")\n</code></pre>"},{"location":"projects/project2-image-classifier/#step-7-confusion-matrix-analysis","title":"Step 7: Confusion Matrix &amp; Analysis","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# Using Neurogebra predictions (or PyTorch \u2014 same visualization code)\npredictions = np.array(predictions)  # from Neurogebra evaluation above\n\n# Confusion matrix\nconfusion = np.zeros((10, 10), dtype=int)\nfor true, pred in zip(y_test, predictions):\n    confusion[true][pred] += 1\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Plot confusion matrix\nim = axes[0].imshow(confusion, cmap='Blues')\naxes[0].set_xlabel(\"Predicted\")\naxes[0].set_ylabel(\"Actual\")\naxes[0].set_title(\"Confusion Matrix\")\naxes[0].set_xticks(range(10))\naxes[0].set_yticks(range(10))\nfor i in range(10):\n    for j in range(10):\n        axes[0].text(j, i, str(confusion[i][j]), \n                    ha='center', va='center', fontsize=8)\nplt.colorbar(im, ax=axes[0])\n\n# Plot per-class accuracy\nclass_acc = confusion.diagonal() / confusion.sum(axis=1)\naxes[1].bar(range(10), class_acc, color='steelblue')\naxes[1].set_xlabel(\"Digit\")\naxes[1].set_ylabel(\"Accuracy\")\naxes[1].set_title(\"Per-Digit Accuracy\")\naxes[1].set_xticks(range(10))\naxes[1].set_ylim(0, 1.1)\nfor i, acc in enumerate(class_acc):\n    axes[1].text(i, acc + 0.02, f\"{acc:.0%}\", ha='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"projects/project2-image-classifier/#step-8-visualize-predictions","title":"Step 8: Visualize Predictions","text":"<pre><code># Show some predictions\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X_test[i].reshape(8, 8), cmap='gray')\n    true_label = y_test[i]\n    pred_label = predictions[i]\n\n    color = 'green' if true_label == pred_label else 'red'\n    ax.set_title(f\"True: {true_label}, Pred: {pred_label}\", \n                color=color, fontsize=11, fontweight='bold')\n    ax.axis('off')\n\nplt.suptitle(\"Test Predictions (Green=Correct, Red=Wrong)\", fontsize=14)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"projects/project2-image-classifier/#understanding-what-the-network-learned-neurogebra-bonus","title":"Understanding What the Network Learned (Neurogebra Bonus)","text":"<pre><code>from neurogebra import MathForge\n\nforge = MathForge()\n\n# Understand the math behind each component\nprint(\"=== What You Built ===\\n\")\n\n# 1. ReLU activation explanation\nrelu = forge.get(\"relu\")\nprint(f\"Activation: ReLU = {relu.symbolic_expr}\")\nprint(f\"  Gradient: {relu.gradient('x').symbolic_expr}\")\nprint(f\"  Purpose: Adds non-linearity, kills negative values\\n\")\n\n# 2. Understanding the loss\nprint(\"Cross-Entropy Loss:\")\nprint(\"  L = -log(P(correct class))\")\nprint(\"  If model is confident and correct \u2192 small loss\")\nprint(\"  If model is wrong \u2192 large loss\\n\")\n\n# 3. Understanding softmax\nprint(\"Softmax converts scores to probabilities:\")\nprint(\"  P(class_i) = exp(score_i) / sum(exp(all_scores))\")\nprint(\"  All probabilities sum to 1.0\\n\")\n\n# 4. Network architecture\nprint(\"Your network architecture:\")\nprint(\"  Input (64 pixels) \u2192 Dense(32) + ReLU \u2192 Dense(10) + Softmax \u2192 Prediction\")\nprint(f\"  Total parameters: {len(mlp.parameters()):,}\")\n</code></pre>"},{"location":"projects/project2-image-classifier/#side-by-side-summary","title":"Side-by-Side Summary","text":"Aspect Neurogebra PyTorch Data prep NumPy only NumPy \u2192 Tensor conversion Model definition Dict-based or class-based Class-based (nn.Module) Training loop Explicit computation visible Abstracted behind <code>.backward()</code> Evaluation Manual iteration Vectorized with <code>torch.no_grad()</code> Speed Slower (educational) Fast (optimized C++) Understanding \u2705 See every gradient \u274c Gradients are hidden Best for Learning ML concepts Production ML"},{"location":"projects/project2-image-classifier/#what-you-learned","title":"What You Learned","text":"<ol> <li>How image classification works (pixels \u2192 features \u2192 class)</li> <li>How neural networks process information through layers</li> <li>How softmax converts scores to probabilities</li> <li>How cross-entropy loss measures classification error</li> <li>How backpropagation flows through a multi-layer network</li> <li>The trade-off between educational clarity and production speed</li> </ol> <p>Next Project: Neural Network from Scratch \u2192</p>"},{"location":"projects/project3-neural-network/","title":"Project 3: Neural Network from Scratch \u2014 Neurogebra vs PyTorch","text":"<p>Build a complete neural network from scratch to solve a real classification problem \u2014 understanding every single component. This is the ultimate learning project.</p>"},{"location":"projects/project3-neural-network/#goal","title":"\ud83c\udfaf Goal","text":"<p>Create a neural network that classifies points into spiral categories \u2014 a problem that cannot be solved with linear models.</p> <pre><code>Input:  (x, y) coordinate on a 2D plane\nOutput: Which spiral (0, 1, or 2) the point belongs to\n</code></pre> <p>This is a classic non-linear classification problem that truly tests neural network capability.</p>"},{"location":"projects/project3-neural-network/#step-1-generate-the-spiral-dataset","title":"Step 1: Generate the Spiral Dataset","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_spirals(n_points=100, n_classes=3, noise=0.1):\n    \"\"\"Generate spiral dataset \u2014 a classic non-linear classification problem.\"\"\"\n    X = np.zeros((n_points * n_classes, 2))\n    y = np.zeros(n_points * n_classes, dtype=int)\n\n    for class_idx in range(n_classes):\n        start = n_points * class_idx\n        end = n_points * (class_idx + 1)\n\n        r = np.linspace(0.0, 1.0, n_points)\n        theta = np.linspace(\n            class_idx * 4, (class_idx + 1) * 4, n_points\n        ) + np.random.randn(n_points) * noise\n\n        X[start:end, 0] = r * np.sin(theta)\n        X[start:end, 1] = r * np.cos(theta)\n        y[start:end] = class_idx\n\n    return X, y\n\n# Generate data\nnp.random.seed(42)\nX, y = generate_spirals(n_points=100, n_classes=3, noise=0.15)\n\n# Train/test split\nindices = np.random.permutation(len(X))\nsplit = int(0.8 * len(X))\nX_train, X_test = X[indices[:split]], X[indices[split:]]\ny_train, y_test = y[indices[:split]], y[indices[split:]]\n\n# Visualize\nplt.figure(figsize=(8, 8))\ncolors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\nfor i in range(3):\n    mask = y_train == i\n    plt.scatter(X_train[mask, 0], X_train[mask, 1], \n               c=colors[i], label=f'Class {i}', alpha=0.7, s=30)\nplt.title(\"Spiral Dataset \u2014 Can Your Neural Network Solve This?\", fontsize=14)\nplt.xlabel(\"x\u2081\")\nplt.ylabel(\"x\u2082\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.show()\n\nprint(f\"Training: {len(X_train)} samples\")\nprint(f\"Testing:  {len(X_test)} samples\")\nprint(f\"Classes:  3 spirals\")\nprint(f\"Features: 2 (x, y coordinates)\")\n</code></pre> <p>Why Spirals?</p> <p>A linear model draws straight lines to separate classes. Spirals are intertwined \u2014  you need a neural network with non-linear activations to separate them. This proves  your network actually works!</p>"},{"location":"projects/project3-neural-network/#step-2-build-the-neural-network","title":"Step 2: Build the Neural Network","text":""},{"location":"projects/project3-neural-network/#neurogebra-every-component-visible","title":"Neurogebra \u2014 Every Component Visible","text":"<pre><code>from neurogebra.core.autograd import Value\nimport random\n\nrandom.seed(42)\n\nclass Neuron:\n    \"\"\"A single neuron: computes w\u00b7x + b, then applies activation.\"\"\"\n\n    def __init__(self, n_inputs, activation='relu'):\n        # Xavier initialization for better training\n        limit = (6 / (n_inputs + 1)) ** 0.5\n        self.w = [Value(random.uniform(-limit, limit)) for _ in range(n_inputs)]\n        self.b = Value(0.0)\n        self.activation = activation\n\n    def __call__(self, x):\n        # Linear: w\u00b7x + b\n        raw = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n\n        # Activation\n        if self.activation == 'relu':\n            return raw.relu()\n        elif self.activation == 'tanh':\n            return raw.tanh()\n        elif self.activation == 'linear':\n            return raw\n        return raw.relu()\n\n    def parameters(self):\n        return self.w + [self.b]\n\n\nclass Layer:\n    \"\"\"A layer of neurons.\"\"\"\n\n    def __init__(self, n_inputs, n_outputs, activation='relu'):\n        self.neurons = [Neuron(n_inputs, activation) for _ in range(n_outputs)]\n\n    def __call__(self, x):\n        return [neuron(x) for neuron in self.neurons]\n\n    def parameters(self):\n        return [p for neuron in self.neurons for p in neuron.parameters()]\n\n\nclass NeuralNetwork:\n    \"\"\"Complete neural network with multiple layers.\"\"\"\n\n    def __init__(self, layer_sizes, activations=None):\n        \"\"\"\n        Args:\n            layer_sizes: [input_size, hidden1, hidden2, ..., output_size]\n            activations: activation for each layer (default: relu, linear for last)\n        \"\"\"\n        if activations is None:\n            activations = ['relu'] * (len(layer_sizes) - 2) + ['linear']\n\n        self.layers = []\n        for i in range(len(layer_sizes) - 1):\n            self.layers.append(\n                Layer(layer_sizes[i], layer_sizes[i+1], activations[i])\n            )\n\n        n_params = len(self.parameters())\n        print(f\"Neural Network created:\")\n        print(f\"  Architecture: {' \u2192 '.join(map(str, layer_sizes))}\")\n        print(f\"  Activations:  {activations}\")\n        print(f\"  Parameters:   {n_params:,}\")\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]\n\n\n# Create neural network: 2 inputs \u2192 16 hidden \u2192 16 hidden \u2192 3 outputs\nnn_neuro = NeuralNetwork(\n    layer_sizes=[2, 16, 16, 3],\n    activations=['relu', 'relu', 'linear']\n)\n</code></pre>"},{"location":"projects/project3-neural-network/#pytorch-the-standard-way","title":"PyTorch \u2014 The Standard Way","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ntorch.manual_seed(42)\n\nclass SpiralNet(nn.Module):\n    \"\"\"Neural network for spiral classification.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(2, 16),\n            nn.ReLU(),\n            nn.Linear(16, 16),\n            nn.ReLU(),\n            nn.Linear(16, 3)\n        )\n\n    def forward(self, x):\n        return self.network(x)\n\nnn_torch = SpiralNet()\ntotal_params = sum(p.numel() for p in nn_torch.parameters())\nprint(f\"\\nPyTorch Neural Network:\")\nprint(f\"  Architecture: 2 \u2192 16 \u2192 16 \u2192 3\")\nprint(f\"  Parameters:   {total_params:,}\")\n</code></pre> <p>Architecture Comparison</p> <p>Both networks have the same structure: 2 \u2192 16 \u2192 16 \u2192 3. Neurogebra: ~35 lines to define (you understand every line). PyTorch: ~15 lines (uses pre-built components).</p>"},{"location":"projects/project3-neural-network/#step-3-implement-softmax-cross-entropy-loss","title":"Step 3: Implement Softmax &amp; Cross-Entropy Loss","text":"NeurogebraPyTorch <pre><code>def softmax_cross_entropy(scores, target):\n    \"\"\"\n    Compute softmax probabilities and cross-entropy loss.\n\n    This is EXACTLY what PyTorch's CrossEntropyLoss does internally,\n    but here you can see every step!\n\n    Steps:\n    1. Find max score (for numerical stability)\n    2. Compute exp(score - max) for each class\n    3. Normalize to get probabilities (softmax)\n    4. Loss = -log(probability of correct class)\n    \"\"\"\n    # Step 1: Numerical stability \u2014 subtract max\n    max_score = max(s.data for s in scores)\n\n    # Step 2: Compute exponentials\n    exp_scores = [(s - Value(max_score)).exp() for s in scores]\n\n    # Step 3: Softmax \u2014 normalize to probabilities\n    sum_exp = sum(exp_scores)\n    probabilities = [e / sum_exp for e in exp_scores]\n\n    # Step 4: Cross-entropy loss\n    # -log(P(correct class))\n    loss = -(probabilities[target].log())\n\n    return loss, probabilities\n\n# Example: verify it works\ndummy_scores = [Value(2.0), Value(1.0), Value(0.1)]  # Raw scores for 3 classes\nloss, probs = softmax_cross_entropy(dummy_scores, target=0)  # Target = class 0\n\nprint(\"=== Softmax + Cross-Entropy Demo ===\")\nprint(f\"Raw scores: [{', '.join(f'{s.data:.1f}' for s in dummy_scores)}]\")\nprint(f\"Probabilities: [{', '.join(f'{p.data:.3f}' for p in probs)}]\")\nprint(f\"Sum of probs: {sum(p.data for p in probs):.3f} (should be 1.0)\")\nprint(f\"Loss (target=class 0): {loss.data:.4f}\")\nprint(f\"(Lower loss = higher confidence in correct class)\")\n</code></pre> <pre><code># PyTorch does this in ONE line:\ncriterion = nn.CrossEntropyLoss()\n\n# That single line contains all the softmax + cross-entropy math!\n# Convenient, but you don't see the internals.\n\n# Demo:\ndummy_scores = torch.tensor([[2.0, 1.0, 0.1]])\ndummy_target = torch.tensor([0])\nloss = criterion(dummy_scores, dummy_target)\n\nprobs = torch.softmax(dummy_scores, dim=1)\nprint(f\"Raw scores:    {dummy_scores.numpy()}\")\nprint(f\"Probabilities: {probs.numpy()}\")\nprint(f\"Loss:          {loss.item():.4f}\")\n</code></pre>"},{"location":"projects/project3-neural-network/#step-4-train-both-networks","title":"Step 4: Train Both Networks","text":""},{"location":"projects/project3-neural-network/#neurogebra-training","title":"Neurogebra Training","text":"<pre><code># \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# NEUROGEBRA TRAINING \u2014 See every gradient flow!\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nlearning_rate = 0.05\nepochs = 30\nbatch_size = 16\nneuro_history = {\"loss\": [], \"accuracy\": []}\n\nprint(\"Training Neurogebra Neural Network...\")\nprint(\"=\" * 50)\n\nfor epoch in range(epochs):\n    # Shuffle training data\n    perm = np.random.permutation(len(X_train))\n\n    epoch_loss = 0.0\n    correct = 0\n    total = 0\n\n    # Mini-batch training\n    for start in range(0, len(X_train), batch_size):\n        batch_idx = perm[start:start + batch_size]\n        batch_loss = Value(0.0)\n\n        for idx in batch_idx:\n            # Convert input to Value objects\n            x_input = [Value(float(X_train[idx, 0])),\n                       Value(float(X_train[idx, 1]))]\n            target = int(y_train[idx])\n\n            # Forward pass \u2014 compute scores\n            scores = nn_neuro(x_input)\n\n            # Compute loss\n            loss, probs = softmax_cross_entropy(scores, target)\n            batch_loss = batch_loss + loss\n\n            # Track accuracy\n            predicted = max(range(3), key=lambda i: scores[i].data)\n            correct += (predicted == target)\n            total += 1\n\n        # Average batch loss\n        batch_loss = batch_loss / len(batch_idx)\n\n        # ==============================\n        # BACKWARD PASS \u2014 The Magic Part\n        # ==============================\n\n        # 1. Zero all gradients\n        for p in nn_neuro.parameters():\n            p.grad = 0.0\n\n        # 2. Backpropagate \u2014 compute dLoss/dParam for every parameter\n        batch_loss.backward()\n\n        # 3. Update every parameter: param = param - lr * gradient\n        for p in nn_neuro.parameters():\n            p.data -= learning_rate * p.grad\n\n        epoch_loss += batch_loss.data\n\n    accuracy = correct / total\n    neuro_history[\"loss\"].append(epoch_loss)\n    neuro_history[\"accuracy\"].append(accuracy)\n\n    if epoch % 5 == 0 or epoch == epochs - 1:\n        print(f\"  Epoch {epoch:&gt;3d}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {accuracy:.1%}\")\n\nprint(f\"\\nFinal Training Accuracy: {neuro_history['accuracy'][-1]:.1%}\")\n</code></pre>"},{"location":"projects/project3-neural-network/#pytorch-training","title":"PyTorch Training","text":"<pre><code># \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# PYTORCH TRAINING \u2014 Fast and optimized\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.long)\n\noptimizer = optim.Adam(nn_torch.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()\n\ntorch_history = {\"loss\": [], \"accuracy\": []}\n\nprint(\"Training PyTorch Neural Network...\")\nprint(\"=\" * 50)\n\nfor epoch in range(epochs):\n    perm = torch.randperm(len(X_train_t))\n\n    epoch_loss = 0.0\n    correct = 0\n    total = 0\n\n    for start in range(0, len(X_train_t), batch_size):\n        X_batch = X_train_t[perm[start:start + batch_size]]\n        y_batch = y_train_t[perm[start:start + batch_size]]\n\n        # Forward pass\n        outputs = nn_torch(X_batch)\n        loss = criterion(outputs, y_batch)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == y_batch).sum().item()\n        total += y_batch.size(0)\n\n    accuracy = correct / total\n    torch_history[\"loss\"].append(epoch_loss)\n    torch_history[\"accuracy\"].append(accuracy)\n\n    if epoch % 5 == 0 or epoch == epochs - 1:\n        print(f\"  Epoch {epoch:&gt;3d}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {accuracy:.1%}\")\n\nprint(f\"\\nFinal Training Accuracy: {torch_history['accuracy'][-1]:.1%}\")\n</code></pre>"},{"location":"projects/project3-neural-network/#step-5-compare-training-progress","title":"Step 5: Compare Training Progress","text":"<pre><code>fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Loss comparison\naxes[0].plot(neuro_history[\"loss\"], label=\"Neurogebra\", linewidth=2, color='#FF6B6B')\naxes[0].plot(torch_history[\"loss\"], label=\"PyTorch\", linewidth=2, color='#4ECDC4')\naxes[0].set_xlabel(\"Epoch\")\naxes[0].set_ylabel(\"Loss\")\naxes[0].set_title(\"Training Loss Comparison\")\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Accuracy comparison\naxes[1].plot(neuro_history[\"accuracy\"], label=\"Neurogebra\", linewidth=2, color='#FF6B6B')\naxes[1].plot(torch_history[\"accuracy\"], label=\"PyTorch\", linewidth=2, color='#4ECDC4')\naxes[1].set_xlabel(\"Epoch\")\naxes[1].set_ylabel(\"Accuracy\")\naxes[1].set_title(\"Training Accuracy Comparison\")\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].set_ylim(0, 1.05)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"projects/project3-neural-network/#step-6-evaluate-on-test-data","title":"Step 6: Evaluate on Test Data","text":"NeurogebraPyTorch <pre><code># Neurogebra test evaluation\nneuro_correct = 0\nneuro_predictions = []\n\nfor i in range(len(X_test)):\n    x_input = [Value(float(X_test[i, 0])), Value(float(X_test[i, 1]))]\n    scores = nn_neuro(x_input)\n    predicted = max(range(3), key=lambda j: scores[j].data)\n    neuro_predictions.append(predicted)\n    neuro_correct += (predicted == y_test[i])\n\nneuro_test_acc = neuro_correct / len(X_test)\nprint(f\"Neurogebra Test Accuracy: {neuro_test_acc:.1%}\")\n</code></pre> <pre><code># PyTorch test evaluation\nX_test_t = torch.tensor(X_test, dtype=torch.float32)\ny_test_t = torch.tensor(y_test, dtype=torch.long)\n\nwith torch.no_grad():\n    outputs = nn_torch(X_test_t)\n    _, torch_predictions = torch.max(outputs, 1)\n\ntorch_test_acc = (torch_predictions == y_test_t).float().mean().item()\ntorch_predictions = torch_predictions.numpy()\nprint(f\"PyTorch Test Accuracy: {torch_test_acc:.1%}\")\n</code></pre>"},{"location":"projects/project3-neural-network/#step-7-visualize-decision-boundaries","title":"Step 7: Visualize Decision Boundaries","text":"<p>This is the most satisfying visualization \u2014 see how the network learned to separate spirals:</p> <pre><code>def plot_decision_boundary(predict_fn, X, y, title, ax):\n    \"\"\"Plot the decision boundary of a classifier.\"\"\"\n    h = 0.02  # Step size\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n\n    xx, yy = np.meshgrid(\n        np.arange(x_min, x_max, h),\n        np.arange(y_min, y_max, h)\n    )\n\n    # Predict on grid\n    grid_points = np.c_[xx.ravel(), yy.ravel()]\n    Z = np.array([predict_fn(p) for p in grid_points])\n    Z = Z.reshape(xx.shape)\n\n    # Plot\n    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n\n    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n    for i in range(3):\n        mask = y == i\n        ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], \n                  label=f'Class {i}', edgecolors='k', linewidth=0.5, s=30)\n\n    ax.set_title(title, fontsize=13, fontweight='bold')\n    ax.legend(loc='upper right')\n    ax.set_xlabel(\"x\u2081\")\n    ax.set_ylabel(\"x\u2082\")\n\n# Prediction functions\ndef neuro_predict(point):\n    x = [Value(float(point[0])), Value(float(point[1]))]\n    scores = nn_neuro(x)\n    return max(range(3), key=lambda i: scores[i].data)\n\ndef torch_predict(point):\n    x = torch.tensor(point, dtype=torch.float32).unsqueeze(0)\n    with torch.no_grad():\n        scores = nn_torch(x)\n    return torch.argmax(scores).item()\n\n# Plot both decision boundaries\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\nplot_decision_boundary(neuro_predict, X_test, y_test, \n                       f\"Neurogebra (Acc: {neuro_test_acc:.0%})\", axes[0])\nplot_decision_boundary(torch_predict, X_test, y_test,\n                       f\"PyTorch (Acc: {torch_test_acc:.0%})\", axes[1])\n\nplt.suptitle(\"Decision Boundaries \u2014 Neural Network Learned to Separate Spirals!\", \n             fontsize=15, fontweight='bold')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"projects/project3-neural-network/#step-8-deep-dive-what-did-the-network-learn","title":"Step 8: Deep Dive \u2014 What Did the Network Learn?","text":""},{"location":"projects/project3-neural-network/#inspect-neurogebras-parameters","title":"Inspect Neurogebra's Parameters","text":"<pre><code>from neurogebra import MathForge\n\nforge = MathForge()\n\nprint(\"=\" * 60)\nprint(\"INSPECTING WHAT THE NETWORK LEARNED\")\nprint(\"=\" * 60)\n\n# 1. Layer-by-layer parameter statistics\nfor i, layer in enumerate(nn_neuro.layers):\n    weights = [p.data for neuron in layer.neurons for p in neuron.w]\n    biases = [neuron.b.data for neuron in layer.neurons]\n\n    print(f\"\\nLayer {i+1}:\")\n    print(f\"  Neurons:      {len(layer.neurons)}\")\n    print(f\"  Weights:      mean={np.mean(weights):.4f}, std={np.std(weights):.4f}\")\n    print(f\"  Weight range: [{min(weights):.4f}, {max(weights):.4f}]\")\n    print(f\"  Biases:       mean={np.mean(biases):.4f}, std={np.std(biases):.4f}\")\n\n# 2. Understanding with MathForge\nprint(\"\\n\" + \"=\" * 60)\nprint(\"THE MATH BEHIND YOUR NETWORK\")\nprint(\"=\" * 60)\n\nrelu = forge.get(\"relu\")\nprint(f\"\\nReLU activation:     {relu.symbolic_expr}\")\nprint(f\"ReLU gradient:       {relu.gradient('x').symbolic_expr}\")\nprint(f\"ReLU at x=2:         {relu.eval(x=2.0)}\")\nprint(f\"ReLU at x=-2:        {relu.eval(x=-2.0)}\")\nprint(f\"ReLU grad at x=2:    {relu.gradient('x').eval(x=2.0)}\")\nprint(f\"ReLU grad at x=-2:   {relu.gradient('x').eval(x=-2.0)}\")\n\nprint(f\"\\nEach neuron computes:\")\nprint(f\"  output = ReLU(w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b)\")\nprint(f\"  = ReLU(weighted_sum + bias)\")\nprint(f\"  = max(0, weighted_sum + bias)\")\n\nprint(f\"\\nThe final layer computes 3 scores (one per class).\")\nprint(f\"Softmax converts scores to probabilities.\")\nprint(f\"We predict the class with highest probability.\")\n</code></pre>"},{"location":"projects/project3-neural-network/#visualize-individual-neuron-activations","title":"Visualize Individual Neuron Activations","text":"<pre><code># See what each neuron in the first layer \"looks at\"\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\nh = 0.05\nx_range = np.arange(-1.5, 1.5, h)\ny_range = np.arange(-1.5, 1.5, h)\nxx, yy = np.meshgrid(x_range, y_range)\n\nfor idx, ax in enumerate(axes.flat[:min(8, len(nn_neuro.layers[0].neurons))]):\n    neuron = nn_neuro.layers[0].neurons[idx]\n\n    # Compute activation for each point\n    Z = np.zeros_like(xx)\n    for i in range(len(x_range)):\n        for j in range(len(y_range)):\n            x_val = Value(float(xx[j, i]))\n            y_val = Value(float(yy[j, i]))\n            result = neuron([x_val, y_val])\n            Z[j, i] = result.data\n\n    im = ax.contourf(xx, yy, Z, levels=20, cmap='viridis')\n    ax.set_title(f\"Neuron {idx+1}\")\n    ax.set_xlabel(\"x\u2081\")\n    ax.set_ylabel(\"x\u2082\")\n\nplt.suptitle(\"What Each Neuron in Layer 1 Responds To\", fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Educational Insight</p> <p>Each neuron learns a different linear boundary (a line in 2D space). The ReLU  activation makes it respond only on one side of that line. By combining many such  neurons across multiple layers, the network can carve out complex curved boundaries  \u2014 which is how it separates the spirals!</p>"},{"location":"projects/project3-neural-network/#final-comparison","title":"Final Comparison","text":""},{"location":"projects/project3-neural-network/#code-complexity","title":"Code Complexity","text":"Component Neurogebra (lines) PyTorch (lines) Neural network definition 60 15 Softmax + cross-entropy 15 1 Training loop 35 20 Evaluation 8 5 Total ~120 ~40"},{"location":"projects/project3-neural-network/#understanding-gained","title":"Understanding Gained","text":"What You Understand Neurogebra PyTorch How neurons compute outputs \u2705 You wrote it \u26a0\ufe0f Hidden in <code>nn.Linear</code> How gradients flow backward \u2705 You see <code>.backward()</code> flow \u26a0\ufe0f Happens inside autograd How softmax works \u2705 You implemented it \u26a0\ufe0f Inside <code>CrossEntropyLoss</code> How weights get updated \u2705 <code>p.data -= lr * p.grad</code> \u26a0\ufe0f Inside <code>optimizer.step()</code> How decision boundaries form \u2705 You can inspect neurons \u26a0\ufe0f Requires extra tools"},{"location":"projects/project3-neural-network/#when-to-use-each","title":"When to Use Each","text":"Scenario Use Neurogebra Use PyTorch Learning ML concepts \u2705 Understanding backprop \u2705 Course assignments \u2705 Research prototyping \u2705 \u2705 Production ML systems \u2705 Large-scale training \u2705 GPU acceleration \u2705 Pre-trained models \u2705"},{"location":"projects/project3-neural-network/#what-you-learned-in-this-project","title":"What You Learned in This Project","text":"<ol> <li>Neural networks are layers of simple neurons stacked together</li> <li>Each neuron computes: output = activation(w\u00b7x + b)</li> <li>Backpropagation computes gradients by following the chain rule backward</li> <li>Softmax converts raw scores to probabilities</li> <li>Cross-entropy measures how wrong the predicted probabilities are</li> <li>Non-linear activations (ReLU) allow networks to learn curved boundaries</li> <li>Multiple layers allow increasingly complex decision boundaries</li> <li>Neurogebra makes every step visible and educational</li> <li>PyTorch provides speed and convenience for production</li> </ol>"},{"location":"projects/project3-neural-network/#congratulations","title":"Congratulations! \ud83c\udf89","text":"<p>You've completed all three projects! You now understand:</p> <ul> <li>Linear Regression \u2014 the foundation of ML</li> <li>Image Classification \u2014 how networks see images</li> <li>Neural Networks from Scratch \u2014 how every component works</li> </ul>"},{"location":"projects/project3-neural-network/#your-learning-path-from-here","title":"Your Learning Path from Here","text":"<pre><code>You are here \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\n\nNeurogebra (understanding)      PyTorch (production)\n\u251c\u2500\u2500 \u2705 Expressions              \u251c\u2500\u2500 torchvision\n\u251c\u2500\u2500 \u2705 Autograd                 \u251c\u2500\u2500 DataLoader\n\u251c\u2500\u2500 \u2705 Training                 \u251c\u2500\u2500 GPU training\n\u251c\u2500\u2500 \u2705 Neural Networks          \u251c\u2500\u2500 Pre-trained models\n\u2514\u2500\u2500 \u2705 Loss &amp; Optimization      \u2514\u2500\u2500 Deployment\n</code></pre> <p>You have a solid foundation. Whether you continue with Neurogebra for deeper understanding  or move to PyTorch for production work, you now know what's actually happening inside the  black box.</p> <p>Back to: Home | API Reference</p>"},{"location":"python-refresher/basics/","title":"Python Basics Recap","text":"<p>If you know Python, this is a quick refresher of the concepts you'll need for Machine Learning. If something looks new, take a moment to understand it.</p>"},{"location":"python-refresher/basics/#variables-and-data-types","title":"Variables and Data Types","text":"<pre><code># Numbers\nage = 25              # int\ntemperature = 98.6    # float\nlearning_rate = 0.01  # float (very common in ML!)\n\n# Strings\nname = \"neural network\"\n\n# Booleans\nis_training = True\nconverged = False\n</code></pre> <p>In ML, most values are <code>float</code></p> <p>Weights, biases, learning rates, loss values \u2014 almost everything in ML is a floating-point number.</p>"},{"location":"python-refresher/basics/#lists-ordered-collections","title":"Lists \u2014 Ordered Collections","text":"<pre><code># A list of numbers (like a dataset)\nscores = [85, 92, 78, 95, 88]\n\n# Access elements (0-indexed)\nprint(scores[0])   # 85 (first)\nprint(scores[-1])  # 88 (last)\n\n# List length\nprint(len(scores))  # 5\n\n# Slicing\nprint(scores[1:3])  # [92, 78]\n\n# List comprehension (very common in ML code)\ndoubled = [x * 2 for x in scores]\nprint(doubled)  # [170, 184, 156, 190, 176]\n</code></pre>"},{"location":"python-refresher/basics/#dictionaries-key-value-pairs","title":"Dictionaries \u2014 Key-Value Pairs","text":"<pre><code># Model parameters\nparams = {\n    \"learning_rate\": 0.01,\n    \"epochs\": 100,\n    \"batch_size\": 32\n}\n\n# Access\nprint(params[\"learning_rate\"])  # 0.01\n\n# Update\nparams[\"epochs\"] = 200\n\n# Loop through\nfor key, value in params.items():\n    print(f\"{key} = {value}\")\n</code></pre> <p>Why dictionaries matter in ML</p> <p>Model hyperparameters, configuration settings, and expression parameters are almost always stored as dictionaries.</p>"},{"location":"python-refresher/basics/#functions","title":"Functions","text":"<pre><code># Basic function\ndef relu(x):\n    \"\"\"ReLU activation function.\"\"\"\n    if x &gt; 0:\n        return x\n    else:\n        return 0\n\nprint(relu(5))    # 5\nprint(relu(-3))   # 0\n\n# Function with default parameters\ndef train(epochs=100, lr=0.01):\n    print(f\"Training for {epochs} epochs with lr={lr}\")\n\ntrain()                    # Uses defaults\ntrain(epochs=200, lr=0.001)  # Custom values\n</code></pre>"},{"location":"python-refresher/basics/#loops","title":"Loops","text":"<pre><code># For loop \u2014 iterating over data\ndata = [1, 2, 3, 4, 5]\ntotal = 0\nfor value in data:\n    total += value\nprint(f\"Sum = {total}\")  # 15\n\n# Range-based loop \u2014 for epochs\nfor epoch in range(5):\n    print(f\"Epoch {epoch}\")\n# Epoch 0, 1, 2, 3, 4\n\n# While loop \u2014 until convergence\nloss = 10.0\nwhile loss &gt; 0.1:\n    loss = loss * 0.5  # Simulating loss decreasing\n    print(f\"Loss: {loss:.2f}\")\n</code></pre>"},{"location":"python-refresher/basics/#classes-and-objects","title":"Classes and Objects","text":"<p>Understanding classes is essential because Neurogebra uses them everywhere.</p> <pre><code>class Neuron:\n    \"\"\"A simple neuron.\"\"\"\n\n    def __init__(self, weight, bias):\n        self.weight = weight\n        self.bias = bias\n\n    def forward(self, x):\n        \"\"\"Compute output: weight * x + bias.\"\"\"\n        return self.weight * x + self.bias\n\n    def __repr__(self):\n        return f\"Neuron(w={self.weight}, b={self.bias})\"\n\n# Create a neuron\nn = Neuron(weight=2.0, bias=1.0)\n\n# Use it\nprint(n.forward(3))    # 2.0 * 3 + 1.0 = 7.0\nprint(n)               # Neuron(w=2.0, b=1.0)\n</code></pre> <p>Neurogebra Example</p> <p>In Neurogebra, <code>Expression</code> is a class. When you do <code>relu = forge.get(\"relu\")</code>, you get an <code>Expression</code> object with methods like <code>.eval()</code>, <code>.gradient()</code>, and <code>.explain()</code>.</p>"},{"location":"python-refresher/basics/#lambda-functions","title":"Lambda Functions","text":"<p>Short, one-line functions used frequently in ML:</p> <pre><code># Lambda: anonymous function\nsquare = lambda x: x ** 2\nprint(square(4))  # 16\n\n# Common in sorting and filtering\nactivations = [\"relu\", \"sigmoid\", \"tanh\", \"gelu\"]\nsorted_acts = sorted(activations, key=lambda a: len(a))\nprint(sorted_acts)  # ['relu', 'tanh', 'gelu', 'sigmoid']\n</code></pre>"},{"location":"python-refresher/basics/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\n\n# In ML context\ntry:\n    from neurogebra import MathForge\n    forge = MathForge()\n    expr = forge.get(\"nonexistent_function\")\nexcept KeyError as e:\n    print(f\"Expression not found: {e}\")\n</code></pre>"},{"location":"python-refresher/basics/#f-strings-formatted-strings","title":"F-Strings (Formatted Strings)","text":"<p>Used constantly for printing training progress:</p> <pre><code>epoch = 42\nloss = 0.0234\naccuracy = 0.9567\n\nprint(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {accuracy:.2%}\")\n# Output: Epoch 42: Loss = 0.0234, Accuracy = 95.67%\n</code></pre> <p>The formatting codes:</p> Code Meaning Example <code>:.4f</code> 4 decimal places <code>0.0234</code> <code>:.2f</code> 2 decimal places <code>0.02</code> <code>:.2%</code> Percentage <code>95.67%</code> <code>:.2e</code> Scientific notation <code>2.34e-02</code>"},{"location":"python-refresher/basics/#importing-modules","title":"Importing Modules","text":"<pre><code># Import entire module\nimport numpy as np\n\n# Import specific items\nfrom neurogebra import MathForge, Expression\n\n# Import with alias\nfrom neurogebra.core.trainer import Trainer\n</code></pre>"},{"location":"python-refresher/basics/#try-it-yourself","title":"Try It Yourself!","text":"<p>Exercise</p> <pre><code># 1. Create a dictionary of hyperparameters\nconfig = {\"lr\": 0.001, \"epochs\": 50, \"optimizer\": \"adam\"}\n\n# 2. Write a function that simulates training\ndef fake_train(config):\n    for epoch in range(config[\"epochs\"]):\n        loss = 1.0 / (epoch + 1)\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}: loss = {loss:.4f}\")\n\nfake_train(config)\n</code></pre> <p>Next: NumPy Essentials \u2192</p>"},{"location":"python-refresher/data-handling/","title":"Data Handling with Python","text":"<p>Before you can train any ML model, you need to load, clean, and prepare data. This page covers the essential techniques.</p>"},{"location":"python-refresher/data-handling/#reading-data-from-files","title":"Reading Data from Files","text":""},{"location":"python-refresher/data-handling/#csv-files","title":"CSV Files","text":"<pre><code>import numpy as np\n\n# Using NumPy (simple numeric data)\ndata = np.loadtxt(\"data.csv\", delimiter=\",\", skiprows=1)\nprint(data.shape)\n\n# Better: Using Python's built-in csv module\nimport csv\n\nwith open(\"data.csv\", \"r\") as f:\n    reader = csv.reader(f)\n    headers = next(reader)  # Skip header row\n    data = [row for row in reader]\n</code></pre>"},{"location":"python-refresher/data-handling/#json-files","title":"JSON Files","text":"<pre><code>import json\n\nwith open(\"config.json\", \"r\") as f:\n    config = json.load(f)\n\nprint(config[\"learning_rate\"])\n</code></pre>"},{"location":"python-refresher/data-handling/#creating-datasets-from-scratch","title":"Creating Datasets from Scratch","text":"<p>For learning ML, you'll often create synthetic datasets:</p> <pre><code>import numpy as np\n\n# === LINEAR DATA ===\n# y = 2x + 1 + noise\nnp.random.seed(42)\nn_samples = 200\nX = np.random.uniform(-5, 5, n_samples)\ny = 2 * X + 1 + np.random.normal(0, 0.5, n_samples)\n\nprint(f\"Features shape: {X.shape}\")   # (200,)\nprint(f\"Targets shape: {y.shape}\")    # (200,)\n\n# === CLASSIFICATION DATA ===\n# Two classes: class 0 centered at (-2, -2), class 1 at (2, 2)\nn_per_class = 100\n\nclass_0 = np.random.randn(n_per_class, 2) + np.array([-2, -2])\nclass_1 = np.random.randn(n_per_class, 2) + np.array([2, 2])\n\nX_clf = np.vstack([class_0, class_1])          # (200, 2)\ny_clf = np.array([0]*n_per_class + [1]*n_per_class)  # (200,)\n\nprint(f\"Classification X: {X_clf.shape}\")  # (200, 2)\nprint(f\"Classification y: {y_clf.shape}\")  # (200,)\n</code></pre>"},{"location":"python-refresher/data-handling/#traintest-split","title":"Train/Test Split","text":"<p>Never evaluate your model on the same data you trained on:</p> <pre><code>import numpy as np\n\ndef train_test_split(X, y, test_ratio=0.2, seed=42):\n    \"\"\"Split data into training and testing sets.\"\"\"\n    np.random.seed(seed)\n    n = len(X)\n    indices = np.random.permutation(n)\n\n    test_size = int(n * test_ratio)\n    test_idx = indices[:test_size]\n    train_idx = indices[test_size:]\n\n    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n\n# Usage\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_ratio=0.2)\n\nprint(f\"Training: {len(X_train)} samples\")  # 160\nprint(f\"Testing:  {len(X_test)} samples\")   # 40\n</code></pre> <p>Golden Rule</p> <p>Always split BEFORE training. If you train on test data, your results are meaningless.</p>"},{"location":"python-refresher/data-handling/#data-normalization","title":"Data Normalization","text":"<p>ML models work better when data is scaled to a standard range:</p> <pre><code>import numpy as np\n\n# === MIN-MAX NORMALIZATION ===\n# Scales data to [0, 1]\ndef normalize(X):\n    X_min = X.min(axis=0)\n    X_max = X.max(axis=0)\n    return (X - X_min) / (X_max - X_min + 1e-8)\n\n# === STANDARDIZATION ===\n# Scales data to mean=0, std=1\ndef standardize(X):\n    mean = X.mean(axis=0)\n    std = X.std(axis=0)\n    return (X - mean) / (std + 1e-8)\n\n# Example\ndata = np.array([100, 200, 300, 400, 500], dtype=float)\nprint(f\"Original:     {data}\")\nprint(f\"Normalized:   {normalize(data)}\")       # [0. 0.25 0.5 0.75 1.]\nprint(f\"Standardized: {standardize(data)}\")     # [-1.41 -0.71 0. 0.71 1.41]\n</code></pre> <p>When to normalize?</p> <ul> <li>Always when features have very different scales (e.g., age 0-100 vs salary 0-1,000,000)</li> <li>Usually for neural networks \u2014 they converge faster with normalized data</li> <li>Sometimes for tree-based methods \u2014 they don't need it but it doesn't hurt</li> </ul>"},{"location":"python-refresher/data-handling/#shuffling-data","title":"Shuffling Data","text":"<pre><code>import numpy as np\n\nX = np.array([1, 2, 3, 4, 5])\ny = np.array([10, 20, 30, 40, 50])\n\n# Shuffle both arrays the same way\nindices = np.random.permutation(len(X))\nX_shuffled = X[indices]\ny_shuffled = y[indices]\n\nprint(f\"X: {X_shuffled}\")  # e.g., [3 1 5 2 4]\nprint(f\"y: {y_shuffled}\")  # e.g., [30 10 50 20 40]\n</code></pre>"},{"location":"python-refresher/data-handling/#mini-batches","title":"Mini-Batches","text":"<p>For large datasets, we train on small chunks called mini-batches:</p> <pre><code>import numpy as np\n\ndef get_batches(X, y, batch_size=32):\n    \"\"\"Yield mini-batches of data.\"\"\"\n    n = len(X)\n    indices = np.random.permutation(n)\n\n    for start in range(0, n, batch_size):\n        end = min(start + batch_size, n)\n        batch_idx = indices[start:end]\n        yield X[batch_idx], y[batch_idx]\n\n# Usage\nX = np.random.randn(100)\ny = np.random.randn(100)\n\nfor batch_X, batch_y in get_batches(X, y, batch_size=32):\n    print(f\"Batch size: {len(batch_X)}\")\n# Output: 32, 32, 32, 4\n</code></pre>"},{"location":"python-refresher/data-handling/#one-hot-encoding","title":"One-Hot Encoding","text":"<p>For classification with multiple classes:</p> <pre><code>import numpy as np\n\ndef one_hot_encode(labels, num_classes=None):\n    \"\"\"Convert integer labels to one-hot vectors.\"\"\"\n    if num_classes is None:\n        num_classes = labels.max() + 1\n\n    one_hot = np.zeros((len(labels), num_classes))\n    one_hot[np.arange(len(labels)), labels] = 1\n    return one_hot\n\n# Example: 3 classes (0, 1, 2)\nlabels = np.array([0, 1, 2, 1, 0])\nencoded = one_hot_encode(labels)\nprint(encoded)\n# [[1. 0. 0.]\n#  [0. 1. 0.]\n#  [0. 0. 1.]\n#  [0. 1. 0.]\n#  [1. 0. 0.]]\n</code></pre>"},{"location":"python-refresher/data-handling/#putting-it-all-together","title":"Putting It All Together","text":"<p>Here's a complete data preparation pipeline:</p> <pre><code>import numpy as np\n\n# 1. Generate data\nnp.random.seed(42)\nn = 500\nX = np.random.randn(n, 3)  # 500 samples, 3 features\ntrue_weights = np.array([2.0, -1.0, 0.5])\ny = X @ true_weights + 0.1 * np.random.randn(n)  # Linear relationship + noise\n\n# 2. Split\ntrain_size = int(0.8 * n)\nX_train, X_test = X[:train_size], X[train_size:]\ny_train, y_test = y[:train_size], y[train_size:]\n\n# 3. Normalize\nmean = X_train.mean(axis=0)\nstd = X_train.std(axis=0)\nX_train_norm = (X_train - mean) / std\nX_test_norm = (X_test - mean) / std  # Use TRAIN statistics!\n\n# 4. Summary\nprint(f\"Training set:  {X_train_norm.shape}\")    # (400, 3)\nprint(f\"Testing set:   {X_test_norm.shape}\")     # (100, 3)\nprint(f\"Train mean: {X_train_norm.mean(axis=0)}\")  # \u2248 [0, 0, 0]\nprint(f\"Train std:  {X_train_norm.std(axis=0)}\")   # \u2248 [1, 1, 1]\n</code></pre> <p>Important</p> <p>Always fit normalization on training data only, then apply the same transformation to test data. Otherwise you're \"leaking\" test information.</p> <p>Next: What is Machine Learning? \u2192</p>"},{"location":"python-refresher/numpy/","title":"NumPy Essentials","text":"<p>NumPy is the foundation of all numerical computing in Python. Every ML framework (including Neurogebra) uses NumPy under the hood.</p>"},{"location":"python-refresher/numpy/#what-is-numpy","title":"What is NumPy?","text":"<p>NumPy provides fast array operations. Instead of using slow Python loops, NumPy operates on entire arrays at once.</p> <pre><code>import numpy as np\n</code></pre> <p>Convention</p> <p>Everyone imports NumPy as <code>np</code>. This is a universal convention.</p>"},{"location":"python-refresher/numpy/#creating-arrays","title":"Creating Arrays","text":"<pre><code>import numpy as np\n\n# From a list\na = np.array([1, 2, 3, 4, 5])\nprint(a)        # [1 2 3 4 5]\nprint(type(a))  # &lt;class 'numpy.ndarray'&gt;\n\n# Array of zeros\nzeros = np.zeros(5)\nprint(zeros)  # [0. 0. 0. 0. 0.]\n\n# Array of ones\nones = np.ones(5)\nprint(ones)  # [1. 1. 1. 1. 1.]\n\n# Range of values\nx = np.arange(0, 10, 2)  # start, stop, step\nprint(x)  # [0 2 4 6 8]\n\n# Evenly spaced values (very common for plotting)\nx = np.linspace(0, 1, 5)  # start, stop, num_points\nprint(x)  # [0.   0.25 0.5  0.75 1.  ]\n\n# Random values\nrandom_data = np.random.randn(5)  # 5 random numbers from normal distribution\nprint(random_data)\n</code></pre>"},{"location":"python-refresher/numpy/#array-properties","title":"Array Properties","text":"<pre><code>a = np.array([[1, 2, 3],\n              [4, 5, 6]])\n\nprint(a.shape)   # (2, 3) \u2014 2 rows, 3 columns\nprint(a.ndim)    # 2 \u2014 dimensions\nprint(a.size)    # 6 \u2014 total elements\nprint(a.dtype)   # int64 \u2014 data type\n</code></pre> <p>Shapes in ML</p> <p>In ML, you'll constantly deal with shapes:</p> <ul> <li><code>(100,)</code> \u2014 100 data points (1D)</li> <li><code>(100, 10)</code> \u2014 100 samples, 10 features (2D)</li> <li><code>(32, 28, 28)</code> \u2014 32 images of 28\u00d728 pixels (3D)</li> </ul>"},{"location":"python-refresher/numpy/#array-operations-element-wise","title":"Array Operations (Element-wise)","text":"<p>NumPy operations work on every element simultaneously:</p> <pre><code>a = np.array([1, 2, 3, 4, 5])\n\n# Arithmetic\nprint(a + 10)     # [11 12 13 14 15]\nprint(a * 2)      # [ 2  4  6  8 10]\nprint(a ** 2)     # [ 1  4  9 16 25]\n\n# Between arrays\nb = np.array([10, 20, 30, 40, 50])\nprint(a + b)      # [11 22 33 44 55]\nprint(a * b)      # [ 10  40  90 160 250]\n</code></pre> <p>Why this matters: In ML, we compute things like <code>predictions - targets</code> on thousands of data points at once.</p>"},{"location":"python-refresher/numpy/#mathematical-functions","title":"Mathematical Functions","text":"<pre><code>x = np.array([-2, -1, 0, 1, 2])\n\n# Common functions\nprint(np.abs(x))      # [2 1 0 1 2]\nprint(np.exp(x))      # [0.135 0.368 1.000 2.718 7.389]\nprint(np.log(np.abs(x) + 1))  # logarithm (add 1 to avoid log(0))\nprint(np.sqrt(np.abs(x)))     # [1.414 1.000 0.000 1.000 1.414]\n\n# Implement ReLU with NumPy\nprint(np.maximum(0, x))  # [0 0 0 1 2] \u2014 ReLU!\n\n# Implement Sigmoid with NumPy\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nprint(sigmoid(x))  # [0.119 0.269 0.5 0.731 0.881]\n</code></pre>"},{"location":"python-refresher/numpy/#statistical-functions","title":"Statistical Functions","text":"<pre><code>data = np.array([85, 92, 78, 95, 88, 76, 91])\n\nprint(np.mean(data))    # 86.43 \u2014 average\nprint(np.std(data))     # 6.90  \u2014 standard deviation\nprint(np.min(data))     # 76\nprint(np.max(data))     # 95\nprint(np.sum(data))     # 605\nprint(np.median(data))  # 88.0\n</code></pre>"},{"location":"python-refresher/numpy/#reshaping-arrays","title":"Reshaping Arrays","text":"<pre><code>a = np.array([1, 2, 3, 4, 5, 6])\n\n# Reshape to 2x3\nb = a.reshape(2, 3)\nprint(b)\n# [[1 2 3]\n#  [4 5 6]]\n\n# Reshape to column vector\nc = a.reshape(-1, 1)  # -1 means \"figure it out\"\nprint(c.shape)  # (6, 1)\n\n# Flatten back to 1D\nd = b.flatten()\nprint(d)  # [1 2 3 4 5 6]\n</code></pre>"},{"location":"python-refresher/numpy/#indexing-and-slicing","title":"Indexing and Slicing","text":"<pre><code>a = np.array([10, 20, 30, 40, 50])\n\n# Basic indexing\nprint(a[0])     # 10\nprint(a[-1])    # 50\n\n# Slicing\nprint(a[1:4])   # [20 30 40]\nprint(a[:3])    # [10 20 30]\n\n# Boolean indexing (filtering)\nprint(a[a &gt; 25])   # [30 40 50]\nprint(a[a % 20 == 0])  # [20 40]\n\n# 2D indexing\nmatrix = np.array([[1, 2, 3],\n                    [4, 5, 6],\n                    [7, 8, 9]])\nprint(matrix[0, 0])    # 1 (row 0, col 0)\nprint(matrix[1, :])    # [4 5 6] (entire row 1)\nprint(matrix[:, 2])    # [3 6 9] (entire column 2)\n</code></pre>"},{"location":"python-refresher/numpy/#the-dot-product","title":"The Dot Product","text":"<p>The dot product is the most important operation in neural networks:</p> <pre><code># Vectors\nweights = np.array([0.5, -0.3, 0.8])\ninputs = np.array([1.0, 2.0, 3.0])\n\n# Dot product: sum of element-wise multiplication\nresult = np.dot(weights, inputs)\nprint(result)  # 0.5*1 + (-0.3)*2 + 0.8*3 = 1.5\n\n# This is exactly what a neuron does!\nbias = 0.1\noutput = np.dot(weights, inputs) + bias\nprint(output)  # 1.6\n</code></pre> <p>Neuron = Dot Product + Bias + Activation</p> <p>Every neuron in a neural network computes: \\(output = activation(weights \\cdot inputs + bias)\\)</p>"},{"location":"python-refresher/numpy/#generating-synthetic-data","title":"Generating Synthetic Data","text":"<p>This is very useful for testing ML models:</p> <pre><code>import numpy as np\n\n# Linear data: y = 2x + 1 + noise\nnp.random.seed(42)  # For reproducibility\nX = np.linspace(0, 10, 100)\nnoise = np.random.normal(0, 0.5, 100)\ny = 2 * X + 1 + noise\n\nprint(f\"X shape: {X.shape}\")  # (100,)\nprint(f\"y shape: {y.shape}\")  # (100,)\nprint(f\"X range: [{X.min():.1f}, {X.max():.1f}]\")\nprint(f\"y range: [{y.min():.1f}, {y.max():.1f}]\")\n</code></pre>"},{"location":"python-refresher/numpy/#numpy-in-neurogebra","title":"NumPy in Neurogebra","text":"<p>Neurogebra expressions accept NumPy arrays:</p> <pre><code>from neurogebra import MathForge\nimport numpy as np\n\nforge = MathForge()\nrelu = forge.get(\"relu\")\n\n# Evaluate on an entire array at once!\nx = np.array([-3, -1, 0, 1, 3])\nresult = relu.eval(x=x)\nprint(result)  # [0 0 0 1 3]\n</code></pre>"},{"location":"python-refresher/numpy/#try-it-yourself","title":"Try It Yourself!","text":"<p>Exercise</p> <pre><code>import numpy as np\n\n# 1. Create a dataset: y = 3x\u00b2 - 2x + 1\nX = np.linspace(-5, 5, 50)\ny = 3 * X**2 - 2 * X + 1\n\n# 2. Compute statistics\nprint(f\"Mean of y: {np.mean(y):.2f}\")\nprint(f\"Std of y: {np.std(y):.2f}\")\n\n# 3. Implement sigmoid using NumPy\nsigmoid = 1 / (1 + np.exp(-X))\nprint(f\"Sigmoid range: [{sigmoid.min():.3f}, {sigmoid.max():.3f}]\")\n</code></pre> <p>Next: Data Handling with Python \u2192</p>"},{"location":"tutorial/activations/","title":"Activation Functions","text":"<p>Activation functions are the secret sauce that gives neural networks the power to learn complex patterns.</p>"},{"location":"tutorial/activations/#why-activation-functions","title":"Why Activation Functions?","text":"<p>Without activation functions, a neural network is just a chain of linear transformations \u2014 which collapses into a single linear function. Activation functions add non-linearity, allowing networks to learn curves, boundaries, and complex relationships.</p> <pre><code>Without activation: Layer1(Layer2(x)) = W1*(W2*x + b2) + b1 = W*x + b  (still linear!)\nWith activation:    relu(W1 * relu(W2*x + b2) + b1)  \u2192 Can learn ANY function!\n</code></pre>"},{"location":"tutorial/activations/#exploring-activations-in-neurogebra","title":"Exploring Activations in Neurogebra","text":"<pre><code>from neurogebra import MathForge\n\nforge = MathForge()\n\n# List all activation functions\nactivations = forge.list_all(category=\"activation\")\nprint(activations)\n# ['relu', 'sigmoid', 'tanh', 'leaky_relu', 'swish', 'gelu', 'softplus', ...]\n</code></pre>"},{"location":"tutorial/activations/#relu-rectified-linear-unit","title":"ReLU (Rectified Linear Unit)","text":"<p>The most popular activation function in deep learning.</p> \\[f(x) = \\max(0, x)\\] <pre><code>relu = forge.get(\"relu\")\n\n# Formula\nprint(relu.symbolic_expr)  # Max(0, x)\n\n# Behavior\nprint(relu.eval(x=5))    # 5   (positive \u2192 pass through)\nprint(relu.eval(x=-3))   # 0   (negative \u2192 blocked)\nprint(relu.eval(x=0))    # 0   (zero \u2192 zero)\n\n# Metadata\nprint(relu.metadata[\"description\"])\n# \"Rectified Linear Unit - outputs x if x &gt; 0, else 0\"\nprint(relu.metadata[\"pros\"])\n# ['Fast computation', 'No vanishing gradient for positive values']\nprint(relu.metadata[\"cons\"])\n# ['Dead neurons for negative values', 'Not zero-centered']\n</code></pre> <p>When to use ReLU</p> <p>Default choice for hidden layers. Use it unless you have a specific reason not to.</p>"},{"location":"tutorial/activations/#sigmoid","title":"Sigmoid","text":"<p>Maps any real number to a value between 0 and 1.</p> \\[f(x) = \\frac{1}{1 + e^{-x}}\\] <pre><code>sigmoid = forge.get(\"sigmoid\")\n\nprint(sigmoid.eval(x=0))      # 0.5  (center)\nprint(sigmoid.eval(x=10))     # \u2248 1.0 (large positive \u2192 1)\nprint(sigmoid.eval(x=-10))    # \u2248 0.0 (large negative \u2192 0)\n</code></pre> <p>When to use Sigmoid</p> <p>Output layer for binary classification (yes/no, spam/not-spam). Outputs a probability.</p>"},{"location":"tutorial/activations/#tanh-hyperbolic-tangent","title":"Tanh (Hyperbolic Tangent)","text":"<p>Like Sigmoid, but outputs between -1 and 1.</p> \\[f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\] <pre><code>tanh = forge.get(\"tanh\")\n\nprint(tanh.eval(x=0))     # 0.0  (zero-centered!)\nprint(tanh.eval(x=5))     # \u2248 1.0\nprint(tanh.eval(x=-5))    # \u2248 -1.0\n</code></pre> <p>When to use Tanh</p> <p>Hidden layers when zero-centered output matters (e.g., RNNs, LSTMs).</p>"},{"location":"tutorial/activations/#leaky-relu","title":"Leaky ReLU","text":"<p>ReLU but allows a small gradient for negative values.</p> \\[f(x) = \\max(\\alpha x, x) \\quad \\text{where } \\alpha = 0.01\\] <pre><code>leaky = forge.get(\"leaky_relu\")\n\nprint(leaky.eval(x=5))    # 5\nprint(leaky.eval(x=-5))   # -0.05  (not zero! Small leak)\n\n# Custom alpha\nleaky02 = forge.get(\"leaky_relu\", params={\"alpha\": 0.2})\nprint(leaky02.eval(x=-5))  # -1.0 (bigger leak)\n</code></pre> <p>When to use Leaky ReLU</p> <p>When you're worried about \"dead neurons\" (neurons that always output 0 with standard ReLU).</p>"},{"location":"tutorial/activations/#swish-silu","title":"Swish (SiLU)","text":"<p>Self-gated activation \u2014 often outperforms ReLU.</p> \\[f(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}}\\] <pre><code>swish = forge.get(\"swish\")\n\nprint(swish.eval(x=5))    # \u2248 4.97\nprint(swish.eval(x=-5))   # \u2248 -0.03\nprint(swish.eval(x=0))    # 0.0\n</code></pre> <p>When to use Swish</p> <p>Modern deep networks. Found by Google's AutoML to outperform ReLU in many cases.</p>"},{"location":"tutorial/activations/#gelu-gaussian-error-linear-unit","title":"GELU (Gaussian Error Linear Unit)","text":"<p>The activation used in GPT, BERT, and modern Transformers.</p> \\[f(x) = 0.5x\\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}\\left(x + 0.044715x^3\\right)\\right)\\right)\\] <pre><code>gelu = forge.get(\"gelu\")\n\nprint(gelu.eval(x=1))     # \u2248 0.841\nprint(gelu.eval(x=-1))    # \u2248 -0.159\nprint(gelu.eval(x=0))     # 0.0\n</code></pre> <p>When to use GELU</p> <p>Transformer models (BERT, GPT, ViT). The current state-of-the-art choice.</p>"},{"location":"tutorial/activations/#softplus","title":"Softplus","text":"<p>A smooth approximation of ReLU.</p> \\[f(x) = \\ln(1 + e^x)\\] <pre><code>softplus = forge.get(\"softplus\")\n\nprint(softplus.eval(x=5))    # \u2248 5.007\nprint(softplus.eval(x=-5))   # \u2248 0.007\nprint(softplus.eval(x=0))    # \u2248 0.693\n</code></pre>"},{"location":"tutorial/activations/#comparison-table","title":"Comparison Table","text":"Activation Formula Range Best For ReLU \\(\\max(0, x)\\) \\([0, \\infty)\\) Default for hidden layers Sigmoid \\(\\frac{1}{1+e^{-x}}\\) \\((0, 1)\\) Binary classification output Tanh \\(\\tanh(x)\\) \\((-1, 1)\\) RNNs, zero-centered needs Leaky ReLU \\(\\max(\\alpha x, x)\\) \\((-\\infty, \\infty)\\) Preventing dead neurons Swish \\(x \\cdot \\sigma(x)\\) \\((-\\infty, \\infty)\\) Modern deep networks GELU see above \\((-\\infty, \\infty)\\) Transformers (BERT, GPT) Softplus \\(\\ln(1+e^x)\\) \\((0, \\infty)\\) Smooth ReLU alternative"},{"location":"tutorial/activations/#side-by-side-comparison","title":"Side-by-Side Comparison","text":"<pre><code>from neurogebra import MathForge\nimport numpy as np\n\nforge = MathForge()\n\nx = 1.0\nprint(f\"{'Activation':&gt;12} | f({x})\")\nprint(\"-\" * 30)\n\nfor name in [\"relu\", \"sigmoid\", \"tanh\", \"leaky_relu\", \"swish\", \"gelu\", \"softplus\"]:\n    expr = forge.get(name)\n    val = expr.eval(x=x)\n    print(f\"{name:&gt;12} | {val:.4f}\")\n</code></pre> <p>Output: <pre><code>  Activation | f(1.0)\n------------------------------\n        relu | 1.0000\n     sigmoid | 0.7311\n        tanh | 0.7616\n  leaky_relu | 1.0000\n       swish | 0.7311\n        gelu | 0.8412\n    softplus | 1.3133\n</code></pre></p>"},{"location":"tutorial/activations/#gradients-of-activation-functions","title":"Gradients of Activation Functions","text":"<p>Understanding how gradients flow through activation functions is crucial:</p> <pre><code>for name in [\"relu\", \"sigmoid\", \"tanh\", \"swish\"]:\n    expr = forge.get(name)\n    grad = expr.gradient(\"x\")\n\n    print(f\"{name}:\")\n    print(f\"  f(x) = {expr.symbolic_expr}\")\n    print(f\"  f'(x) = {grad.symbolic_expr}\")\n    print(f\"  f'(1.0) = {grad.eval(x=1.0):.4f}\")\n    print()\n</code></pre>"},{"location":"tutorial/activations/#try-it-yourself","title":"Try It Yourself!","text":"<p>Exercise</p> <ol> <li>Get all activation functions and evaluate them at <code>x = -2, -1, 0, 1, 2</code></li> <li>Which activation outputs the largest value at <code>x = 1</code>?</li> <li>Which activation has the largest gradient at <code>x = 0</code>?</li> <li>Create a custom activation: <code>f(x) = x * sigmoid(2*x)</code></li> </ol> <p>Next: Loss Functions \u2192</p>"},{"location":"tutorial/autograd/","title":"Autograd Engine","text":"<p>Neurogebra includes a built-in automatic differentiation engine that tracks computations and computes gradients automatically \u2014 just like PyTorch's autograd, but simpler and more educational.</p>"},{"location":"tutorial/autograd/#what-is-autograd","title":"What is Autograd?","text":"<p>Autograd = Automatic Gradient computation.</p> <p>Instead of computing derivatives by hand or symbolically, autograd:</p> <ol> <li>Records every operation you perform (builds a computation graph)</li> <li>When you call <code>.backward()</code>, it walks backwards through the graph</li> <li>Computes the gradient of each value using the chain rule</li> </ol>"},{"location":"tutorial/autograd/#the-value-class","title":"The Value Class","text":"<p><code>Value</code> wraps a number and tracks its gradient:</p> <pre><code>from neurogebra.core.autograd import Value\n\n# Create values\na = Value(2.0)\nb = Value(3.0)\n\nprint(a)  # Value(data=2.0, grad=0.0)\nprint(b)  # Value(data=3.0, grad=0.0)\n</code></pre>"},{"location":"tutorial/autograd/#forward-pass-build-the-graph","title":"Forward Pass \u2014 Build the Graph","text":"<pre><code>from neurogebra.core.autograd import Value\n\na = Value(2.0)\nb = Value(3.0)\n\n# Each operation creates a new Value and records the connection\nc = a + b      # c = 5.0\nd = a * b      # d = 6.0\ne = c + d      # e = 11.0\n\nprint(f\"a = {a.data}\")  # 2.0\nprint(f\"b = {b.data}\")  # 3.0\nprint(f\"c = a + b = {c.data}\")  # 5.0\nprint(f\"d = a * b = {d.data}\")  # 6.0\nprint(f\"e = c + d = {e.data}\")  # 11.0\n</code></pre> <p>The computation graph looks like:</p> <pre><code>a (2.0) \u2500\u2500\u252c\u2500\u2500[+]\u2500\u2500 c (5.0) \u2500\u2500\u2510\n           \u2502                   [+]\u2500\u2500 e (11.0)\nb (3.0) \u2500\u2500\u253c\u2500\u2500[+]\u2500\u2500 c         \u2502\n           \u2502                   \u2502\na (2.0) \u2500\u2500\u2524                   \u2502\n           \u2514\u2500\u2500[*]\u2500\u2500 d (6.0) \u2500\u2500\u2518\nb (3.0) \u2500\u2500\u2518\n</code></pre>"},{"location":"tutorial/autograd/#backward-pass-compute-gradients","title":"Backward Pass \u2014 Compute Gradients","text":"<pre><code># Compute de/da and de/db\ne.backward()\n\nprint(f\"de/da = {a.grad}\")  # 4.0  (from + path: 1, from * path: b=3 \u2192 total: 1+3=4)\nprint(f\"de/db = {b.grad}\")  # 3.0  (from + path: 1, from * path: a=2 \u2192 total: 1+2=3)\n</code></pre> <p>Let's verify manually:</p> <ul> <li>\\(e = (a + b) + (a \\cdot b) = a + b + ab\\)</li> <li>\\(\\frac{\\partial e}{\\partial a} = 1 + b = 1 + 3 = 4\\) \u2705</li> <li>\\(\\frac{\\partial e}{\\partial b} = 1 + a = 1 + 2 = 3\\) \u2705</li> </ul>"},{"location":"tutorial/autograd/#supported-operations","title":"Supported Operations","text":"<pre><code>from neurogebra.core.autograd import Value\n\nx = Value(2.0)\n\n# Basic arithmetic\ny = x + 3        # Addition\ny = x * 3        # Multiplication\ny = x ** 2       # Power\ny = x - 1        # Subtraction\ny = x / 2        # Division\ny = -x           # Negation\n\n# Activation functions\ny = x.relu()     # ReLU\ny = x.sigmoid()  # Sigmoid\ny = x.tanh()     # Tanh\ny = x.exp()      # Exponential\ny = x.log()      # Natural log\n</code></pre>"},{"location":"tutorial/autograd/#building-a-neuron","title":"Building a Neuron","text":"<p>A single neuron is: \\(output = activation(w_1 x_1 + w_2 x_2 + b)\\)</p> <pre><code>from neurogebra.core.autograd import Value\n\n# Inputs\nx1 = Value(2.0)\nx2 = Value(3.0)\n\n# Learnable parameters\nw1 = Value(0.5)\nw2 = Value(-0.3)\nb = Value(0.1)\n\n# Forward pass\nz = w1 * x1 + w2 * x2 + b   # Linear: 0.5*2 + (-0.3)*3 + 0.1 = 0.2\noutput = z.sigmoid()          # Activation: \u03c3(0.2) \u2248 0.5498\n\n# Backward pass\noutput.backward()\n\nprint(f\"Output: {output.data:.4f}\")\nprint(f\"Gradients:\")\nprint(f\"  dout/dw1 = {w1.grad:.4f}\")\nprint(f\"  dout/dw2 = {w2.grad:.4f}\")\nprint(f\"  dout/db  = {b.grad:.4f}\")\n</code></pre>"},{"location":"tutorial/autograd/#manual-training-loop-with-autograd","title":"Manual Training Loop with Autograd","text":"<p>This is how PyTorch works internally \u2014 and now you can see every step:</p> <pre><code>from neurogebra.core.autograd import Value\nimport random\n\n# Training data: y = 2x + 1\ndata = [(1, 3), (2, 5), (3, 7), (4, 9)]\n\n# Learnable parameters\nw = Value(0.0)\nb = Value(0.0)\nlearning_rate = 0.01\n\nfor epoch in range(100):\n    total_loss = Value(0.0)\n\n    for x_val, y_val in data:\n        # Forward pass\n        x = Value(x_val)\n        y_pred = w * x + b\n\n        # Loss (MSE for one sample)\n        loss = (y_pred - Value(y_val)) ** 2\n        total_loss = total_loss + loss\n\n    # Backward pass\n    total_loss.backward()\n\n    # Update parameters (gradient descent)\n    w.data -= learning_rate * w.grad\n    b.data -= learning_rate * b.grad\n\n    # Reset gradients for next epoch\n    w.grad = 0.0\n    b.grad = 0.0\n\n    if epoch % 20 == 0:\n        print(f\"Epoch {epoch:&gt;3}: loss = {total_loss.data:.4f}, w = {w.data:.4f}, b = {b.data:.4f}\")\n\nprint(f\"\\nLearned: y = {w.data:.2f}x + {b.data:.2f}\")\n# Expected: y \u2248 2.00x + 1.00\n</code></pre>"},{"location":"tutorial/autograd/#building-a-mini-neural-network","title":"Building a Mini Neural Network","text":"<pre><code>from neurogebra.core.autograd import Value\nimport random\n\nclass Neuron:\n    def __init__(self, n_inputs):\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(n_inputs)]\n        self.b = Value(0.0)\n\n    def __call__(self, x):\n        # w \u00b7 x + b\n        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n        return act.relu()\n\n    def parameters(self):\n        return self.w + [self.b]\n\nclass MLP:\n    def __init__(self, n_inputs, layer_sizes):\n        sizes = [n_inputs] + layer_sizes\n        self.layers = []\n        for i in range(len(layer_sizes)):\n            neurons = [Neuron(sizes[i]) for _ in range(sizes[i+1])]\n            self.layers.append(neurons)\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = [neuron(x) for neuron in layer]\n        return x[0] if len(x) == 1 else x\n\n    def parameters(self):\n        return [p for layer in self.layers for neuron in layer for p in neuron.parameters()]\n\n# Create a tiny network: 2 inputs \u2192 4 hidden \u2192 1 output\nrandom.seed(42)\nmodel = MLP(2, [4, 1])\n\nprint(f\"Total parameters: {len(model.parameters())}\")\n# (2+1)*4 + (4+1)*1 = 12 + 5 = 17 parameters\n</code></pre>"},{"location":"tutorial/autograd/#zero-grad-why-it-matters","title":"Zero Grad \u2014 Why It Matters","text":"<pre><code>x = Value(3.0)\ny = x ** 2\ny.backward()\nprint(f\"After first backward: x.grad = {x.grad}\")  # 6.0\n\n# If we compute again WITHOUT zeroing:\ny = x ** 2\ny.backward()\nprint(f\"After second backward: x.grad = {x.grad}\")  # 12.0 \u2014 WRONG! Accumulated!\n\n# Always zero gradients between iterations:\nx.zero_grad()\ny = x ** 2\ny.backward()\nprint(f\"After zero + backward: x.grad = {x.grad}\")  # 6.0 \u2014 Correct!\n</code></pre>"},{"location":"tutorial/autograd/#summary","title":"Summary","text":"Step What Happens Code Create values Wrap numbers <code>x = Value(2.0)</code> Forward pass Compute result <code>y = w * x + b</code> Backward pass Compute gradients <code>y.backward()</code> Read gradients See derivatives <code>w.grad</code> Update weights Learn <code>w.data -= lr * w.grad</code> Zero gradients Reset for next iteration <code>w.zero_grad()</code> <p>Next: Tensors \u2192</p>"},{"location":"tutorial/composition/","title":"Expression Composition","text":"<p>One of Neurogebra's most powerful features is the ability to combine simple expressions into complex ones.</p>"},{"location":"tutorial/composition/#why-compose","title":"Why Compose?","text":"<p>Real ML involves complex math built from simple pieces:</p> <ul> <li>Custom loss = MSE + regularization</li> <li>Complex activation = combination of simple activations</li> <li>Neural network layer = linear transformation + activation</li> </ul>"},{"location":"tutorial/composition/#arithmetic-composition","title":"Arithmetic Composition","text":""},{"location":"tutorial/composition/#addition","title":"Addition","text":"<pre><code>from neurogebra import MathForge\n\nforge = MathForge()\n\nmse = forge.get(\"mse\")\nmae = forge.get(\"mae\")\n\n# Combine losses\nhybrid_loss = mse + mae\nresult = hybrid_loss.eval(y_pred=3.0, y_true=5.0)\nprint(f\"Hybrid loss: {result}\")  # MSE(4) + MAE(2) = 6\n</code></pre>"},{"location":"tutorial/composition/#scalar-multiplication","title":"Scalar Multiplication","text":"<pre><code># Weighted loss: 70% MSE + 30% MAE\nweighted_loss = 0.7 * mse + 0.3 * mae\n</code></pre>"},{"location":"tutorial/composition/#string-based-composition","title":"String-Based Composition","text":"<pre><code># Quick composition from string\nloss = forge.compose(\"mse + 0.1*mae\")\nprint(loss.eval(y_pred=3.0, y_true=5.0))\n</code></pre>"},{"location":"tutorial/composition/#functional-composition","title":"Functional Composition","text":"<p>Compose expressions like mathematical function composition: \\(f(g(x))\\)</p> <pre><code>from neurogebra import Expression, MathForge\n\nforge = MathForge()\n\n# Create a linear transformation\nlinear = Expression(\"linear\", \"2*x + 1\")\n\n# Compose with sigmoid: sigmoid(2*x + 1)\nsigmoid = forge.get(\"sigmoid\")\ncomposed = sigmoid.compose(linear)\n\nprint(f\"Formula: {composed.symbolic_expr}\")\n\n# Evaluate\nprint(f\"f(0) = {composed.eval(x=0):.4f}\")  # sigmoid(1) \u2248 0.7311\nprint(f\"f(1) = {composed.eval(x=1):.4f}\")  # sigmoid(3) \u2248 0.9526\n</code></pre>"},{"location":"tutorial/composition/#building-custom-activations","title":"Building Custom Activations","text":"<pre><code>from neurogebra import Expression, MathForge\n\nforge = MathForge()\n\n# Parametric ReLU: max(alpha*x, x)\nprelu = Expression(\n    \"parametric_relu\",\n    \"Max(alpha*x, x)\",\n    params={\"alpha\": 0.1},\n    trainable_params=[\"alpha\"],  # alpha can be learned!\n    metadata={\"category\": \"activation\"}\n)\n\nprint(prelu.eval(x=5))     # 5\nprint(prelu.eval(x=-5))    # -0.5 (with alpha=0.1)\n\n# Register it for reuse\nforge.register(\"prelu\", prelu)\n</code></pre>"},{"location":"tutorial/composition/#building-custom-loss-functions","title":"Building Custom Loss Functions","text":"<pre><code># Focal Loss (used in object detection)\n# It down-weights easy examples and focuses on hard ones\nfocal = Expression(\n    \"focal_loss\",\n    \"-(alpha * y_true * (1-y_pred)**gamma * log(y_pred) + (1-alpha) * (1-y_true) * y_pred**gamma * log(1-y_pred))\",\n    params={\"alpha\": 0.25, \"gamma\": 2.0},\n    metadata={\n        \"category\": \"loss\",\n        \"description\": \"Focal loss for imbalanced classification\"\n    }\n)\n\nforge.register(\"focal_loss\", focal)\n</code></pre>"},{"location":"tutorial/composition/#building-regularized-losses","title":"Building Regularized Losses","text":"<p>Regularization prevents overfitting by penalizing large weights:</p> <pre><code>from neurogebra import MathForge, Expression\n\nforge = MathForge()\n\n# Base loss\nmse = forge.get(\"mse\")\n\n# L2 regularizer from repository\nl2 = Expression(\"l2_term\", \"lambda_reg * w**2\",\n                params={\"lambda_reg\": 0.01})\n\n# Regularized loss = MSE + L2\n# In practice, you'd sum L2 over all weights\nprint(\"Base MSE:\", mse.eval(y_pred=3, y_true=5))\nprint(\"L2 penalty:\", l2.eval(w=2.0))     # 0.01 * 4 = 0.04\n</code></pre>"},{"location":"tutorial/composition/#multi-step-composition","title":"Multi-Step Composition","text":"<p>Build complex pipelines step by step:</p> <pre><code>from neurogebra import Expression\n\n# Step 1: Normalize input\nnormalize = Expression(\"normalize\", \"(x - mean) / std\",\n                       params={\"mean\": 0.0, \"std\": 1.0})\n\n# Step 2: Linear transformation\nlinear = Expression(\"linear\", \"w * x + b\",\n                    params={\"w\": 1.0, \"b\": 0.0})\n\n# Step 3: Activation\nfrom neurogebra import MathForge\nforge = MathForge()\nrelu = forge.get(\"relu\")\n\n# Chain them: relu(w * normalize(x) + b)\nstep1 = linear.compose(normalize)   # w * ((x-mean)/std) + b\nfinal = relu.compose(step1)         # relu(above)\n\nprint(f\"Pipeline: {final.symbolic_expr}\")\n</code></pre>"},{"location":"tutorial/composition/#practical-example-custom-model","title":"Practical Example: Custom Model","text":"<pre><code>from neurogebra import Expression, MathForge\nimport numpy as np\n\nforge = MathForge()\n\n# Build a simple 2-layer network expression\n# Layer 1: h = relu(w1*x + b1)\n# Layer 2: y = w2*h + b2\n\n# We can compose this step by step\nlayer1_linear = Expression(\"l1\", \"w1*x + b1\",\n                           params={\"w1\": 0.5, \"b1\": 0.0})\n\nrelu = forge.get(\"relu\")\nlayer1 = relu.compose(layer1_linear)  # relu(w1*x + b1)\n\nprint(f\"Layer 1 output at x=2: {layer1.eval(x=2)}\")\n# relu(0.5*2 + 0) = relu(1.0) = 1.0\n</code></pre>"},{"location":"tutorial/composition/#tips-for-composition","title":"Tips for Composition","text":"<p>Best Practices</p> <ol> <li>Start simple \u2014 compose from well-tested building blocks</li> <li>Test each step \u2014 evaluate intermediate expressions</li> <li>Register reusable expressions \u2014 <code>forge.register(\"name\", expr)</code></li> <li>Use string composition for quick experiments \u2014 <code>forge.compose(\"mse + 0.1*mae\")</code></li> <li>Check gradients \u2014 composed expressions have gradients too!</li> </ol> <p>Next: Training Expressions \u2192</p>"},{"location":"tutorial/datasets/","title":"Datasets","text":"<p>Neurogebra includes built-in dataset loaders for common ML tasks, so you can start training immediately without hunting for data.</p>"},{"location":"tutorial/datasets/#loading-datasets","title":"Loading Datasets","text":"<pre><code>from neurogebra.datasets import loaders\n\n# See what's available\nprint(dir(loaders))\n</code></pre>"},{"location":"tutorial/datasets/#built-in-datasets","title":"Built-in Datasets","text":""},{"location":"tutorial/datasets/#synthetic-datasets","title":"Synthetic Datasets","text":"<p>These are generated on-the-fly \u2014 perfect for learning and testing:</p> <pre><code>import numpy as np\n\n# === LINEAR DATA ===\n# For regression practice\nnp.random.seed(42)\nX = np.linspace(0, 10, 200)\ny = 2.5 * X + 3.0 + np.random.normal(0, 1, 200)\n\n# === CLASSIFICATION DATA ===\n# Two spirals (classic ML challenge)\ndef make_spirals(n_points=200, noise=0.5):\n    n = n_points // 2\n    theta = np.linspace(0, 4*np.pi, n)\n\n    r = theta / (4*np.pi)\n    x1 = r * np.cos(theta) + np.random.randn(n) * noise * 0.1\n    y1 = r * np.sin(theta) + np.random.randn(n) * noise * 0.1\n\n    x2 = -r * np.cos(theta) + np.random.randn(n) * noise * 0.1\n    y2 = -r * np.sin(theta) + np.random.randn(n) * noise * 0.1\n\n    X = np.vstack([np.column_stack([x1, y1]),\n                   np.column_stack([x2, y2])])\n    y = np.array([0]*n + [1]*n)\n    return X, y\n\nX_spiral, y_spiral = make_spirals()\nprint(f\"Spirals: X={X_spiral.shape}, y={y_spiral.shape}\")\n</code></pre>"},{"location":"tutorial/datasets/#creating-your-own-datasets","title":"Creating Your Own Datasets","text":""},{"location":"tutorial/datasets/#regression-data","title":"Regression Data","text":"<pre><code>import numpy as np\n\ndef make_regression_data(n=500, noise=0.5, seed=42):\n    \"\"\"Create synthetic regression data.\"\"\"\n    np.random.seed(seed)\n    X = np.random.uniform(-5, 5, (n, 1))\n    y = 3 * X[:, 0] ** 2 - 2 * X[:, 0] + 1 + np.random.normal(0, noise, n)\n    return X, y\n\nX, y = make_regression_data()\nprint(f\"X shape: {X.shape}\")  # (500, 1)\nprint(f\"y shape: {y.shape}\")  # (500,)\n</code></pre>"},{"location":"tutorial/datasets/#classification-data","title":"Classification Data","text":"<pre><code>def make_classification_data(n=400, seed=42):\n    \"\"\"Create synthetic binary classification data.\"\"\"\n    np.random.seed(seed)\n\n    # Class 0: centered at (-1, -1)\n    X0 = np.random.randn(n//2, 2) * 0.8 + [-1, -1]\n\n    # Class 1: centered at (1, 1)\n    X1 = np.random.randn(n//2, 2) * 0.8 + [1, 1]\n\n    X = np.vstack([X0, X1])\n    y = np.array([0]*(n//2) + [1]*(n//2))\n\n    # Shuffle\n    idx = np.random.permutation(n)\n    return X[idx], y[idx]\n\nX, y = make_classification_data()\nprint(f\"X shape: {X.shape}\")  # (400, 2)\nprint(f\"y shape: {y.shape}\")  # (400,)\nprint(f\"Classes: {np.unique(y)}\")  # [0 1]\n</code></pre>"},{"location":"tutorial/datasets/#data-splitting","title":"Data Splitting","text":"<pre><code>def train_test_split(X, y, test_ratio=0.2, seed=42):\n    np.random.seed(seed)\n    n = len(X)\n    idx = np.random.permutation(n)\n    split = int(n * (1 - test_ratio))\n\n    return (X[idx[:split]], X[idx[split:]],\n            y[idx[:split]], y[idx[split:]])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nprint(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n</code></pre>"},{"location":"tutorial/datasets/#using-datasets-with-neurogebra","title":"Using Datasets with Neurogebra","text":""},{"location":"tutorial/datasets/#training-on-synthetic-data","title":"Training on Synthetic Data","text":"<pre><code>import numpy as np\nfrom neurogebra import Expression\nfrom neurogebra.core.trainer import Trainer\n\n# Generate data: y = 2x + 1\nX = np.linspace(0, 10, 200)\ny = 2 * X + 1 + np.random.normal(0, 0.5, 200)\n\n# Create model\nmodel = Expression(\"linear\", \"m*x + b\",\n                   params={\"m\": 0.0, \"b\": 0.0},\n                   trainable_params=[\"m\", \"b\"])\n\n# Train\ntrainer = Trainer(model, learning_rate=0.01, optimizer=\"adam\")\nhistory = trainer.fit(X, y, epochs=200)\n\nprint(f\"Learned: y = {model.params['m']:.2f}x + {model.params['b']:.2f}\")\n</code></pre>"},{"location":"tutorial/datasets/#dataset-best-practices","title":"Dataset Best Practices","text":"<p>Tips for Working with Data</p> <ol> <li>Always set a random seed for reproducibility: <code>np.random.seed(42)</code></li> <li>Split before processing \u2014 normalize using training data statistics only</li> <li>Visualize your data before training to understand its structure</li> <li>Start small \u2014 use small datasets while debugging, scale up later</li> <li>Shuffle your data \u2014 ensures model doesn't learn order-dependent patterns</li> </ol> <p>Next: Custom Expressions \u2192</p>"},{"location":"tutorial/expressions/","title":"Expressions \u2014 Building Blocks","text":"<p>The <code>Expression</code> class is the fundamental unit of Neurogebra. Everything in the library is built on expressions.</p>"},{"location":"tutorial/expressions/#what-is-an-expression","title":"What is an Expression?","text":"<p>An Expression is a symbolic mathematical formula that can:</p> <ul> <li>\u2705 Evaluate \u2014 compute a numerical result</li> <li>\u2705 Differentiate \u2014 compute gradients symbolically</li> <li>\u2705 Explain \u2014 describe itself in plain English</li> <li>\u2705 Compose \u2014 combine with other expressions</li> <li>\u2705 Train \u2014 learn parameters from data</li> <li>\u2705 Visualize \u2014 plot itself</li> </ul>"},{"location":"tutorial/expressions/#creating-an-expression","title":"Creating an Expression","text":"<pre><code>from neurogebra import Expression\n\n# Simple: name + formula\nsquare = Expression(\"square\", \"x**2\")\n\n# With parameters\nline = Expression(\"line\", \"m*x + b\", params={\"m\": 2.0, \"b\": 1.0})\n\n# With trainable parameters\ntrainable_line = Expression(\n    \"trainable_line\",\n    \"m*x + b\",\n    params={\"m\": 0.0, \"b\": 0.0},\n    trainable_params=[\"m\", \"b\"]\n)\n\n# With metadata\ncustom = Expression(\n    \"my_function\",\n    \"x * tanh(x)\",\n    metadata={\n        \"category\": \"activation\",\n        \"description\": \"Custom activation function\",\n        \"usage\": \"Experimental hidden layers\"\n    }\n)\n</code></pre>"},{"location":"tutorial/expressions/#evaluating-expressions","title":"Evaluating Expressions","text":""},{"location":"tutorial/expressions/#with-keyword-arguments","title":"With Keyword Arguments","text":"<pre><code>expr = Expression(\"poly\", \"a*x**2 + b*x + c\",\n                  params={\"a\": 1.0, \"b\": -2.0, \"c\": 1.0})\n\n# Parameters are substituted automatically\nresult = expr.eval(x=3)\nprint(result)  # 1*9 + (-2)*3 + 1 = 4.0\n</code></pre>"},{"location":"tutorial/expressions/#with-numpy-arrays","title":"With NumPy Arrays","text":"<pre><code>import numpy as np\n\nsquare = Expression(\"square\", \"x**2\")\nx = np.array([1, 2, 3, 4, 5])\nresult = square.eval(x=x)\nprint(result)  # [ 1  4  9 16 25]\n</code></pre>"},{"location":"tutorial/expressions/#expression-with-multiple-variables","title":"Expression with Multiple Variables","text":"<pre><code>loss = Expression(\"mse\", \"(y_pred - y_true)**2\")\nresult = loss.eval(y_pred=3.0, y_true=5.0)\nprint(result)  # 4.0\n</code></pre>"},{"location":"tutorial/expressions/#expression-properties","title":"Expression Properties","text":"<pre><code>from neurogebra import MathForge\n\nforge = MathForge()\nsigmoid = forge.get(\"sigmoid\")\n\n# Name\nprint(sigmoid.name)             # \"sigmoid\"\n\n# Symbolic formula (SymPy expression)\nprint(sigmoid.symbolic_expr)    # 1/(1 + exp(-x))\n\n# Variables (free symbols)\nprint(sigmoid.variables)        # [x]\n\n# Parameters\nprint(sigmoid.params)           # {}\n\n# Metadata\nprint(sigmoid.metadata)\n# {'category': 'activation', 'description': '...', ...}\n</code></pre>"},{"location":"tutorial/expressions/#explaining-expressions","title":"Explaining Expressions","text":"<p>Every expression can explain itself:</p> <pre><code>relu = forge.get(\"relu\")\nprint(relu.explain())\n\nsigmoid = forge.get(\"sigmoid\")\nprint(sigmoid.explain())\n\n# Metadata gives you detailed info\nprint(sigmoid.metadata[\"description\"])\nprint(sigmoid.metadata[\"usage\"])\nprint(sigmoid.metadata[\"pros\"])\nprint(sigmoid.metadata[\"cons\"])\n</code></pre>"},{"location":"tutorial/expressions/#expression-arithmetic","title":"Expression Arithmetic","text":"<p>Expressions support standard math operations:</p> <pre><code>from neurogebra import Expression\n\na = Expression(\"a\", \"x**2\")\nb = Expression(\"b\", \"x + 1\")\n\n# Addition\nc = a + b           # x\u00b2 + x + 1\n\n# Subtraction  \nd = a - b           # x\u00b2 - x - 1\n\n# Multiplication\ne = a * b           # x\u00b2 * (x + 1) = x\u00b3 + x\u00b2\n\n# Scalar multiplication\nf = 2 * a           # 2x\u00b2\ng = a * 0.5         # 0.5x\u00b2\n</code></pre>"},{"location":"tutorial/expressions/#cloning-expressions","title":"Cloning Expressions","text":"<p>Create independent copies:</p> <pre><code>original = forge.get(\"relu\")\nclone = original.clone()\n\n# Modifying clone doesn't affect original\nclone.params[\"new_param\"] = 42\nprint(\"new_param\" in original.params)  # False\n</code></pre>"},{"location":"tutorial/expressions/#calling-expressions","title":"Calling Expressions","text":"<p>Expressions are callable (you can use them like functions):</p> <pre><code>relu = forge.get(\"relu\")\n\n# These are equivalent:\nresult1 = relu.eval(x=5)\nresult2 = relu(5)\nprint(result1, result2)  # 5, 5\n</code></pre>"},{"location":"tutorial/expressions/#complete-example","title":"Complete Example","text":"<pre><code>from neurogebra import Expression, MathForge\nimport numpy as np\n\n# === Create Custom Expression ===\ngaussian = Expression(\n    \"gaussian\",\n    \"exp(-x**2 / (2 * sigma**2))\",\n    params={\"sigma\": 1.0},\n    metadata={\n        \"category\": \"statistics\",\n        \"description\": \"Gaussian (bell curve) function\"\n    }\n)\n\n# === Evaluate ===\nx = np.linspace(-3, 3, 7)\ny = gaussian.eval(x=x)\nprint(\"Gaussian values:\")\nfor xi, yi in zip(x, y):\n    print(f\"  x={xi:+.1f}  \u2192  f(x)={yi:.4f}\")\n\n# === Gradient ===\ngrad = gaussian.gradient(\"x\")\nprint(f\"\\nGradient formula: {grad.symbolic_expr}\")\n\n# === Change parameter ===\ngaussian.params[\"sigma\"] = 0.5  # Narrower bell curve\ny_narrow = gaussian.eval(x=x)\nprint(\"\\nNarrower Gaussian:\")\nfor xi, yi in zip(x, y_narrow):\n    print(f\"  x={xi:+.1f}  \u2192  f(x)={yi:.4f}\")\n</code></pre>"},{"location":"tutorial/expressions/#quick-reference","title":"Quick Reference","text":"Property/Method Description Example <code>.name</code> Expression name <code>\"relu\"</code> <code>.symbolic_expr</code> SymPy formula <code>Max(0, x)</code> <code>.variables</code> Free variables <code>[x]</code> <code>.params</code> Parameter dict <code>{\"alpha\": 0.01}</code> <code>.metadata</code> Extra info <code>{\"category\": \"activation\"}</code> <code>.eval(**kwargs)</code> Evaluate numerically <code>.eval(x=5)</code> <code>.gradient(var)</code> Symbolic derivative <code>.gradient(\"x\")</code> <code>.compose(other)</code> Compose: self(other) <code>.compose(linear)</code> <code>.clone()</code> Deep copy <code>.clone()</code> <code>.explain()</code> Plain English explanation <code>.explain()</code> <code>.visualize()</code> Plot the expression <code>.visualize()</code> <p>Next: Activation Functions \u2192</p>"},{"location":"tutorial/gradients/","title":"Gradients &amp; Differentiation","text":"<p>Gradients are how neural networks learn. This is the single most important concept in all of ML.</p>"},{"location":"tutorial/gradients/#the-big-picture","title":"The Big Picture","text":"<p>Training a neural network is just this:</p> <ol> <li>Make a prediction</li> <li>Measure the error (loss)</li> <li>Compute gradients \u2014 how much does each parameter affect the error?</li> <li>Adjust parameters in the direction that reduces error</li> <li>Repeat</li> </ol> <p>Step 3 is what this page is about.</p>"},{"location":"tutorial/gradients/#what-is-a-gradient","title":"What is a Gradient?","text":"<p>A gradient (derivative) tells you: if I change this input a tiny bit, how much does the output change?</p> \\[f(x) = x^2 \\quad \\Rightarrow \\quad f'(x) = 2x\\] <p>At \\(x = 3\\): \\(f'(3) = 6\\), meaning the output changes ~6 times faster than the input at that point.</p>"},{"location":"tutorial/gradients/#symbolic-gradients-in-neurogebra","title":"Symbolic Gradients in Neurogebra","text":"<p>Neurogebra computes gradients symbolically \u2014 you get the actual derivative formula, not just a number:</p> <pre><code>from neurogebra import Expression\n\n# Define a function\nf = Expression(\"quadratic\", \"x**2 + 3*x + 2\")\n\n# Compute its derivative\nf_prime = f.gradient(\"x\")\n\n# See the formula\nprint(f\"f(x) = {f.symbolic_expr}\")       # x**2 + 3*x + 2\nprint(f\"f'(x) = {f_prime.symbolic_expr}\")  # 2*x + 3\n\n# Evaluate at specific points\nprint(f\"f'(0) = {f_prime.eval(x=0)}\")    # 3\nprint(f\"f'(1) = {f_prime.eval(x=1)}\")    # 5\nprint(f\"f'(-1) = {f_prime.eval(x=-1)}\")  # 1\n</code></pre>"},{"location":"tutorial/gradients/#gradients-of-activation-functions","title":"Gradients of Activation Functions","text":"<pre><code>from neurogebra import MathForge\n\nforge = MathForge()\n\n# ReLU gradient\nrelu = forge.get(\"relu\")\nrelu_grad = relu.gradient(\"x\")\nprint(f\"ReLU'(x) = {relu_grad.symbolic_expr}\")\n# Derivative is 1 for x &gt; 0, 0 for x &lt; 0\n\n# Sigmoid gradient\nsigmoid = forge.get(\"sigmoid\")\nsig_grad = sigmoid.gradient(\"x\")\nprint(f\"Sigmoid'(x) = {sig_grad.symbolic_expr}\")\n# \u03c3'(x) = \u03c3(x) \u00b7 (1 - \u03c3(x))\n\n# Tanh gradient\ntanh = forge.get(\"tanh\")\ntanh_grad = tanh.gradient(\"x\")\nprint(f\"Tanh'(x) = {tanh_grad.symbolic_expr}\")\n# tanh'(x) = 1 - tanh\u00b2(x)\n</code></pre>"},{"location":"tutorial/gradients/#gradients-of-loss-functions","title":"Gradients of Loss Functions","text":"<p>This is directly used in training:</p> <pre><code>mse = forge.get(\"mse\")\nprint(f\"MSE = {mse.symbolic_expr}\")\n# (y_pred - y_true)**2\n\nmse_grad = mse.gradient(\"y_pred\")\nprint(f\"dMSE/d(y_pred) = {mse_grad.symbolic_expr}\")\n# 2*(y_pred - y_true)\n\n# Interpretation:\n# If prediction &gt; target \u2192 gradient is positive \u2192 decrease prediction\n# If prediction &lt; target \u2192 gradient is negative \u2192 increase prediction\nprint(f\"  pred=7, true=5: grad = {mse_grad.eval(y_pred=7, y_true=5)}\")  # 4\nprint(f\"  pred=3, true=5: grad = {mse_grad.eval(y_pred=3, y_true=5)}\")  # -4\nprint(f\"  pred=5, true=5: grad = {mse_grad.eval(y_pred=5, y_true=5)}\")  # 0 (perfect!)\n</code></pre>"},{"location":"tutorial/gradients/#higher-order-derivatives","title":"Higher-Order Derivatives","text":"<p>You can differentiate multiple times:</p> <pre><code>f = Expression(\"cubic\", \"x**3\")\n\nf1 = f.gradient(\"x\")     # First derivative\nf2 = f1.gradient(\"x\")    # Second derivative\nf3 = f2.gradient(\"x\")    # Third derivative\n\nprint(f\"f(x)    = {f.symbolic_expr}\")    # x\u00b3\nprint(f\"f'(x)   = {f1.symbolic_expr}\")   # 3x\u00b2\nprint(f\"f''(x)  = {f2.symbolic_expr}\")   # 6x\nprint(f\"f'''(x) = {f3.symbolic_expr}\")   # 6\n</code></pre>"},{"location":"tutorial/gradients/#partial-derivatives","title":"Partial Derivatives","text":"<p>For functions with multiple variables:</p> <pre><code># f(x, y) = x\u00b2y + xy\u00b2\nf = Expression(\"multi\", \"x**2*y + x*y**2\")\n\n# Partial derivative with respect to x\ndf_dx = f.gradient(\"x\")\nprint(f\"\u2202f/\u2202x = {df_dx.symbolic_expr}\")  # 2*x*y + y\u00b2\n\n# Partial derivative with respect to y\ndf_dy = f.gradient(\"y\")\nprint(f\"\u2202f/\u2202y = {df_dy.symbolic_expr}\")  # x\u00b2 + 2*x*y\n</code></pre>"},{"location":"tutorial/gradients/#gradient-descent-using-gradients-to-learn","title":"Gradient Descent \u2014 Using Gradients to Learn","text":"<p>Here's a manual gradient descent implementation to see how gradients drive learning:</p> <pre><code>import numpy as np\nfrom neurogebra import Expression\n\n# Goal: Find x that minimizes f(x) = (x - 3)\u00b2\n# Answer should be x = 3\n\nf = Expression(\"parabola\", \"(x - 3)**2\")\nf_grad = f.gradient(\"x\")  # f'(x) = 2*(x-3)\n\n# Gradient descent\nx = 10.0          # Start far from minimum\nlr = 0.1          # Learning rate\n\nprint(f\"{'Step':&gt;4} | {'x':&gt;8} | {'f(x)':&gt;8} | {'gradient':&gt;8}\")\nprint(\"-\" * 45)\n\nfor step in range(15):\n    fx = f.eval(x=x)\n    gx = f_grad.eval(x=x)\n\n    print(f\"{step:&gt;4} | {x:&gt;8.4f} | {fx:&gt;8.4f} | {gx:&gt;8.4f}\")\n\n    # Update: move opposite to gradient\n    x = x - lr * gx\n\nprint(f\"\\nFinal x = {x:.4f}\")  # Should be close to 3.0\n</code></pre>"},{"location":"tutorial/gradients/#the-vanishing-gradient-problem","title":"The Vanishing Gradient Problem","text":"<p>Some activations have gradients that become very small, making learning slow or impossible:</p> <pre><code>import numpy as np\n\nforge = MathForge()\n\n# Sigmoid gradient becomes tiny for large |x|\nsig_grad = forge.get(\"sigmoid\").gradient(\"x\")\nfor x_val in [-10, -5, 0, 5, 10]:\n    g = sig_grad.eval(x=x_val)\n    print(f\"  sigmoid'({x_val:&gt;3}) = {g:.6f}\")\n\n# Output:\n# sigmoid'(-10) = 0.000045  \u2190 Almost zero! Learning stops.\n# sigmoid'( -5) = 0.006648\n# sigmoid'(  0) = 0.250000  \u2190 OK\n# sigmoid'(  5) = 0.006648\n# sigmoid'( 10) = 0.000045  \u2190 Almost zero!\n\nprint(\"\\nReLU doesn't have this problem:\")\nrelu_grad = forge.get(\"relu\").gradient(\"x\")\nfor x_val in [-10, -5, 0, 5, 10]:\n    g = relu_grad.eval(x=x_val)\n    print(f\"  relu'({x_val:&gt;3}) = {g}\")\n</code></pre> <p>Why this matters</p> <p>The vanishing gradient problem is why ReLU replaced Sigmoid as the default activation for hidden layers. With Sigmoid, deep networks couldn't learn because gradients became too small.</p>"},{"location":"tutorial/gradients/#numerical-gradients-finite-differences","title":"Numerical Gradients (Finite Differences)","text":"<p>Sometimes you want to verify symbolic gradients using numerical approximation:</p> <pre><code>def numerical_gradient(expr, var_name, point, epsilon=1e-5):\n    \"\"\"Compute gradient numerically using finite differences.\"\"\"\n    kwargs_plus = {var_name: point + epsilon}\n    kwargs_minus = {var_name: point - epsilon}\n    return (expr.eval(**kwargs_plus) - expr.eval(**kwargs_minus)) / (2 * epsilon)\n\nf = Expression(\"test\", \"x**3 + 2*x + 1\")\nf_grad = f.gradient(\"x\")\n\nx_point = 2.0\nsymbolic = f_grad.eval(x=x_point)\nnumerical = numerical_gradient(f, \"x\", x_point)\n\nprint(f\"Symbolic gradient:  {symbolic:.6f}\")   # 14.000000\nprint(f\"Numerical gradient: {numerical:.6f}\")  # 14.000000\nprint(f\"Difference: {abs(symbolic - numerical):.10f}\")  # ~0\n</code></pre>"},{"location":"tutorial/gradients/#summary","title":"Summary","text":"Concept What It Is Why It Matters Gradient Rate of change Tells model how to adjust Positive gradient Output increases with input Decrease the parameter Negative gradient Output decreases with input Increase the parameter Zero gradient At a minimum/maximum Training converged (maybe) Vanishing gradient Gradient too small Model stops learning Exploding gradient Gradient too large Training becomes unstable <p>Next: Expression Composition \u2192</p>"},{"location":"tutorial/losses/","title":"Loss Functions","text":"<p>A loss function measures how wrong your model's predictions are. The goal of training is to minimize the loss.</p>"},{"location":"tutorial/losses/#the-concept","title":"The Concept","text":"<pre><code>Model Prediction: \u0177 = 4.5\nActual Value:     y  = 5.0\n\nLoss = some_function(\u0177, y)\n      = how_wrong_is_the_prediction\n\nIf loss is high \u2192 model is bad \u2192 adjust weights\nIf loss is low  \u2192 model is good \u2192 we're done (or close)\n</code></pre>"},{"location":"tutorial/losses/#exploring-loss-functions","title":"Exploring Loss Functions","text":"<pre><code>from neurogebra import MathForge\n\nforge = MathForge()\nlosses = forge.list_all(category=\"loss\")\nprint(losses)\n# ['mse', 'mae', 'binary_crossentropy', 'huber', 'log_cosh', 'hinge', ...]\n</code></pre>"},{"location":"tutorial/losses/#mse-mean-squared-error","title":"MSE \u2014 Mean Squared Error","text":"<p>The most common loss for regression tasks.</p> \\[\\text{MSE} = (y_{pred} - y_{true})^2\\] <pre><code>mse = forge.get(\"mse\")\n\n# Close prediction \u2192 small loss\nprint(mse.eval(y_pred=4.8, y_true=5.0))  # 0.04\n\n# Bad prediction \u2192 large loss\nprint(mse.eval(y_pred=2.0, y_true=5.0))  # 9.0\n\n# Perfect prediction \u2192 zero loss\nprint(mse.eval(y_pred=5.0, y_true=5.0))  # 0.0\n</code></pre> <p>Properties:</p> <ul> <li>Squares the error \u2192 penalizes large errors heavily</li> <li>Always non-negative</li> <li>Smooth and differentiable \u2192 nice gradients</li> </ul> <pre><code># The gradient tells the model HOW to adjust\ngrad = mse.gradient(\"y_pred\")\nprint(grad.symbolic_expr)  # 2*(y_pred - y_true)\n# If pred &gt; true: gradient is positive \u2192 decrease prediction\n# If pred &lt; true: gradient is negative \u2192 increase prediction\n</code></pre> <p>When to use MSE</p> <p>Regression tasks. Best when outliers are rare and you want to penalize large errors.</p>"},{"location":"tutorial/losses/#mae-mean-absolute-error","title":"MAE \u2014 Mean Absolute Error","text":"<p>More robust to outliers than MSE.</p> \\[\\text{MAE} = |y_{pred} - y_{true}|\\] <pre><code>mae = forge.get(\"mae\")\n\nprint(mae.eval(y_pred=4.8, y_true=5.0))  # 0.2\nprint(mae.eval(y_pred=2.0, y_true=5.0))  # 3.0\nprint(mae.eval(y_pred=5.0, y_true=5.0))  # 0.0\n</code></pre> <p>MSE vs MAE:</p> Error MSE MAE Small (0.2) 0.04 0.2 Medium (3.0) 9.0 3.0 Large (10.0) 100.0 10.0 <p>MSE squares errors, so it punishes outliers much more. MAE treats all errors linearly.</p> <p>When to use MAE</p> <p>Regression with outliers. More robust than MSE but converges slower.</p>"},{"location":"tutorial/losses/#binary-cross-entropy","title":"Binary Cross-Entropy","text":"<p>The standard loss for binary classification (yes/no problems).</p> \\[\\text{BCE} = -[y_{true} \\cdot \\log(y_{pred}) + (1-y_{true}) \\cdot \\log(1-y_{pred})]\\] <pre><code>bce = forge.get(\"binary_crossentropy\")\n\n# Model says 0.9 probability, actual is 1 (correct!) \u2192 low loss\nprint(bce.eval(y_pred=0.9, y_true=1.0))  # \u2248 0.105\n\n# Model says 0.1 probability, actual is 1 (wrong!) \u2192 high loss\nprint(bce.eval(y_pred=0.1, y_true=1.0))  # \u2248 2.303\n\n# Model says 0.9, actual is 0 (wrong!) \u2192 high loss\nprint(bce.eval(y_pred=0.9, y_true=0.0))  # \u2248 2.303\n</code></pre> <p>When to use Binary Cross-Entropy</p> <p>Binary classification (spam/not-spam, disease/healthy). Always pair with sigmoid output.</p>"},{"location":"tutorial/losses/#huber-loss","title":"Huber Loss","text":"<p>Best of both worlds \u2014 MSE for small errors, MAE for large errors.</p> \\[L_\\delta = \\begin{cases} \\frac{1}{2}(y_{pred} - y_{true})^2 &amp; \\text{if } |y_{pred} - y_{true}| \\leq \\delta \\\\ \\delta|y_{pred} - y_{true}| - \\frac{1}{2}\\delta^2 &amp; \\text{otherwise} \\end{cases}\\] <pre><code>huber = forge.get(\"huber\")\n\n# Small error \u2192 behaves like MSE (smooth)\n# Large error \u2192 behaves like MAE (robust)\nprint(huber.eval(y_pred=4.8, y_true=5.0))  # \u2248 0.02 (MSE-like)\nprint(huber.eval(y_pred=0.0, y_true=5.0))  # \u2248 4.5  (MAE-like, not 25!)\n</code></pre> <p>When to use Huber</p> <p>Regression with occasional outliers. Great balance between MSE and MAE. Popular in reinforcement learning.</p>"},{"location":"tutorial/losses/#hinge-loss","title":"Hinge Loss","text":"<p>For SVM-style classification with margin.</p> \\[L = \\max(0, 1 - y_{true} \\cdot y_{pred})\\] <pre><code>hinge = forge.get(\"hinge\")\n\n# Correct with margin \u2192 zero loss\nprint(hinge.eval(y_pred=2.0, y_true=1.0))   # 0 (correct &amp; confident)\n\n# Correct but no margin \u2192 some loss  \nprint(hinge.eval(y_pred=0.5, y_true=1.0))  # 0.5\n\n# Wrong \u2192 high loss\nprint(hinge.eval(y_pred=-1.0, y_true=1.0))  # 2.0\n</code></pre>"},{"location":"tutorial/losses/#log-cosh-loss","title":"Log-Cosh Loss","text":"<p>Smooth alternative to Huber Loss.</p> \\[L = \\log(\\cosh(y_{pred} - y_{true}))\\] <pre><code>log_cosh = forge.get(\"log_cosh\")\n\nprint(log_cosh.eval(y_pred=4.8, y_true=5.0))  # \u2248 0.020\nprint(log_cosh.eval(y_pred=0.0, y_true=5.0))  # \u2248 4.307\n</code></pre>"},{"location":"tutorial/losses/#choosing-the-right-loss-function","title":"Choosing the Right Loss Function","text":"<pre><code>What's your task?\n\u2502\n\u251c\u2500\u2500 REGRESSION (predicting a number)\n\u2502   \u251c\u2500\u2500 No outliers       \u2192 MSE\n\u2502   \u251c\u2500\u2500 Has outliers      \u2192 MAE or Huber\n\u2502   \u2514\u2500\u2500 Smooth &amp; robust   \u2192 Log-Cosh\n\u2502\n\u251c\u2500\u2500 BINARY CLASSIFICATION (yes/no)\n\u2502   \u2514\u2500\u2500 \u2192 Binary Cross-Entropy + Sigmoid\n\u2502\n\u251c\u2500\u2500 MULTI-CLASS CLASSIFICATION (cat/dog/bird)\n\u2502   \u2514\u2500\u2500 \u2192 Cross-Entropy + Softmax\n\u2502\n\u2514\u2500\u2500 MARGIN-BASED (SVM-style)\n    \u2514\u2500\u2500 \u2192 Hinge Loss\n</code></pre>"},{"location":"tutorial/losses/#comparing-loss-functions","title":"Comparing Loss Functions","text":"<pre><code>from neurogebra import MathForge\nimport numpy as np\n\nforge = MathForge()\n\n# Different error levels\nerrors = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n\nprint(f\"{'Error':&gt;6} | {'MSE':&gt;8} | {'MAE':&gt;8} | {'Huber':&gt;8}\")\nprint(\"-\" * 40)\n\nfor err in errors:\n    mse_val = forge.get(\"mse\").eval(y_pred=err, y_true=0)\n    mae_val = forge.get(\"mae\").eval(y_pred=err, y_true=0)\n    # Huber needs delta param\n    huber_val = min(0.5 * err**2, abs(err) - 0.5)  # delta=1\n\n    print(f\"{err:&gt;6.1f} | {mse_val:&gt;8.3f} | {mae_val:&gt;8.3f} | {huber_val:&gt;8.3f}\")\n</code></pre>"},{"location":"tutorial/losses/#composing-custom-losses","title":"Composing Custom Losses","text":"<pre><code># Weighted combination\nhybrid = forge.compose(\"mse + 0.1*mae\")\n\n# Evaluate\nresult = hybrid.eval(y_pred=3.0, y_true=5.0)\nprint(f\"Hybrid loss: {result}\")\n</code></pre>"},{"location":"tutorial/losses/#try-it-yourself","title":"Try It Yourself!","text":"<p>Exercise</p> <ol> <li>Compute MSE, MAE, and Huber loss for predictions <code>[1, 2, 3]</code> vs targets <code>[1.5, 2.5, 3.5]</code></li> <li>Which loss function is most sensitive to the error <code>(pred=0, true=10)</code>?</li> <li>Create a custom weighted loss: <code>0.8*mse + 0.2*mae</code></li> <li>Compute the gradient of MSE with respect to <code>y_pred</code></li> </ol> <p>Next: Gradients &amp; Differentiation \u2192</p>"},{"location":"tutorial/mathforge/","title":"MathForge \u2014 The Core","text":"<p><code>MathForge</code> is the central hub of Neurogebra. It's the first thing you create and the main way you interact with the library.</p>"},{"location":"tutorial/mathforge/#what-is-mathforge","title":"What is MathForge?","text":"<p>MathForge is a repository manager that gives you access to 50+ pre-built mathematical expressions used in Machine Learning.</p> <p>Think of it as a library catalog \u2014 you ask for a book (expression) by name, and it gives you a full object you can work with.</p>"},{"location":"tutorial/mathforge/#creating-mathforge","title":"Creating MathForge","text":"<pre><code>from neurogebra import MathForge\n\nforge = MathForge()\n</code></pre> <p>When you create a <code>MathForge</code>, it automatically loads all expressions from these categories:</p> <ul> <li>Activations (ReLU, Sigmoid, Tanh, etc.)</li> <li>Losses (MSE, MAE, Cross-Entropy, etc.)</li> <li>Regularizers (L1, L2, Elastic Net)</li> <li>Algebra (Polynomial, Quadratic, etc.)</li> <li>Calculus (Derivative-related expressions)</li> <li>Statistics (Mean, Variance, etc.)</li> <li>Linear Algebra (Dot product, Norms, etc.)</li> <li>Metrics (Accuracy, Precision, etc.)</li> <li>Optimization (Gradient descent steps)</li> <li>Transforms (Normalization, Standardization)</li> </ul>"},{"location":"tutorial/mathforge/#getting-expressions","title":"Getting Expressions","text":""},{"location":"tutorial/mathforge/#basic-get","title":"Basic Get","text":"<pre><code># Get by name\nrelu = forge.get(\"relu\")\nsigmoid = forge.get(\"sigmoid\")\nmse = forge.get(\"mse\")\n</code></pre>"},{"location":"tutorial/mathforge/#get-with-custom-parameters","title":"Get with Custom Parameters","text":"<pre><code># Leaky ReLU has an 'alpha' parameter (default 0.01)\nleaky = forge.get(\"leaky_relu\")\nprint(leaky.params)  # {'alpha': 0.01}\n\n# Override it\nleaky_custom = forge.get(\"leaky_relu\", params={\"alpha\": 0.2})\nprint(leaky_custom.params)  # {'alpha': 0.2}\n</code></pre>"},{"location":"tutorial/mathforge/#get-as-trainable","title":"Get as Trainable","text":"<pre><code># Make all parameters trainable\ntrainable_expr = forge.get(\"leaky_relu\", trainable=True)\n# Now 'alpha' can be learned from data\n</code></pre>"},{"location":"tutorial/mathforge/#listing-expressions","title":"Listing Expressions","text":""},{"location":"tutorial/mathforge/#list-everything","title":"List Everything","text":"<pre><code>all_exprs = forge.list_all()\nprint(f\"Total expressions: {len(all_exprs)}\")\nprint(all_exprs)\n</code></pre>"},{"location":"tutorial/mathforge/#list-by-category","title":"List by Category","text":"<pre><code># Activation functions\nactivations = forge.list_all(category=\"activation\")\nprint(\"Activations:\", activations)\n\n# Loss functions\nlosses = forge.list_all(category=\"loss\")\nprint(\"Losses:\", losses)\n\n# Regularizers\nregularizers = forge.list_all(category=\"regularizer\")\nprint(\"Regularizers:\", regularizers)\n</code></pre>"},{"location":"tutorial/mathforge/#searching","title":"Searching","text":"<p>Don't know the exact name? Search for it:</p> <pre><code># Search by keyword\nresults = forge.search(\"smooth\")\nprint(results)\n\nresults = forge.search(\"classification\")\nprint(results)\n\nresults = forge.search(\"gradient\")\nprint(results)\n</code></pre>"},{"location":"tutorial/mathforge/#composing-expressions","title":"Composing Expressions","text":"<p>Combine expressions using string notation:</p> <pre><code># Custom loss = MSE + 0.1 * MAE\nhybrid_loss = forge.compose(\"mse + 0.1*mae\")\n\n# Evaluate the composed expression\nresult = hybrid_loss.eval(y_pred=3.0, y_true=5.0)\nprint(f\"Hybrid loss: {result}\")\n</code></pre>"},{"location":"tutorial/mathforge/#registering-custom-expressions","title":"Registering Custom Expressions","text":"<p>You can add your own expressions to the forge:</p> <pre><code>from neurogebra import Expression\n\n# Create a custom expression\nmy_func = Expression(\n    \"my_activation\",\n    \"x * tanh(x)\",\n    metadata={\"category\": \"activation\", \"description\": \"x*tanh(x) activation\"}\n)\n\n# Register it\nforge.register(\"my_activation\", my_func)\n\n# Now you can get it like any built-in expression\nretrieved = forge.get(\"my_activation\")\nprint(retrieved.eval(x=2.0))\n</code></pre>"},{"location":"tutorial/mathforge/#complete-example","title":"Complete Example","text":"<pre><code>from neurogebra import MathForge\n\nforge = MathForge()\n\n# 1. Explore what's available\nprint(\"=== Available Categories ===\")\nfor category in [\"activation\", \"loss\", \"regularizer\"]:\n    exprs = forge.list_all(category=category)\n    print(f\"  {category}: {len(exprs)} expressions\")\n\n# 2. Get and explore an expression\nprint(\"\\n=== Sigmoid ===\")\nsigmoid = forge.get(\"sigmoid\")\nprint(f\"  Formula: {sigmoid.symbolic_expr}\")\nprint(f\"  sigmoid(0) = {sigmoid.eval(x=0)}\")\nprint(f\"  sigmoid(5) = {sigmoid.eval(x=5):.4f}\")\n\n# 3. Compare activations\nprint(\"\\n=== Activation Comparison at x=1.0 ===\")\nfor name in [\"relu\", \"sigmoid\", \"tanh\", \"swish\", \"gelu\"]:\n    expr = forge.get(name)\n    val = expr.eval(x=1.0)\n    print(f\"  {name:&gt;10}: {val:.4f}\")\n\n# 4. Compose a hybrid loss\nprint(\"\\n=== Hybrid Loss ===\")\nloss = forge.compose(\"mse + 0.1*mae\")\nerror = loss.eval(y_pred=2.5, y_true=3.0)\nprint(f\"  Loss value: {error:.4f}\")\n</code></pre>"},{"location":"tutorial/mathforge/#quick-reference","title":"Quick Reference","text":"Method Description Example <code>forge.get(name)</code> Get expression by name <code>forge.get(\"relu\")</code> <code>forge.get(name, params={})</code> Get with custom params <code>forge.get(\"leaky_relu\", params={\"alpha\": 0.2})</code> <code>forge.list_all()</code> List all expressions <code>forge.list_all()</code> <code>forge.list_all(category=)</code> List by category <code>forge.list_all(category=\"activation\")</code> <code>forge.search(query)</code> Search expressions <code>forge.search(\"smooth\")</code> <code>forge.compose(str)</code> Compose from string <code>forge.compose(\"mse + 0.1*mae\")</code> <code>forge.register(name, expr)</code> Add custom expression <code>forge.register(\"my_fn\", expr)</code> <p>Next: Expressions \u2014 Building Blocks \u2192</p>"},{"location":"tutorial/model-builder/","title":"ModelBuilder","text":"<p><code>ModelBuilder</code> provides an educational, intuitive interface for building neural network architectures \u2014 similar to Keras, but with built-in explanations.</p>"},{"location":"tutorial/model-builder/#what-is-modelbuilder","title":"What is ModelBuilder?","text":"<p>ModelBuilder lets you:</p> <ul> <li>Build neural networks layer by layer</li> <li>Understand what each layer does</li> <li>Get explanations and best-practice suggestions</li> <li>Use pre-built templates for common tasks</li> </ul>"},{"location":"tutorial/model-builder/#quick-start","title":"Quick Start","text":"<pre><code>from neurogebra import ModelBuilder\n\nbuilder = ModelBuilder()\n\n# Build a simple classifier\nmodel = builder.Sequential([\n    builder.Dense(128, activation=\"relu\"),\n    builder.Dropout(0.2),\n    builder.Dense(64, activation=\"relu\"),\n    builder.Dense(10, activation=\"softmax\")\n])\n\n# See what you built\nmodel.summary()\n</code></pre>"},{"location":"tutorial/model-builder/#layer-types","title":"Layer Types","text":""},{"location":"tutorial/model-builder/#dense-fully-connected","title":"Dense (Fully Connected)","text":"<p>Every neuron connects to every neuron in the next layer.</p> <pre><code># Basic dense layer\nlayer = builder.Dense(128, activation=\"relu\")\n\n# With input shape (first layer only)\nfirst_layer = builder.Dense(128, activation=\"relu\", input_shape=(784,))\n\n# Learn what it does\nlayer.explain()\n</code></pre> <p>Output of <code>explain()</code>: <pre><code>\ud83d\udcda DENSE Layer\n   Fully connected layer - every neuron connects to every neuron in next layer\n\n   Best used for:\n   \u2022 Classification\n   \u2022 Regression\n   \u2022 Final output layer\n\n   Neurons/Units: 128\n   Activation: relu\n</code></pre></p>"},{"location":"tutorial/model-builder/#dropout","title":"Dropout","text":"<p>Randomly deactivates neurons during training to prevent overfitting.</p> <pre><code>dropout = builder.Dropout(rate=0.2)  # Drop 20% of neurons\ndropout.explain()\n</code></pre>"},{"location":"tutorial/model-builder/#conv2d-convolutional","title":"Conv2D (Convolutional)","text":"<p>Detects patterns in images using sliding filters.</p> <pre><code>conv = builder.Conv2D(filters=32, kernel_size=3, activation=\"relu\")\nconv.explain()\n</code></pre>"},{"location":"tutorial/model-builder/#maxpooling2d","title":"MaxPooling2D","text":"<p>Reduces image dimensions by taking maximum values in regions.</p> <pre><code>pool = builder.MaxPooling2D(pool_size=2)\npool.explain()\n</code></pre>"},{"location":"tutorial/model-builder/#flatten","title":"Flatten","text":"<p>Converts multi-dimensional data to 1D for dense layers.</p> <pre><code>flatten = builder.Flatten()\n</code></pre>"},{"location":"tutorial/model-builder/#batchnorm","title":"BatchNorm","text":"<p>Normalizes layer outputs to stabilize training.</p> <pre><code>bn = builder.BatchNorm()\nbn.explain()\n</code></pre>"},{"location":"tutorial/model-builder/#building-models","title":"Building Models","text":""},{"location":"tutorial/model-builder/#sequential-model","title":"Sequential Model","text":"<p>Layers stacked one after another:</p> <pre><code># Simple classifier\nclassifier = builder.Sequential([\n    builder.Dense(128, activation=\"relu\"),\n    builder.Dropout(0.2),\n    builder.Dense(64, activation=\"relu\"),\n    builder.Dense(10, activation=\"softmax\")\n], name=\"my_classifier\")\n\nclassifier.summary()\n</code></pre>"},{"location":"tutorial/model-builder/#from-template","title":"From Template","text":"<p>Use pre-built architectures:</p> <pre><code># See available templates\nbuilder.list_templates()\n\n# Create from template\nmodel = builder.from_template(\"simple_classifier\")\nmodel.summary()\n\n# Other templates:\n# - \"simple_classifier\"  \u2192 Basic dense network\n# - \"image_classifier\"   \u2192 CNN for images\n# - \"regression\"         \u2192 For predicting numbers\n# - \"binary_classifier\"  \u2192 For yes/no tasks\n</code></pre>"},{"location":"tutorial/model-builder/#model-templates","title":"Model Templates","text":""},{"location":"tutorial/model-builder/#simple-classifier","title":"Simple Classifier","text":"<pre><code>model = builder.from_template(\"simple_classifier\")\nmodel.explain_architecture()\n</code></pre> <p>Architecture: <pre><code>Dense(128, relu) \u2192 Dropout(0.2) \u2192 Dense(64, relu) \u2192 Dense(10, softmax)\n</code></pre></p>"},{"location":"tutorial/model-builder/#image-classifier-cnn","title":"Image Classifier (CNN)","text":"<pre><code>model = builder.from_template(\"image_classifier\")\nmodel.explain_architecture()\n</code></pre> <p>Architecture: <pre><code>Conv2D(32) \u2192 MaxPool \u2192 Conv2D(64) \u2192 MaxPool \u2192 Flatten \u2192 Dense(128) \u2192 Dense(10)\n</code></pre></p>"},{"location":"tutorial/model-builder/#regression-network","title":"Regression Network","text":"<pre><code>model = builder.from_template(\"regression\")\n</code></pre> <p>Architecture: <pre><code>Dense(64, relu) \u2192 Dense(32, relu) \u2192 Dense(1, linear)\n</code></pre></p>"},{"location":"tutorial/model-builder/#binary-classifier","title":"Binary Classifier","text":"<pre><code>model = builder.from_template(\"binary_classifier\")\n</code></pre> <p>Architecture: <pre><code>Dense(64, relu) \u2192 Dropout(0.3) \u2192 Dense(32, relu) \u2192 Dense(1, sigmoid)\n</code></pre></p>"},{"location":"tutorial/model-builder/#architecture-suggestions","title":"Architecture Suggestions","text":"<p>ModelBuilder can suggest architectures based on your task:</p> <pre><code># Get suggestion for a task\nbuilder.suggest_architecture(\n    task=\"classification\",\n    input_size=784,\n    output_size=10\n)\n</code></pre>"},{"location":"tutorial/model-builder/#understanding-your-model","title":"Understanding Your Model","text":""},{"location":"tutorial/model-builder/#summary","title":"Summary","text":"<pre><code>model.summary()\n</code></pre> <p>Shows the layer structure with types, units, and activations.</p>"},{"location":"tutorial/model-builder/#explain-architecture","title":"Explain Architecture","text":"<pre><code>model.explain_architecture()\n</code></pre> <p>Gives plain-English explanation of what each layer does and why it's there.</p>"},{"location":"tutorial/model-builder/#building-an-image-classifier-step-by-step","title":"Building an Image Classifier Step by Step","text":"<pre><code>from neurogebra import ModelBuilder\n\nbuilder = ModelBuilder()\n\n# Step 1: Feature extraction (convolutions)\n# Step 2: Dimensionality reduction (pooling)\n# Step 3: Classification (dense layers)\n\nmodel = builder.Sequential([\n    # Feature extraction\n    builder.Conv2D(32, kernel_size=3, activation=\"relu\"),\n    builder.MaxPooling2D(pool_size=2),\n    builder.Conv2D(64, kernel_size=3, activation=\"relu\"),\n    builder.MaxPooling2D(pool_size=2),\n\n    # Bridge\n    builder.Flatten(),\n\n    # Classification\n    builder.Dense(128, activation=\"relu\"),\n    builder.Dropout(0.5),\n    builder.Dense(10, activation=\"softmax\")\n], name=\"mnist_classifier\")\n\n# Understand it\nmodel.summary()\nmodel.explain_architecture()\n</code></pre>"},{"location":"tutorial/model-builder/#layer-selection-guide","title":"Layer Selection Guide","text":"Task Layers to Use Tabular data classification Dense \u2192 Dropout \u2192 Dense \u2192 Softmax Image classification Conv2D \u2192 MaxPool \u2192 Dense \u2192 Softmax Binary classification Dense \u2192 Dense \u2192 Dense(1) \u2192 Sigmoid Regression Dense \u2192 Dense \u2192 Dense(1) \u2192 Linear Layer When to Use Dense Always (at least for the output) Dropout When you have overfitting Conv2D For images/spatial data MaxPooling2D After Conv2D to reduce size Flatten Between Conv and Dense BatchNorm For deeper networks to stabilize training <p>Next: NeuroCraft Interface \u2192</p>"},{"location":"tutorial/neurocraft/","title":"NeuroCraft Interface","text":"<p><code>NeuroCraft</code> is Neurogebra's enhanced educational interface \u2014 like MathForge but with built-in learning tools, tutorials, and smarter error handling.</p>"},{"location":"tutorial/neurocraft/#neurocraft-vs-mathforge","title":"NeuroCraft vs MathForge","text":"Feature MathForge NeuroCraft Get expressions \u2705 \u2705 Search \u2705 \u2705 Smart with suggestions Educational mode \u274c \u2705 Built-in explanations Tutorials \u274c \u2705 Interactive tutorials \"Did you mean?\" \u274c \u2705 Typo correction Compare expressions \u274c \u2705 Side-by-side comparison Quick access \u274c \u2705 <code>quick_activation()</code>, <code>quick_loss()</code>"},{"location":"tutorial/neurocraft/#getting-started","title":"Getting Started","text":"<pre><code>from neurogebra import NeuroCraft\n\n# Educational mode ON (default) \u2014 shows tips and explanations\ncraft = NeuroCraft(educational_mode=True)\n# Output:\n# \ud83c\udf93 Welcome to Neurogebra!\n#    Type craft.tutorial() to start learning\n#    Type craft.search('activation') to explore\n\n# Quiet mode \u2014 for experienced users\ncraft = NeuroCraft(educational_mode=False)\n</code></pre>"},{"location":"tutorial/neurocraft/#getting-expressions","title":"Getting Expressions","text":"<pre><code># Standard get\nrelu = craft.get(\"relu\")\n\n# With instant explanation\nsigmoid = craft.get(\"sigmoid\", explain=True)\n# Automatically prints explanation when educational_mode=True\n\n# With parameter override\nleaky = craft.get(\"leaky_relu\", params={\"alpha\": 0.2})\n</code></pre>"},{"location":"tutorial/neurocraft/#smart-error-handling","title":"Smart Error Handling","text":"<pre><code># If you mistype:\ntry:\n    craft.get(\"rellu\")  # Typo!\nexcept KeyError as e:\n    print(e)\n# Expression 'rellu' not found.\n#    Did you mean: relu, leaky_relu, gelu?\n#    Use craft.search('rellu') to find related expressions.\n</code></pre>"},{"location":"tutorial/neurocraft/#searching-with-neurocraft","title":"Searching with NeuroCraft","text":"<pre><code># Search with detailed output\nresults = craft.search(\"activation\")\n# Prints a formatted list with descriptions\n\n# Search by category\nresults = craft.search(\"loss\", category=\"loss\")\n\n# Search specific terms\nresults = craft.search(\"smooth\")\nresults = craft.search(\"classification\")\n</code></pre>"},{"location":"tutorial/neurocraft/#quick-access-methods","title":"Quick Access Methods","text":"<pre><code># Quick activation \u2014 get + explain in one call\nrelu = craft.quick_activation(\"relu\")\n\n# Quick loss\nmse = craft.quick_loss(\"mse\")\n\n# These automatically show explanations in educational mode\n</code></pre>"},{"location":"tutorial/neurocraft/#comparing-expressions","title":"Comparing Expressions","text":"<pre><code># Visual comparison of activations\ncraft.compare(\n    [\"relu\", \"sigmoid\", \"tanh\", \"swish\"],\n    metric=\"behavior\"\n)\n\n# Compare gradients\ncraft.compare(\n    [\"relu\", \"sigmoid\", \"tanh\"],\n    metric=\"gradient\"\n)\n</code></pre>"},{"location":"tutorial/neurocraft/#interactive-tutorials","title":"Interactive Tutorials","text":"<pre><code># Show tutorial menu\ncraft.tutorial()\n\n# Start a specific tutorial\ncraft.tutorial(\"basics\")\ncraft.tutorial(\"activations\")\ncraft.tutorial(\"training\")\ncraft.tutorial(\"first_model\")\n</code></pre>"},{"location":"tutorial/neurocraft/#listing-all-expressions","title":"Listing All Expressions","text":"<pre><code># All expressions\nall_exprs = craft.list_all()\nprint(f\"Total: {len(all_exprs)}\")\n\n# By category\nprint(\"Activations:\", craft.list_all(category=\"activation\"))\nprint(\"Losses:\", craft.list_all(category=\"loss\"))\nprint(\"Regularizers:\", craft.list_all(category=\"regularizer\"))\n</code></pre>"},{"location":"tutorial/neurocraft/#composing-with-neurocraft","title":"Composing with NeuroCraft","text":"<pre><code># String composition\nhybrid_loss = craft.compose(\"mse + 0.1*mae\")\n\n# Evaluate\nresult = hybrid_loss.eval(y_pred=3.0, y_true=5.0)\nprint(f\"Loss: {result}\")\n</code></pre>"},{"location":"tutorial/neurocraft/#registering-custom-expressions","title":"Registering Custom Expressions","text":"<pre><code>from neurogebra import Expression\n\n# Create your own\nmy_activation = Expression(\n    \"xtanh\",\n    \"x * tanh(x)\",\n    metadata={\n        \"category\": \"activation\",\n        \"description\": \"x*tanh(x) \u2014 smooth activation\"\n    }\n)\n\n# Register it\ncraft.register(\"xtanh\", my_activation)\n\n# Now available everywhere\nretrieved = craft.get(\"xtanh\")\nprint(retrieved.eval(x=2.0))\n</code></pre>"},{"location":"tutorial/neurocraft/#when-to-use-neurocraft-vs-mathforge","title":"When to Use NeuroCraft vs MathForge","text":"Scenario Use Learning/exploring NeuroCraft with <code>educational_mode=True</code> Production code MathForge (lighter, no prints) Interactive notebooks NeuroCraft Scripts and pipelines MathForge Teaching others NeuroCraft <p>Next: Datasets \u2192</p>"},{"location":"tutorial/tensors/","title":"Tensors","text":"<p>Tensors extend the <code>Value</code> concept to multi-dimensional arrays \u2014 enabling batched operations essential for efficient ML.</p>"},{"location":"tutorial/tensors/#what-is-a-tensor","title":"What is a Tensor?","text":"<p>A tensor is a multi-dimensional array with automatic gradient tracking:</p> <ul> <li>Scalar (0D tensor): <code>42</code></li> <li>Vector (1D tensor): <code>[1, 2, 3]</code></li> <li>Matrix (2D tensor): <code>[[1, 2], [3, 4]]</code></li> <li>3D tensor: A batch of matrices</li> </ul>"},{"location":"tutorial/tensors/#creating-tensors","title":"Creating Tensors","text":"<pre><code>from neurogebra.core.autograd import Tensor\n\n# From a list\nt = Tensor([1.0, 2.0, 3.0])\nprint(t)        # Tensor(shape=(3,), requires_grad=False)\nprint(t.data)   # [1. 2. 3.]\n\n# With gradient tracking enabled\nt = Tensor([1.0, 2.0, 3.0], requires_grad=True)\nprint(t)        # Tensor(shape=(3,), requires_grad=True)\nprint(t.grad)   # [0. 0. 0.] \u2014 initialized to zeros\n\n# 2D tensor\nm = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\nprint(m.shape)  # (2, 2)\nprint(m.ndim)   # 2\n</code></pre>"},{"location":"tutorial/tensors/#tensor-properties","title":"Tensor Properties","text":"<pre><code>t = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n\nprint(t.shape)          # (3,)\nprint(t.ndim)           # 1\nprint(t.data)           # [1. 2. 3.]\nprint(t.grad)           # [0. 0. 0.]\nprint(t.requires_grad)  # True\n</code></pre>"},{"location":"tutorial/tensors/#tensor-operations","title":"Tensor Operations","text":""},{"location":"tutorial/tensors/#element-wise-arithmetic","title":"Element-wise Arithmetic","text":"<pre><code>a = Tensor([1.0, 2.0, 3.0], requires_grad=True)\nb = Tensor([4.0, 5.0, 6.0], requires_grad=True)\n\nc = a + b              # [5.0, 7.0, 9.0]\nd = a * b              # [4.0, 10.0, 18.0]\ne = a - b              # [-3.0, -3.0, -3.0]\nf = a ** 2             # [1.0, 4.0, 9.0]\n\nprint(f\"a + b = {c.data}\")\nprint(f\"a * b = {d.data}\")\nprint(f\"a ** 2 = {f.data}\")\n</code></pre>"},{"location":"tutorial/tensors/#scalar-operations","title":"Scalar Operations","text":"<pre><code>a = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n\nb = a * 3              # [3.0, 6.0, 9.0]\nc = a + 10             # [11.0, 12.0, 13.0]\n</code></pre>"},{"location":"tutorial/tensors/#reduction-operations","title":"Reduction Operations","text":"<pre><code>a = Tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n\ns = a.sum()     # Sum of all elements \u2192 10.0\nm = a.mean()    # Mean of all elements \u2192 2.5\n\nprint(f\"sum = {s.data}\")   # 10.0\nprint(f\"mean = {m.data}\")  # 2.5\n</code></pre>"},{"location":"tutorial/tensors/#backward-pass-with-tensors","title":"Backward Pass with Tensors","text":"<pre><code>from neurogebra.core.autograd import Tensor\n\n# Create tensor with gradient tracking\nx = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n\n# Computation: y = sum(x\u00b2) = 1 + 4 + 9 = 14\ny = (x ** 2).sum()\n\n# Backward pass\ny.backward()\n\nprint(f\"x = {x.data}\")       # [1. 2. 3.]\nprint(f\"y = x\u00b2 sum = {y.data}\")  # 14.0\nprint(f\"dy/dx = {x.grad}\")   # [2. 4. 6.] \u2014 which is 2*x\n</code></pre>"},{"location":"tutorial/tensors/#practical-example-batch-mse-loss","title":"Practical Example: Batch MSE Loss","text":"<pre><code>from neurogebra.core.autograd import Tensor\n\n# Predictions and targets (batch of 4)\npredictions = Tensor([2.5, 0.1, 3.0, 7.0], requires_grad=True)\ntargets = Tensor([3.0, 0.0, 2.5, 8.0])\n\n# MSE = mean((pred - target)\u00b2)\ndiff = predictions - targets\nsquared = diff ** 2\nloss = squared.mean()\n\nprint(f\"Predictions: {predictions.data}\")\nprint(f\"Targets:     {targets.data}\")\nprint(f\"MSE Loss:    {loss.data:.4f}\")\n\n# Backpropagate\nloss.backward()\nprint(f\"Gradients:   {predictions.grad}\")\n# Each gradient = 2*(pred-target)/n\n</code></pre>"},{"location":"tutorial/tensors/#zeroing-gradients","title":"Zeroing Gradients","text":"<p>Always zero gradients between training iterations:</p> <pre><code>x = Tensor([1.0, 2.0], requires_grad=True)\n\n# First iteration\ny = (x ** 2).sum()\ny.backward()\nprint(f\"After 1st backward: grad = {x.grad}\")  # [2. 4.]\n\n# Zero and compute again\nx.zero_grad()\ny = (x ** 2).sum()\ny.backward()\nprint(f\"After zero + 2nd backward: grad = {x.grad}\")  # [2. 4.] \u2014 correct\n</code></pre>"},{"location":"tutorial/tensors/#key-differences-value-vs-tensor","title":"Key Differences: Value vs Tensor","text":"Feature Value Tensor Data type Single float NumPy array Shape Scalar Any shape Use case Educational, simple networks Batched training Gradient Single float Array (same shape as data) Operations +, , *, relu, sigmoid, tanh +, , *, sum, mean <p>Next: ModelBuilder \u2192</p>"},{"location":"tutorial/training-observatory/","title":"\ud83d\udd2d Training Observatory","text":"<p>New in v1.2.1 \u2014 See every neuron fire. Watch every gradient flow. Understand every weight update \u2014 in colour.</p> <p>The Training Observatory is an advanced training logging and visualization system that brings unprecedented mathematical transparency to neural network training.</p>"},{"location":"tutorial/training-observatory/#quick-start","title":"Quick Start","text":"<p>Add one argument to <code>model.compile()</code> and the Observatory is active:</p> <pre><code>from neurogebra.builders.model_builder import ModelBuilder\n\nbuilder = ModelBuilder()\nmodel = builder.Sequential([\n    builder.Dense(64, activation=\"relu\"),\n    builder.Dense(32, activation=\"tanh\"),\n    builder.Dense(1, activation=\"sigmoid\"),\n], name=\"my_model\")\n\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    learning_rate=0.01,\n    log_level=\"expert\",          # \u2190 enables the Observatory\n)\n\nmodel.fit(X_train, y_train, epochs=20, batch_size=32)\n</code></pre>"},{"location":"tutorial/training-observatory/#log-levels","title":"Log Levels","text":"Level Value What You See <code>\"silent\"</code> 0 Nothing (turn off all logging) <code>\"basic\"</code> 1 Epoch-level loss/accuracy, training start/end <code>\"detailed\"</code> 2 + Batch-level progress, timing information <code>\"expert\"</code> 3 + Layer-by-layer formulas, gradient norms, weight stats <code>\"debug\"</code> 4 + Every tensor shape, raw statistics, full computation trace"},{"location":"tutorial/training-observatory/#colour-coding","title":"Colour Coding","text":"<p>The Observatory uses colour to communicate at a glance:</p> <ul> <li>\ud83d\udfe2 Green \u2014 Healthy: loss decreasing, gradients stable, metrics improving</li> <li>\ud83d\udfe1 Yellow \u2014 Warning: something needs attention (high variance, early saturation)</li> <li>\ud83d\udd34 Red \u2014 Danger: vanishing/exploding gradients, diverging loss</li> <li>\ud83d\udea8 White-on-Red \u2014 Critical: NaN/Inf detected, training corrupted</li> <li>\ud83d\udfe3 Magenta \u2014 Mathematical formulas (forward/backward equations)</li> <li>\ud83d\udd35 Blue \u2014 Informational (progress messages)</li> <li>\u2b1c Dim white \u2014 Supplementary details</li> </ul>"},{"location":"tutorial/training-observatory/#preset-configurations","title":"Preset Configurations","text":"<p>Instead of setting <code>log_level</code>, you can pass a full <code>LogConfig</code> object:</p> <pre><code>from neurogebra.logging.config import LogConfig\n\n# Choose a preset\nconfig = LogConfig.minimal()     # Just epoch progress\nconfig = LogConfig.standard()    # + timing + health checks\nconfig = LogConfig.verbose()     # Full math depth \u2014 every formula &amp; gradient\nconfig = LogConfig.research()    # Everything + auto-export to files\n\nmodel.compile(loss=\"mse\", optimizer=\"adam\", log_config=config)\n</code></pre>"},{"location":"tutorial/training-observatory/#customising-a-preset","title":"Customising a preset","text":"<pre><code>config = LogConfig.verbose()\nconfig.health_check_interval = 5    # run diagnostics every 5 epochs\nconfig.export_formats = [\"json\", \"html\"]\nconfig.export_dir = \"./my_logs\"\n\nmodel.compile(loss=\"mse\", optimizer=\"adam\", log_config=config)\n</code></pre>"},{"location":"tutorial/training-observatory/#what-the-observatory-shows","title":"What the Observatory Shows","text":""},{"location":"tutorial/training-observatory/#forward-pass-formulas","title":"Forward Pass Formulas","text":"<pre><code>Forward:  a\u2081 = relu(W\u2081\u00b7x + b\u2081)    \u2502 shape: (32, 64) \u2192 (32, 32)\nForward:  a\u2082 = tanh(W\u2082\u00b7a\u2081 + b\u2082)   \u2502 shape: (32, 32) \u2192 (32, 16)\nForward:  \u0177  = \u03c3(W\u2083\u00b7a\u2082 + b\u2083)      \u2502 shape: (32, 16) \u2192 (32, 1)\n</code></pre>"},{"location":"tutorial/training-observatory/#backward-pass-formulas","title":"Backward Pass Formulas","text":"<pre><code>Backward: \u2202L/\u2202W\u2083 = \u2202L/\u2202\u0177 \u2299 \u03c3'(z\u2083) \u00b7 a\u2082\u1d40\nBackward: \u2202L/\u2202W\u2082 = \u2202L/\u2202a\u2082 \u2299 tanh'(z\u2082) \u00b7 a\u2081\u1d40\nBackward: \u2202L/\u2202W\u2081 = \u2202L/\u2202a\u2081 \u2299 relu'(z\u2081) \u00b7 x\u1d40\n</code></pre>"},{"location":"tutorial/training-observatory/#health-diagnostics","title":"Health Diagnostics","text":"<pre><code>\ud83d\udea8 [CRITICAL] NaN/Inf Detected\n   NaN values found in training loss!\n   \u2192 Check for division by zero in your data\n   \u2192 Reduce learning rate (try 1e-4)\n   \u2192 Add gradient clipping\n\n\u26a0\ufe0f  [WARNING] Overfitting Detected\n   Validation loss increasing while training loss decreases\n   \u2192 Add dropout layers (rate 0.2-0.5)\n   \u2192 Reduce model complexity\n   \u2192 Increase training data\n</code></pre>"},{"location":"tutorial/training-observatory/#export-formats","title":"Export Formats","text":"Format File Contents JSON <code>training_log.json</code> Full structured event log CSV <code>metrics.csv</code> Epoch-level metrics table HTML <code>report.html</code> Self-contained report with Chart.js graphs Markdown <code>report.md</code> Human-readable training report <pre><code>config = LogConfig.research()\nconfig.export_formats = [\"json\", \"csv\", \"html\", \"markdown\"]\nconfig.export_dir = \"./training_logs\"\n\nmodel.compile(loss=\"mse\", optimizer=\"adam\", log_config=config)\nmodel.fit(X, y, epochs=50)\n# \u2192 Files saved to ./training_logs/\n</code></pre>"},{"location":"tutorial/training-observatory/#standalone-usage","title":"Standalone Usage","text":"<p>You can use the monitoring tools without a model:</p> <pre><code>from neurogebra.logging.monitors import GradientMonitor\nfrom neurogebra.logging.health_checks import SmartHealthChecker\nimport numpy as np\n\n# Check gradient health\ngm = GradientMonitor()\nstats = gm.record(\"my_layer\", np.random.randn(64, 32) * 0.001)\nprint(stats[\"status\"])   # \"healthy\" | \"danger\" | \"critical\"\n\n# Diagnose training history\nchecker = SmartHealthChecker()\nalerts = checker.run_all(\n    epoch=10,\n    train_losses=[1.0, 0.8, 0.5, 0.3, 0.15],\n    val_losses=[1.0, 0.9, 0.85, 0.95, 1.1],\n)\nfor alert in alerts:\n    print(f\"[{alert.severity}] {alert.message}\")\n</code></pre>"},{"location":"tutorial/training-observatory/#next-steps","title":"Next Steps","text":"<ul> <li>See the Observatory Deep Dive for advanced usage</li> <li>Run the full demo: <code>python examples/training_observatory_demo.py</code></li> <li>Read the API Reference for complete method documentation</li> </ul>"},{"location":"tutorial/training/","title":"Training Expressions","text":"<p>This is where the magic happens \u2014 teaching expressions to learn from data.</p>"},{"location":"tutorial/training/#the-concept","title":"The Concept","text":"<p>\"Training\" means finding the parameter values that make the expression match the data as closely as possible.</p> <pre><code>Before training: y = 0.0*x + 0.0  (random \u2014 useless)\nAfter training:  y = 2.0*x + 1.0  (learned from data!)\n</code></pre>"},{"location":"tutorial/training/#step-by-step-training-a-linear-model","title":"Step-by-Step: Training a Linear Model","text":""},{"location":"tutorial/training/#step-1-create-a-trainable-expression","title":"Step 1: Create a Trainable Expression","text":"<pre><code>from neurogebra import Expression\n\nmodel = Expression(\n    \"linear_model\",\n    \"m*x + b\",\n    params={\"m\": 0.0, \"b\": 0.0},        # Start with zeros\n    trainable_params=[\"m\", \"b\"]           # These will be learned\n)\n\nprint(f\"Before training: y = {model.params['m']}*x + {model.params['b']}\")\n# Before training: y = 0.0*x + 0.0\n</code></pre>"},{"location":"tutorial/training/#step-2-prepare-data","title":"Step 2: Prepare Data","text":"<pre><code>import numpy as np\n\n# True relationship: y = 2x + 1\nnp.random.seed(42)\nX = np.linspace(0, 10, 100)\ny = 2 * X + 1 + np.random.normal(0, 0.5, 100)  # Add some noise\n</code></pre>"},{"location":"tutorial/training/#step-3-create-a-trainer","title":"Step 3: Create a Trainer","text":"<pre><code>from neurogebra.core.trainer import Trainer\n\ntrainer = Trainer(\n    model,\n    learning_rate=0.01,   # How big each adjustment step is\n    optimizer=\"sgd\"        # Stochastic Gradient Descent\n)\n</code></pre>"},{"location":"tutorial/training/#step-4-train","title":"Step 4: Train!","text":"<pre><code>history = trainer.fit(\n    X, y,\n    epochs=200,       # How many times to loop through the data\n    verbose=True       # Print progress\n)\n</code></pre> <p>Output: <pre><code>Epoch    0/200: Loss = 25.431200\nEpoch   20/200: Loss = 3.214100\nEpoch   40/200: Loss = 0.891230\nEpoch   60/200: Loss = 0.412340\n...\nEpoch  200/200: Loss = 0.251230\n</code></pre></p>"},{"location":"tutorial/training/#step-5-check-results","title":"Step 5: Check Results","text":"<pre><code>print(f\"After training: y = {model.params['m']:.2f}*x + {model.params['b']:.2f}\")\n# After training: y = 2.01*x + 0.98\n# Very close to the true y = 2x + 1!\n</code></pre>"},{"location":"tutorial/training/#understanding-the-trainer","title":"Understanding the Trainer","text":""},{"location":"tutorial/training/#optimizers","title":"Optimizers","text":"Optimizer Description When to Use <code>\"sgd\"</code> Stochastic Gradient Descent Simple, educational, basic tasks <code>\"adam\"</code> Adaptive Moment Estimation Default choice, works almost always <pre><code># SGD \u2014 simple but sometimes slow\ntrainer_sgd = Trainer(model, learning_rate=0.01, optimizer=\"sgd\")\n\n# Adam \u2014 adaptive learning rate, usually faster\ntrainer_adam = Trainer(model, learning_rate=0.01, optimizer=\"adam\")\n</code></pre>"},{"location":"tutorial/training/#learning-rate","title":"Learning Rate","text":"<p>The learning rate controls how much parameters change each step:</p> <pre><code># Too high \u2192 overshoots, loss oscillates or explodes\ntrainer = Trainer(model, learning_rate=1.0)    # Bad!\n\n# Too low \u2192 takes forever to converge\ntrainer = Trainer(model, learning_rate=0.00001)  # Very slow!\n\n# Just right \u2192 smooth convergence\ntrainer = Trainer(model, learning_rate=0.01)    # Good starting point\n</code></pre>"},{"location":"tutorial/training/#loss-functions","title":"Loss Functions","text":"<pre><code># Default: MSE (mean squared error)\nhistory = trainer.fit(X, y, loss_fn=\"mse\")\n\n# Alternative: MAE (mean absolute error)\nhistory = trainer.fit(X, y, loss_fn=\"mae\")\n\n# Alternative: Huber (robust to outliers)\nhistory = trainer.fit(X, y, loss_fn=\"huber\")\n</code></pre>"},{"location":"tutorial/training/#mini-batch-training","title":"Mini-Batch Training","text":"<pre><code># Full batch (default) \u2014 uses all data each step\nhistory = trainer.fit(X, y, batch_size=None)\n\n# Mini-batch \u2014 uses small chunks (faster for large datasets)\nhistory = trainer.fit(X, y, batch_size=32)\n</code></pre>"},{"location":"tutorial/training/#training-history","title":"Training History","text":"<p>The <code>fit()</code> method returns a history dictionary:</p> <pre><code>history = trainer.fit(X, y, epochs=100)\n\n# Loss over time\nprint(history[\"loss\"][:5])   # First 5 losses\nprint(history[\"loss\"][-5:])  # Last 5 losses\n\n# Parameters over time\nprint(history[\"params\"][0])    # Parameters at epoch 0\nprint(history[\"params\"][-1])   # Parameters at last epoch\n</code></pre>"},{"location":"tutorial/training/#example-training-a-quadratic-model","title":"Example: Training a Quadratic Model","text":"<pre><code>import numpy as np\nfrom neurogebra import Expression\nfrom neurogebra.core.trainer import Trainer\n\n# True function: y = x\u00b2 - 2x + 1\nX = np.linspace(-3, 3, 100)\ny = X**2 - 2*X + 1 + np.random.normal(0, 0.3, 100)\n\n# Model with unknown coefficients\nmodel = Expression(\n    \"quadratic\",\n    \"a*x**2 + b*x + c\",\n    params={\"a\": 0.0, \"b\": 0.0, \"c\": 0.0},\n    trainable_params=[\"a\", \"b\", \"c\"]\n)\n\n# Train\ntrainer = Trainer(model, learning_rate=0.001, optimizer=\"adam\")\nhistory = trainer.fit(X, y, epochs=500, verbose=True)\n\nprint(f\"\\nLearned: y = {model.params['a']:.2f}x\u00b2 + ({model.params['b']:.2f})x + {model.params['c']:.2f}\")\n# Expected: y \u2248 1.00x\u00b2 + (-2.00)x + 1.00\n</code></pre>"},{"location":"tutorial/training/#example-using-a-callback","title":"Example: Using a Callback","text":"<pre><code>def my_callback(epoch, loss, params):\n    \"\"\"Called after each epoch.\"\"\"\n    if loss &lt; 0.5:\n        print(f\"  [Early stop possible] Epoch {epoch}: loss = {loss:.4f}\")\n\ntrainer = Trainer(model, learning_rate=0.01, optimizer=\"adam\")\nhistory = trainer.fit(X, y, epochs=200, callback=my_callback)\n</code></pre>"},{"location":"tutorial/training/#training-tips","title":"Training Tips","text":"<p>Best Practices</p> <p>Start with Adam optimizer \u2014 it handles most situations well.</p> <p>Use learning rate 0.01 as a starting point. If loss oscillates, decrease it. If loss decreases too slowly, increase it.</p> <p>Watch the loss curve:</p> <ul> <li>Smooth decrease \u2192 good</li> <li>Oscillating \u2192 learning rate too high</li> <li>Flat (no decrease) \u2192 learning rate too low or model too simple</li> <li>Sudden explosion \u2192 learning rate WAY too high</li> </ul> <p>Normalize your data before training.</p> <p>Use enough epochs \u2014 but not too many (overfitting risk).</p>"},{"location":"tutorial/training/#complete-training-pipeline","title":"Complete Training Pipeline","text":"<pre><code>import numpy as np\nfrom neurogebra import Expression\nfrom neurogebra.core.trainer import Trainer\n\n# 1. Generate data\nnp.random.seed(42)\nX = np.linspace(-5, 5, 200)\ny = 0.5 * X**2 + 2 * X - 3 + np.random.normal(0, 1, 200)\n\n# 2. Split data (80% train, 20% test)\nsplit = int(0.8 * len(X))\nX_train, X_test = X[:split], X[split:]\ny_train, y_test = y[:split], y[split:]\n\n# 3. Define model\nmodel = Expression(\n    \"polynomial\",\n    \"a*x**2 + b*x + c\",\n    params={\"a\": 0.0, \"b\": 0.0, \"c\": 0.0},\n    trainable_params=[\"a\", \"b\", \"c\"]\n)\n\n# 4. Train\ntrainer = Trainer(model, learning_rate=0.001, optimizer=\"adam\")\nhistory = trainer.fit(X_train, y_train, epochs=500, verbose=True)\n\n# 5. Evaluate on test set\npredictions = np.array([model.eval(x=xi) for xi in X_test])\ntest_mse = np.mean((predictions - y_test) ** 2)\n\nprint(f\"\\nLearned: y = {model.params['a']:.2f}x\u00b2 + {model.params['b']:.2f}x + {model.params['c']:.2f}\")\nprint(f\"Test MSE: {test_mse:.4f}\")\nprint(f\"True:     y = 0.50x\u00b2 + 2.00x - 3.00\")\n</code></pre> <p>Next: Autograd Engine \u2192</p>"},{"location":"tutorials/advanced/","title":"Advanced Tutorial","text":"<p>This tutorial covers framework bridges, visualization, and advanced patterns.</p>"},{"location":"tutorials/advanced/#framework-bridges","title":"Framework Bridges","text":""},{"location":"tutorials/advanced/#pytorch-integration","title":"PyTorch Integration","text":"<pre><code>from neurogebra import MathForge\nfrom neurogebra.bridges.pytorch_bridge import to_pytorch\n\nforge = MathForge()\nsigmoid = forge.get(\"sigmoid\")\n\n# Convert to PyTorch module\ntorch_sigmoid = to_pytorch(sigmoid)\n\n# Use in PyTorch\nimport torch\nx = torch.randn(10)\noutput = torch_sigmoid(x)\n</code></pre>"},{"location":"tutorials/advanced/#creating-custom-layers","title":"Creating Custom Layers","text":"<pre><code>from neurogebra import Expression\nfrom neurogebra.bridges.pytorch_bridge import to_pytorch\n\ncustom = Expression(\n    \"custom_act\",\n    \"x * tanh(log(1 + exp(x)))\",  # x * tanh(softplus(x)) = Mish\n)\n\n# Use as PyTorch module\nmish_module = to_pytorch(custom)\n</code></pre>"},{"location":"tutorials/advanced/#visualization","title":"Visualization","text":""},{"location":"tutorials/advanced/#static-plots","title":"Static Plots","text":"<pre><code>from neurogebra import MathForge\nfrom neurogebra.viz.plotting import (\n    plot_expression,\n    plot_comparison,\n    plot_gradient,\n    plot_training_history,\n)\n\nforge = MathForge()\n\n# Plot single expression\nrelu = forge.get(\"relu\")\nfig = plot_expression(relu, x_range=(-3, 3))\n\n# Compare multiple\nactivations = [\n    forge.get(\"relu\"),\n    forge.get(\"sigmoid\"),\n    forge.get(\"tanh\"),\n    forge.get(\"swish\"),\n]\nfig = plot_comparison(activations)\n\n# Plot with gradient\nsigmoid = forge.get(\"sigmoid\")\nfig = plot_gradient(sigmoid)\n</code></pre>"},{"location":"tutorials/advanced/#interactive-plots","title":"Interactive Plots","text":"<pre><code>from neurogebra.viz.interactive import interactive_plot, interactive_comparison\n\n# Requires: pip install neurogebra[viz]\nfig = interactive_plot(sigmoid, x_range=(-5, 5))\nfig.show()\n</code></pre>"},{"location":"tutorials/advanced/#advanced-composition-patterns","title":"Advanced Composition Patterns","text":""},{"location":"tutorials/advanced/#building-loss-functions","title":"Building Loss Functions","text":"<pre><code>from neurogebra import MathForge, Expression\nfrom neurogebra.repository.regularizers import get_regularizers\n\nforge = MathForge()\n\n# Base loss\nmse = forge.get(\"mse\")\n\n# Add L2 regularization\nregs = get_regularizers()\nl2 = regs[\"l2\"]\n\n# Custom regularized loss\nreg_loss = mse + 0.01 * l2\n</code></pre>"},{"location":"tutorials/advanced/#expression-calculus","title":"Expression Calculus","text":"<pre><code>from neurogebra import Expression\n\n# Create expression\nf = Expression(\"gaussian\", \"exp(-x**2 / 2)\")\n\n# Differentiate\nf_prime = f.gradient(\"x\")\nf_double_prime = f_prime.gradient(\"x\")\n\n# Integrate\nF = f.integrate(\"x\")\n\n# Simplify\nsimplified = f_double_prime.simplify()\n</code></pre>"},{"location":"tutorials/advanced/#explanation-engine","title":"Explanation Engine","text":"<pre><code>from neurogebra import MathForge\nfrom neurogebra.utils.explain import ExpressionExplainer\n\nforge = MathForge()\ngelu = forge.get(\"gelu\")\n\n# Different formats\ntext = ExpressionExplainer.explain(gelu, level=\"advanced\", format=\"text\")\nmd = ExpressionExplainer.explain(gelu, level=\"advanced\", format=\"markdown\")\nlatex = ExpressionExplainer.explain(gelu, level=\"advanced\", format=\"latex\")\n\nprint(md)\n</code></pre>"},{"location":"tutorials/advanced/#performance-tips","title":"Performance Tips","text":"<ol> <li>Batch evaluation: Evaluate with arrays instead of loops</li> <li>Pre-compile: Expressions compile on creation; reuse them</li> <li>Minimize symbolic ops: Use <code>.eval()</code> for numerical work, symbolic only when needed</li> <li>Use Numba: Install <code>neurogebra[fast]</code> for JIT-compiled evaluation</li> </ol>"},{"location":"tutorials/advanced/#best-practices","title":"Best Practices","text":"<ul> <li>Use <code>MathForge.search()</code> to discover expressions</li> <li>Always check <code>.explain()</code> before using unfamiliar expressions</li> <li>Register custom expressions for reuse</li> <li>Use the trainer for parameter fitting instead of manual optimization</li> <li>Leverage composition for building complex expressions from simple ones</li> </ul>"},{"location":"tutorials/beginner/","title":"Beginner Tutorial","text":"<p>Welcome to Neurogebra! This tutorial covers the fundamental concepts.</p>"},{"location":"tutorials/beginner/#what-is-neurogebra","title":"What is Neurogebra?","text":"<p>Neurogebra is a mathematics library designed for AI developers. It lets you work with mathematical expressions that are:</p> <ul> <li>Symbolic - See and manipulate formulas</li> <li>Numerical - Evaluate efficiently with NumPy</li> <li>Trainable - Learn parameters from data</li> <li>Educational - Understand what you're using</li> </ul>"},{"location":"tutorials/beginner/#step-1-create-a-mathforge","title":"Step 1: Create a MathForge","text":"<p>MathForge is your main entry point:</p> <pre><code>from neurogebra import MathForge\n\nforge = MathForge()\n</code></pre>"},{"location":"tutorials/beginner/#step-2-get-an-expression","title":"Step 2: Get an Expression","text":"<pre><code># Get ReLU activation\nrelu = forge.get(\"relu\")\n\n# See what it is\nprint(relu)           # Max(0, x)\nprint(relu.formula)   # LaTeX\nprint(relu.explain())  # Full explanation\n</code></pre>"},{"location":"tutorials/beginner/#step-3-evaluate","title":"Step 3: Evaluate","text":"<pre><code>import numpy as np\n\n# Single value\nresult = relu.eval(x=5)    # 5\nresult = relu.eval(x=-3)   # 0\n\n# Array\nresult = relu.eval(x=np.array([-2, -1, 0, 1, 2]))\n# [0, 0, 0, 1, 2]\n</code></pre>"},{"location":"tutorials/beginner/#step-4-compute-gradients","title":"Step 4: Compute Gradients","text":"<pre><code># Symbolic gradient\nrelu_grad = relu.gradient(\"x\")\nprint(relu_grad)  # Derivative expression\n\n# Evaluate gradient\ngrad_value = relu_grad.eval(x=2)\n</code></pre>"},{"location":"tutorials/beginner/#step-5-explore","title":"Step 5: Explore","text":"<pre><code># List all available expressions\nall_exprs = forge.list_all()\n\n# Category-wise\nactivations = forge.list_all(category=\"activation\")\nlosses = forge.list_all(category=\"loss\")\n\n# Search\nresults = forge.search(\"smooth\")\n</code></pre>"},{"location":"tutorials/beginner/#whats-next","title":"What's Next?","text":"<ul> <li>Try different activations: <code>sigmoid</code>, <code>tanh</code>, <code>swish</code>, <code>gelu</code></li> <li>Look at loss functions: <code>mse</code>, <code>mae</code>, <code>huber</code></li> <li>Move to the Intermediate Tutorial</li> </ul>"},{"location":"tutorials/intermediate/","title":"Intermediate Tutorial","text":"<p>This tutorial covers composition, training, and the autograd engine.</p>"},{"location":"tutorials/intermediate/#expression-composition","title":"Expression Composition","text":""},{"location":"tutorials/intermediate/#arithmetic-operations","title":"Arithmetic Operations","text":"<pre><code>from neurogebra import MathForge, Expression\n\nforge = MathForge()\n\nmse = forge.get(\"mse\")\nmae = forge.get(\"mae\")\n\n# Weighted combination\nhybrid_loss = 0.7 * mse + 0.3 * mae\n\n# String-based composition\ncomposed = forge.compose(\"mse + 0.1*mae\")\n</code></pre>"},{"location":"tutorials/intermediate/#functional-composition","title":"Functional Composition","text":"<pre><code># f(g(x)) composition\nsigmoid = forge.get(\"sigmoid\")\nlinear = Expression(\"linear\", \"2*x + 1\")\n\ncomposed = sigmoid.compose(linear)\n# sigmoid(2*x + 1)\nresult = composed.eval(x=0)\n</code></pre>"},{"location":"tutorials/intermediate/#training-expressions","title":"Training Expressions","text":""},{"location":"tutorials/intermediate/#basic-training","title":"Basic Training","text":"<pre><code>import numpy as np\nfrom neurogebra import Expression\nfrom neurogebra.core.trainer import Trainer\n\n# Trainable quadratic\nexpr = Expression(\n    \"quadratic\",\n    \"a*x**2 + b*x + c\",\n    params={\"a\": 0.0, \"b\": 0.0, \"c\": 0.0},\n    trainable_params=[\"a\", \"b\", \"c\"]\n)\n\n# Data from y = x^2 - 2x + 1\nX = np.linspace(-3, 3, 50)\ny = X**2 - 2*X + 1\n\n# Train with SGD\ntrainer = Trainer(expr, learning_rate=0.001)\nhistory = trainer.fit(X, y, epochs=500, verbose=True)\n\nprint(f\"a={expr.params['a']:.2f}, b={expr.params['b']:.2f}, c={expr.params['c']:.2f}\")\n</code></pre>"},{"location":"tutorials/intermediate/#using-adam-optimizer","title":"Using Adam Optimizer","text":"<pre><code>trainer = Trainer(expr, learning_rate=0.01, optimizer=\"adam\")\nhistory = trainer.fit(X, y, epochs=200)\n</code></pre>"},{"location":"tutorials/intermediate/#autograd-engine","title":"Autograd Engine","text":""},{"location":"tutorials/intermediate/#value-class","title":"Value Class","text":"<pre><code>from neurogebra.core.autograd import Value\n\n# Create values\nx = Value(2.0)\nw1 = Value(-3.0)\nw2 = Value(1.0)\n\n# Forward pass\nh = (w1 * x + w2).relu()\nloss = h ** 2\n\n# Backward pass\nloss.backward()\n\nprint(f\"dL/dw1 = {w1.grad}\")\nprint(f\"dL/dw2 = {w2.grad}\")\nprint(f\"dL/dx = {x.grad}\")\n</code></pre>"},{"location":"tutorials/intermediate/#building-a-simple-neuron","title":"Building a Simple Neuron","text":"<pre><code>from neurogebra.core.autograd import Value\n\n# Inputs\nx1 = Value(2.0)\nx2 = Value(3.0)\n\n# Weights and bias\nw1 = Value(0.5)\nw2 = Value(-0.3)\nb = Value(0.1)\n\n# Neuron computation\nz = w1 * x1 + w2 * x2 + b\noutput = z.sigmoid()\n\n# Backpropagate\noutput.backward()\n\nprint(f\"Output: {output.data:.4f}\")\nprint(f\"dout/dw1 = {w1.grad:.4f}\")\nprint(f\"dout/dw2 = {w2.grad:.4f}\")\n</code></pre>"},{"location":"tutorials/intermediate/#custom-expressions","title":"Custom Expressions","text":"<pre><code>from neurogebra import MathForge, Expression\n\nforge = MathForge()\n\n# Create custom activation\nmy_act = Expression(\n    \"parametric_swish\",\n    \"x * (1 / (1 + exp(-beta * x)))\",\n    params={\"beta\": 1.0},\n    metadata={\n        \"category\": \"activation\",\n        \"description\": \"Parametric Swish with learnable beta\",\n    }\n)\n\n# Register for later use\nforge.register(\"parametric_swish\", my_act)\n\n# Now accessible via forge\nretrieved = forge.get(\"parametric_swish\")\n</code></pre>"},{"location":"tutorials/intermediate/#next-steps","title":"Next Steps","text":"<ul> <li>Advanced Tutorial - Framework bridges, visualization</li> </ul>"}]}