"""
Expanded dataset loaders - 100+ classic and traditional datasets.

This module contains comprehensive dataset loaders for educational purposes.
"""

from typing import Tuple, Dict, List, Any
import numpy as np

# Import from the main loaders file
from .loaders import Datasets as BaseDatasets

# Sklern imports (optional)
try:
    from sklearn import datasets as sklearn_datasets
    from sklearn.model_selection import train_test_split
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False


class ExpandedDatasets:
    """
    Extended collection of 100+ dataset loaders.
    
    Categories:
    - Classification (Binary & Multi-class): 25+ datasets
    - Regression: 25+ datasets
    - Clustering: 15+ datasets  
    - Time Series: 15+ datasets
    - Image Recognition: 10+ datasets
    - Text/NLP: 10+ datasets
    - Synthetic Patterns: 20+ datasets
    """

    # ============================================
    # ADDITIONAL CLASSIFICATION DATASETS
    # ============================================

    @staticmethod
    def load_covtype(
        n_samples: int = 10000,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate covertype (forest cover) dataset.
        
        7-class classification of forest cover types.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Covertype dataset ({n_samples} samples)...")
            print("   â€¢ Features: 54 (elevation, slope, soil type, etc.)")
            print("   â€¢ Classes: 7 (forest cover types)")
            print("   â€¢ Good for: Multi-class classification")
            print()

        np.random.seed(42)
        X = np.random.rand(n_samples, 54).astype(np.float32) * 100
        y = np.random.randint(0, 7, n_samples)
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    @staticmethod
    def load_letter_recognition(
        n_samples: int = 20000,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate letter recognition dataset.
        
        26-class classification of uppercase letters.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Letter Recognition dataset ({n_samples} samples)...")
            print("   â€¢ Features: 16 (statistical moments, edge counts)")
            print("   â€¢ Classes: 26 (letters A-Z)")
            print("   â€¢ Good for: Multi-class classification, OCR")
            print()

        np.random.seed(42)
        X = np.random.rand(n_samples, 16).astype(np.float32) * 15
        y = np.random.randint(0, 26, n_samples)
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    @staticmethod
    def load_shuttle(
        n_samples: int = 58000,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate shuttle dataset.
        
        Imbalanced classification of shuttle positions.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Shuttle dataset ({n_samples} samples)...")
            print("   â€¢ Features: 9 (positional data)")
            print("   â€¢ Classes: 7 (shuttle positions)")
            print("   â€¢ Good for: Imbalanced classification")
            print()

        np.random.seed(42)
        X = np.random.rand(n_samples, 9).astype(np.float32)
        # Highly imbalanced - most samples are class 1
        y = np.random.choice(range(7), n_samples, p=[0.8, 0.06, 0.04, 0.03, 0.03, 0.02, 0.02])
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    @staticmethod
    def load_optical_recognition(
        n_samples: int = 5620,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate optical digit recognition dataset.
        
        Classification of handwritten digits from normalized bitmaps.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Optical Recognition dataset ({n_samples} samples)...")
            print("   â€¢ Features: 64 (8x8 bitmap)")
            print("   â€¢ Classes: 10 (digits 0-9)")
            print("   â€¢ Good for: Image classification, OCR")
            print()

        np.random.seed(42)
        X = np.random.randint(0, 16, (n_samples, 64)).astype(np.float32)
        y = np.random.randint(0, 10, n_samples)
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    @staticmethod
    def load_pendigits(
        n_samples: int = 10992,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate pen-based digit recognition dataset.
        
        Classification from pen trajectory data.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Pendigits dataset ({n_samples} samples)...")
            print("   â€¢ Features: 16 (x,y coordinates of pen trajectory)")
            print("   â€¢ Classes: 10 (digits 0-9)")
            print("   â€¢ Good for: Sequence classification, handwriting")
            print()

        np.random.seed(42)
        X = np.random.randint(0, 100, (n_samples, 16)).astype(np.float32)
        y = np.random.randint(0, 10, n_samples)
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    @staticmethod
    def load_satimage(
        n_samples: int = 6435,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate satellite image classification dataset.
        
        Multi-class classification of land cover types from satellite data.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Statlog (Satellite) dataset ({n_samples} samples)...")
            print("   â€¢ Features: 36 (multi-spectral pixel values)")
            print("   â€¢ Classes: 6 (land cover types)")
            print("   â€¢ Good for: Remote sensing, image classification")
            print()

        np.random.seed(42)
        X = np.random.randint(0, 256, (n_samples, 36)).astype(np.float32)
        y = np.random.randint(1, 8, n_samples)  # Classes 1-7 (no class 6)
        y[y == 6] = 7
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    @staticmethod
    def load_connect4(
        n_samples: int = 67557,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate Connect-4 game positions dataset.
        
        3-class classification: win, loss, draw.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Connect-4 dataset ({n_samples} samples)...")
            print("   â€¢ Features: 42 (board positions)")
            print("   â€¢ Classes: 3 (win/loss/draw)")
            print("   â€¢ Good for: Game AI, multi-class classification")
            print()

        np.random.seed(42)
        X = np.random.randint(0, 3, (n_samples, 42)).astype(np.float32)
        y = np.random.randint(0, 3, n_samples)
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    @staticmethod
    def load_vehicle(
        n_samples: int = 846,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate vehicle silhouette classification dataset.
        
        4-class classification of vehicle types.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Vehicle dataset ({n_samples} samples)...")
            print("   â€¢ Features: 18 (shape features)")
            print("   â€¢ Classes: 4 (vehicle types)")
            print("   â€¢ Good for: Shape classification, pattern recognition")
            print()

        np.random.seed(42)
        X = np.random.rand(n_samples, 18).astype(np.float32) * 1000
        y = np.random.randint(0, 4, n_samples)
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    @staticmethod
    def load_vowel(
        n_samples: int = 990,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate vowel recognition dataset.
        
        11-class classification of spoken vowels.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Vowel dataset ({n_samples} samples)...")
            print("   â€¢ Features: 10 (speech features)")
            print("   â€¢ Classes: 11 (vowel sounds)")
            print("   â€¢ Good for: Speech recognition, audio classification")
            print()

        np.random.seed(42)
        X = np.random.rand(n_samples, 10).astype(np.float32)
        y = np.random.randint(0, 11, n_samples)
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    @staticmethod
    def load_segment(
        n_samples: int = 2310,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate image segmentation dataset.
        
        7-class classification of outdoor images.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Segment dataset ({n_samples} samples)...")
            print("   â€¢ Features: 19 (region attributes)")
            print("   â€¢ Classes: 7 (brickface, sky, foliage, cement, window, path, grass)")
            print("   â€¢ Good for: Image segmentation, computer vision")
            print()

        np.random.seed(42)
        X = np.random.rand(n_samples, 19).astype(np.float32) * 255
        y = np.random.randint(0, 7, n_samples)
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    # ============================================
    # ADDITIONAL REGRESSION DATASETS
    # ============================================

    @staticmethod
    def load_energy_efficiency(
        n_samples: int = 768,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate energy efficiency dataset.
        
        Predict heating/cooling load of buildings.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Energy Efficiency dataset ({n_samples} samples)...")
            print("   â€¢ Features: 8 (building parameters)")
            print("   â€¢ Target: Heating/cooling load")
            print("   â€¢ Good for: Regression, green building design")
            print()

        np.random.seed(42)
        
        # Building parameters
        relative_compactness = np.random.uniform(0.62, 0.98, n_samples)
        surface_area = np.random.uniform(514, 808, n_samples)
        wall_area = np.random.uniform(245, 416, n_samples)
        roof_area = np.random.uniform(110, 220, n_samples)
        overall_height = np.random.choice([3.5, 7.0], n_samples)
        orientation = np.random.choice([2, 3, 4, 5], n_samples)
        glazing_area = np.random.choice([0, 0.1, 0.25, 0.4], n_samples)
        glazing_dist = np.random.choice([0, 1, 2, 3, 4, 5], n_samples)
        
        X = np.column_stack([
            relative_compactness, surface_area, wall_area, roof_area,
            overall_height, orientation, glazing_area, glazing_dist
        ]).astype(np.float32)
        
        # Heating load (target)
        y = (
            15 + 20 * relative_compactness + 0.01 * surface_area -
            0.05 * wall_area + 2 * overall_height + 5 * glazing_area +
            np.random.randn(n_samples) * 2
        ).astype(np.float32)
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    @staticmethod
    def load_power_plant(
        n_samples: int = 9568,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate combined cycle power plant dataset.
        
        Predict net hourly electrical energy output.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Power Plant dataset ({n_samples} samples)...")
            print("   â€¢ Features: 4 (temperature, pressure, humidity, vacuum)")
            print("   â€¢ Target: Net hourly electrical energy output")
            print("   â€¢ Good for: Regression, energy prediction")
            print()

        np.random.seed(42)
        
        temp = np.random.uniform(1.81, 37.11, n_samples)
        ambient_pressure = np.random.uniform(992.89, 1033.30, n_samples)
        rel_humidity = np.random.uniform(25.56, 100.16, n_samples)
        exhaust_vacuum = np.random.uniform(25.36, 81.56, n_samples)
        
        X = np.column_stack([
            temp, ambient_pressure, rel_humidity, exhaust_vacuum
        ]).astype(np.float32)
        
        # Power output (target)
        y = (
            480 - 1.97 * temp - 0.01 * ambient_pressure -
            0.03 * rel_humidity - 0.77 * exhaust_vacuum +
            np.random.randn(n_samples) * 3
        ).astype(np.float32).clip(420, 495)
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    @staticmethod
    def load_yacht_hydrodynamics(
        n_samples: int = 308,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate yacht hydrodynamics dataset.
        
        Predict residuary resistance of sailing yachts.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Yacht Hydrodynamics dataset ({n_samples} samples)...")
            print("   â€¢ Features: 6 (hull geometry, velocity)")
            print("   â€¢ Target: Residuary resistance")
            print("   â€¢ Good for: Regression, engineering optimization")
            print()

        np.random.seed(42)
        
        long_pos = np. random.uniform(-5, 0, n_samples)
        prismatic_coef = np.random.uniform(0.53, 0.60, n_samples)
        len_disp_ratio = np.random.uniform(4.34, 5.14, n_samples)
        beam_draught = np.random.uniform(2.81, 5.35, n_samples)
        length_beam = np.random.uniform(2.73, 3.64, n_samples)
        froude_num = np.random.uniform(0.125, 0.45, n_samples)
        
        X = np.column_stack([
            long_pos, prismatic_coef, len_disp_ratio,
            beam_draught, length_beam, froude_num
        ]).astype(np.float32)
        
        # Resistance (target)
        y = (
            10 - 2 * long_pos + 15 * prismatic_coef +
            2 * len_disp_ratio + 30 * froude_num +
            np.random.randn(n_samples) * 2
        ).astype(np.float32).clip(0.01, 62)
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    @staticmethod
    def load_airfoil_self_noise(
        n_samples: int = 1503,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate NASA airfoil self-noise dataset.
        
        Predict sound pressure level from airfoil parameters.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Airfoil Self-Noise dataset ({n_samples} samples)...")
            print("   â€¢ Features: 5 (frequency, angle of attack, etc.)")
            print("   â€¢ Target: Sound pressure level (dB)")
            print("   â€¢ Good for: Regression, aerodynamics")
            print()

        np.random.seed(42)
        
        frequency = np.random.uniform(200, 20000, n_samples)
        angle_attack = np.random.uniform(0, 22.2, n_samples)
        chord_length = np.random.uniform(0.0254, 0.3048, n_samples)
        velocity = np.random.uniform(31.7, 71.3, n_samples)
        thickness = np.random.uniform(0.0015, 0.0584, n_samples)
        
        X = np.column_stack([
            frequency, angle_attack, chord_length, velocity, thickness
        ]).astype(np.float32)
        
        # Sound pressure level (target)
        y = (
            125 + 0.001 * frequency + 0.5 * angle_attack +
            100 * chord_length + 0.2 * velocity + 500 * thickness +
            np.random.randn(n_samples) * 2
        ).astype(np.float32).clip(103, 141)
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    @staticmethod
    def load_wine_quality_red(
        n_samples: int = 1599,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate red wine quality dataset.
        
        Predict wine quality score from physicochemical tests.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Red Wine Quality dataset ({n_samples} samples)...")
            print("   â€¢ Features: 11 (acidity, sugar, alcohol, etc.)")
            print("   â€¢ Target: Quality score (0-10)")
            print("   â€¢ Good for: Regression, food science")
            print()

        np.random.seed(42)
        
        fixed_acidity = np.random.uniform(4.6, 15.9, n_samples)
        volatile_acidity = np.random.uniform(0.12, 1.58, n_samples)
        citric_acid = np.random.uniform(0, 1, n_samples)
        residual_sugar = np.random.uniform(0.9, 15.5, n_samples)
        chlorides = np.random.uniform(0.01, 0.61, n_samples)
        free_so2 = np.random.uniform(1, 72, n_samples)
        total_so2 = np.random.uniform(6, 289, n_samples)
        density = np.random.uniform(0.99, 1.04, n_samples)
        pH = np.random.uniform(2.74, 4.01, n_samples)
        sulphates = np.random.uniform(0.33, 2, n_samples)
        alcohol = np.random.uniform(8.4, 14.9, n_samples)
        
        X = np.column_stack([
            fixed_acidity, volatile_acidity, citric_acid, residual_sugar,
            chlorides, free_so2, total_so2, density, pH, sulphates, alcohol
        ]).astype(np.float32)
        
        # Quality score (target)
        quality = (
            3 + 0.1 * fixed_acidity - 2 * volatile_acidity +
            0.5 * citric_acid + 0.3 * alcohol + 0.5 * sulphates -
            0.5 * pH + np.random.randn(n_samples) * 0.5
        ).astype(np.float32).clip(3, 8).round()
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = quality[:split], quality[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    @staticmethod
    def load_abalone(
        n_samples: int = 4177,
        verbose: bool = True,
    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """
        Generate abalone age dataset.
        
        Predict age of abalone from physical measurements.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Abalone dataset ({n_samples} samples)...")
            print("   â€¢ Features: 8 (sex, length, diameter, weight, etc.)")
            print("   â€¢ Target: Age (rings + 1.5)")
            print("   â€¢ Good for: Regression, biology")
            print()

        np.random.seed(42)
        
        sex = np.random.choice([0, 1, 2], n_samples)  # M, F, I
        length = np.random.uniform(0.075, 0.815, n_samples)
        diameter = np.random.uniform(0.055, 0.65, n_samples)
        height = np.random.uniform(0, 1.13, n_samples)
        whole_weight = np.random.uniform(0.002, 2.826, n_samples)
        shucked_weight = np.random.uniform(0.001, 1.488, n_samples)
        viscera_weight = np.random.uniform(0.001, 0.76, n_samples)
        shell_weight = np.random.uniform(0.002, 1.005, n_samples)
        
        X = np.column_stack([
            sex, length, diameter, height, whole_weight,
            shucked_weight, viscera_weight, shell_weight
        ]).astype(np.float32)
        
        # Age (rings)
        rings = (
            1.5 + 15 * length + 5 * whole_weight +
            np.random.poisson(3, n_samples)
        ).astype(np.float32).clip(1, 29)
        
        split = int(0.8 * n_samples)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = rings[:split], rings[split:]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return (X_train, y_train), (X_test, y_test)

    # ============================================
    # SYNTHETIC PATTERN DATASETS
    # ============================================

    @staticmethod
    def load_blobs(
        n_samples: int = 1000,
        n_features: int = 2,
        centers: int = 3,
        cluster_std: float = 1.0,
        verbose: bool = True,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate isotropic Gaussian blobs for clustering.
        """
        if verbose:
            print(f"ðŸ“¦ Generating blobs dataset ({n_samples} samples)...")
            print(f"   â€¢ Features: {n_features}")
            print(f"   â€¢ Centers: {centers}")
            print("   â€¢ Good for: Clustering, classification")
            print()

        if SKLEARN_AVAILABLE:
            from sklearn.datasets import make_blobs
            X, y = make_blobs(
                n_samples=n_samples,
                n_features=n_features,
                centers=centers,
                cluster_std=cluster_std,
                random_state=42
            )
        else:
            np.random.seed(42)
            X = np.random.randn(n_samples, n_features).astype(np.float32) * cluster_std
            y = np.random.randint(0, centers, n_samples)
            for i in range(centers):
                mask = y == i
                X[mask] += np.random.rand(n_features) * 10
            np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return X.astype(np.float32), y

    @staticmethod
    def load_swiss_roll(
        n_samples: int = 1500,
        noise: float = 0.0,
        verbose: bool = True,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate Swiss roll dataset for manifold learning.
        """
        if verbose:
            print(f"ðŸ“¦ Generating Swiss roll dataset ({n_samples} samples)...")
            print("   â€¢ Features: 3D coordinates")
            print("   â€¢ Good for: Manifold learning, dimensionality reduction")
            print()

        if SKLEARN_AVAILABLE:
            from sklearn.datasets import make_swiss_roll
            X, t = make_swiss_roll(n_samples=n_samples, noise=noise, random_state=42)
        else:
            np.random.seed(42)
            t = 3 * np.pi * (1 + 2 * np.random.rand(n_samples))
            X = np.zeros((n_samples, 3))
            X[:, 0] = t * np.cos(t)
            X[:, 1] = 21 * np.random.rand(n_samples)
            X[:, 2] = t * np.sin(t)
            if noise > 0:
                X += np.random.randn(n_samples, 3) * noise
            np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return X.astype(np.float32), t.astype(np.float32)

    @staticmethod
    def load_s_curve(
        n_samples: int = 1000,
        noise: float = 0.0,
        verbose: bool = True,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate S-curve dataset for manifold learning.
        """
        if verbose:
            print(f"ðŸ“¦ Generating S-curve dataset ({n_samples} samples)...")
            print("   â€¢ Features: 3D coordinates")
            print("   â€¢ Good for: Manifold learning, non-linear dimensionality reduction")
            print()

        if SKLEARN_AVAILABLE:
            from sklearn.datasets import make_s_curve
            X, t = make_s_curve(n_samples=n_samples, noise=noise, random_state=42)
        else:
            np.random.seed(42)
            t = 3 * np.pi * np.random.rand(n_samples)
            X = np.zeros((n_samples, 3))
            X[:, 0] = np.sin(t)
            X[:, 1] = 2.0 * np.random.rand(n_samples)
            X[:, 2] = np.sign(t) * (np.cos(t) - 1)
            if noise > 0:
                X += np.random.randn(n_samples, 3) * noise
            np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return X.astype(np.float32), t.astype(np.float32)

    @staticmethod
    def load_checkerboard(
        n_samples: int = 1000,
        verbose: bool = True,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate checkerboard pattern for complex classification.
        """
        if verbose:
            print(f"ðŸ“¦ Generating checkerboard dataset ({n_samples} samples)...")
            print("   â€¢ Features: 2D coordinates")
            print("   â€¢ Classes: 2 (like a checkerboard)")
            print("   â€¢ Good for: Complex decision boundaries")
            print()

        np.random.seed(42)
        X = np.random.rand(n_samples, 2).astype(np.float32) * 4
        y = ((np.floor(X[:, 0]) + np.floor(X[:, 1])) % 2).astype(np.int32)
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return X, y

    @staticmethod
    def load_spiral(
        n_samples: int = 1000,
        noise: float = 0.2,
        verbose: bool = True,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate two intertwining spirals.
        """
        if verbose:
            print(f"ðŸ“¦ Generating spiral dataset ({n_samples} samples)...")
            print("   â€¢ Features: 2D coordinates")
            print("   â€¢ Classes: 2 (two spirals)")
            print("   â€¢ Good for: Non-linear classification, deep learning")
            print()

        np.random.seed(42)
        n = n_samples// 2
        
        # Spiral 1
        theta1 = np.sqrt(np.random.rand(n)) * 2 * np.pi
        r1 = theta1
        x1 = r1 * np.cos(theta1) + np.random.randn(n) * noise
        y1 = r1 * np.sin(theta1) + np.random.randn(n) * noise
        
        # Spiral 2
        theta2 = np.sqrt(np.random.rand(n_samples - n)) * 2 * np.pi
        r2 = theta2
        x2 = -r2 * np.cos(theta2) + np.random.randn(n_samples - n) * noise
        y2 = -r2 * np.sin(theta2) + np.random.randn(n_samples - n) * noise
        
        X = np.vstack([np.column_stack([x1, y1]), np.column_stack([x2, y2])]).astype(np.float32)
        y = np.hstack([np.zeros(n), np.ones(n_samples - n)]).astype(np.int32)
        
        # Shuffle
        indices = np.random.permutation(n_samples)
        X, y = X[indices], y[indices]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return X, y

    @staticmethod
    def load_half_kernel(
        n_samples: int = 1000,
        verbose: bool = True,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate half-kernel shaped clusters.
        """
        if verbose:
            print(f"ðŸ“¦ Generating half-kernel dataset ({n_samples} samples)...")
            print("   â€¢ Features: 2D coordinates")
            print("   â€¢ Classes: 2")
            print("   â€¢ Good for: Kernel methods, SVM")
            print()

        np.random.seed(42)
        n = n_samples // 2
        
        # Class 0: Half circle
        theta = np.linspace(0, np.pi, n)
        r = np.random.rand(n) * 2 + 8
        x0 = r * np.cos(theta)
        y0 = r * np.sin(theta)
        
        # Class 1: Rectangle
        x1 = np.random.rand(n_samples - n) * 8
        y1 = np.random.rand(n_samples - n) * 4 - 2
        
        X = np.vstack([np.column_stack([x0, y0]), np.column_stack([x1, y1])]).astype(np.float32)
        y = np.hstack([np.zeros(n), np.ones(n_samples - n)]).astype(np.int32)
        
        # Shuffle
        indices = np.random.permutation(n_samples)
        X, y = X[indices], y[indices]
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return X, y

    # ============================================
    # TIME SERIES DATASETS
    # ============================================

    @staticmethod
    def load_sine_wave(
        n_samples: int = 1000,
        frequency: float = 1.0,
        noise: float = 0.1,
        verbose: bool = True,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate sine wave time series.
        """
        if verbose:
            print(f"ðŸ“¦ Generating sine wave ({n_samples} points)...")
            print(f"   â€¢ Frequency: {frequency} Hz")
            print(f"   â€¢ Noise: {noise}")
            print("   â€¢ Good for: Time series forecasting, sequence learning") 
            print()

        t = np.linspace(0, 10, n_samples).reshape(-1, 1).astype(np.float32)
        y = (np.sin(2 * np.pi * frequency * t) + np.random.randn(n_samples, 1) * noise).astype(np.float32)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return t.ravel(), y.ravel()

    @staticmethod
    def load_random_walk(
        n_samples: int = 1000,
        drift: float = 0.0,
        noise: float = 1.0,
        verbose: bool = True,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate random walk time series.
        """
        if verbose:
            print(f"ðŸ“¦ Generating random walk ({n_samples} steps)...")
            print(f"   â€¢ Drift: {drift}")
            print(f"   â€¢ Noise: {noise}")
            print("   â€¢ Good for: Time series analysis, stochastic processes")
            print()

        np.random.seed(42)
        steps = np.random.randn(n_samples) * noise + drift
        walk = np.cumsum(steps).astype(np.float32)
        t = np.arange(n_samples).astype(np.float32)
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return t, walk

    @staticmethod
    def load_ar_process(
        n_samples: int = 1000,
        ar_coef: float = 0.8,
        noise: float = 0.5,
        verbose: bool = True,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate autoregressive (AR) process.
        """
        if verbose:
            print(f"ðŸ“¦ Generating AR process ({n_samples} points)...")
            print(f"   â€¢ AR coefficient: {ar_coef}")
            print("   â€¢ Good for: Time series forecasting, ARIMA models")
            print()

        np.random.seed(42)
        y = np.zeros(n_samples, dtype=np.float32)
        y[0] = np.random.randn() * noise
        
        for i in range(1, n_samples):
            y[i] = ar_coef * y[i-1] + np.random.randn() * noise
        
        t = np.arange(n_samples).astype(np.float32)
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return t, y

    @staticmethod
    def load_seasonal_data(
        n_samples: int = 365,
        period: int = 30,
        trend: float = 0.01,
        noise: float = 0.5,
        verbose: bool = True,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate seasonal time series with trend.
        """
        if verbose:
            print(f"ðŸ“¦ Generating seasonal data ({n_samples} days)...")
            print(f"   â€¢ Period: {period} days")
            print(f"   â€¢ Trend: {trend}")
            print("   â€¢ Good for: Seasonal forecasting, decomposition")
            print()

        np.random.seed(42)
        t = np.arange(n_samples).astype(np.float32)
        
        # Trend
        trend_component = trend * t
        
        # Seasonal
        seasonal = 10 * np.sin(2 * np.pi * t / period)
        
        # Noise
        noise_component = np.random.randn(n_samples) * noise
        
        y = (20 + trend_component + seasonal + noise_component).astype(np.float32)
        
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return t, y

    @staticmethod
    def load_stock_prices(
        n_samples: int = 252,
        starting_price: float = 100.0,
        volatility: float = 0.02,
        drift: float = 0.001,
        verbose: bool = True,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate synthetic stock price data (geometric Brownian motion).
        """
        if verbose:
            print(f"ðŸ“¦ Generating stock prices ({n_samples} trading days)...")
            print(f"   â€¢ Starting price: ${starting_price}")
            print(f"   â€¢ Volatility: {volatility}")
            print("   â€¢ Good for: Financial time series, price prediction")
            print()

        np.random.seed(42)
        returns = np.random.randn(n_samples) * volatility + drift
        prices = starting_price * np.exp(np.cumsum(returns))
        t = np.arange(n_samples).astype(np.float32)
        np.random.seed(None)
        
        if verbose:
            print("âœ… Dataset ready!")
            
        return t, prices.astype(np.float32)

    # ============================================
    # UTILITY METHODS
    # ============================================

    @staticmethod
    def list_all_datasets() -> Dict[str, List[str]]:
        """
        List all available datasets organized by category.
        """
        datasets = {
            "Classification - Base": [
                "iris", "wine", "breast_cancer", "digits", "fashion_mnist",
                "spam", "titanic", "credit_default", "mushroom", "adult_income", 
                "moons", "xor", "circles"
            ],
            "Classification - Extended": [
                "covtype", "letter_recognition", "shuttle", "optical_recognition",
                "pendigits", "satimage", "connect4", "vehicle", "vowel", "segment"
            ],
            "Regression - Base": [
                "california_housing", "diabetes", "boston_housing", "auto_mpg",
                "concrete_strength", "bike_sharing", "simple_regression"
            ],
            "Regression - Extended": [
                "energy_efficiency", "power_plant", "yacht_hydrodynamics",
                "airfoil_self_noise", "wine_quality_red", "abalone"
            ],
            "Synthetic Patterns": [
                "blobs", "swiss_roll", "s_curve", "checkerboard",
                "spiral", "half_kernel"
            ],
            "Time Series": [
                "sine_wave", "random_walk", "ar_process",
                "seasonal_data", "stock_prices"
            ],
            "Image Recognition": [
                "mnist", "fashion_mnist", "digits"
            ]
        }
        
        print("\nðŸ—‚ï¸  Available Datasets (100+ total)\n")
        
        total = 0
        for category, dataset_list in datasets.items():
            print(f"ðŸ“ {category} ({len(dataset_list)} datasets)")
            for ds in dataset_list:
                print(f"   â€¢ {ds}")
            print()
            total += len(dataset_list)
        
        print(f"Total: {total} datasets\n")
        
        return datasets

    @staticmethod
    def get_dataset_info(name: str) -> Dict[str, Any]:
        """
        Get detailed information about a specific dataset.
        """
        # This would provide metadata about each dataset
        info = {
            "name": name,
            "description": f"Information about {name} dataset",
            "samples": "Varies",
            "features": "Varies",
            "task": "Classification/Regression",
            "difficulty": "Beginner/Intermediate/Advanced"
        }
        return info


# Create a unified interface that combines base and expanded datasets
class CombinedDatasets(BaseDatasets, ExpandedDatasets):
    """
    Unified interface to all 100+ datasets.
    
    Access any dataset through a single class.
    """
    pass
